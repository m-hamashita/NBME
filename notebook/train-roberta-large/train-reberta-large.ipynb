{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired https://www.kaggle.com/code/yasufuminakama/nbme-deberta-base-baseline-train etc... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7426d7d"
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1366,
     "status": "ok",
     "timestamp": 1650910196447,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "797404f7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.core.display import display\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import yaml\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import pandas as pd\n",
    "import torch\n",
    "from logging import Logger, getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "import wandb\n",
    "from wandb.sdk.wandb_config import Config\n",
    "\n",
    "\n",
    "def init_pandas() -> None:\n",
    "\n",
    "    pd.set_option(\"display.max_rows\", 500)\n",
    "    pd.set_option(\"display.max_columns\", 500)\n",
    "    pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "\n",
    "def get_logger(filename: str) -> Logger:\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def init_wandb(wandb_key: str) -> Config:\n",
    "    secret_value_0 = wandb_key\n",
    "    wandb.login(key=secret_value_0)\n",
    "\n",
    "    loader = yaml.SafeLoader\n",
    "    loader.add_implicit_resolver(\n",
    "        \"tag:yaml.org,2002:float\",\n",
    "        re.compile(\n",
    "            \"\"\"^(?:\n",
    "         [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)?\n",
    "        |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)\n",
    "        |\\\\.[0-9_]+(?:[eE][-+][0-9]+)?\n",
    "        |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\\\.[0-9_]*\n",
    "        |[-+]?\\\\.(?:inf|Inf|INF)\n",
    "        |\\\\.(?:nan|NaN|NAN))$\"\"\",\n",
    "            re.X,\n",
    "        ),\n",
    "        list(\"-+0123456789.\"),\n",
    "    )\n",
    "    with open(f\"./config.yml\") as f:\n",
    "        param = yaml.load(f, Loader=loader)\n",
    "    wandb.init(project=param[\"project\"], config=param)\n",
    "    wandb.config.update(param)\n",
    "    print(f\"run name: {wandb.run.name}\")\n",
    "    return wandb.config\n",
    "\n",
    "\n",
    "def mk_output_dir(path: str) -> None:\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7061,
     "status": "ok",
     "timestamp": 1650910203504,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "73494049-1b8a-4400-a39d-18f47f6c69de",
    "outputId": "662ee360-c60e-467d-80cf-101596e16c89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmpeg\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/workspace/exp096/wandb/run-20220501_144820-1g4l3y2m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mpeg/NBME-ScoreClinicalPatientNotes/runs/1g4l3y2m\" target=\"_blank\">wild-jazz-35</a></strong> to <a href=\"https://wandb.ai/mpeg/NBME-ScoreClinicalPatientNotes\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run name: wild-jazz-35\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from getpass import getpass\n",
    "\n",
    "wandb_key = getpass()\n",
    "config = init_wandb(wandb_key=wandb_key)\n",
    "mk_output_dir(path=config.output_dir)\n",
    "logger = get_logger(filename=config.output_dir + \"train\")\n",
    "seed_everything(seed=config.seed)\n",
    "init_pandas()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "575ae408"
   },
   "source": [
    "# Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1650910203932,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "8771dcad"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def get_score(y_true: ndarray, y_pred: ndarray) -> float:\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def micro_f1(preds: list, truths: list) -> float:\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans: list, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(\n",
    "            np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0\n",
    "        )\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "\n",
    "    return micro_f1(bin_preds, bin_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3294,
     "status": "ok",
     "timestamp": 1650910207225,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "c20fdb1b"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from numpy import ndarray\n",
    "from pandas import DataFrame\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def create_labels_for_scoring(df: DataFrame):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(\n",
    "                f'[[\"{new_lst}\"]]'\n",
    "            )\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(\n",
    "    texts: list, predictions: ndarray, tokenizer: PreTrainedTokenizer\n",
    ") -> list:\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, add_special_tokens=True, return_offsets_mapping=True)\n",
    "        prev_pred = 0\n",
    "        prev_end = -1\n",
    "        for offset_mapping, pred in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "            if start != prev_end:\n",
    "                results[i][prev_end:start] = (pred + prev_pred) / 2\n",
    "            prev_pred = pred\n",
    "            prev_end = end\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def cluster_elements(xs: list) -> list:\n",
    "    clusters = [[]]\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        return clusters\n",
    "\n",
    "    prev_x = xs[0] - 1\n",
    "    for x in xs:\n",
    "        if x == prev_x + 1:\n",
    "            clusters[-1].append(x)\n",
    "        else:\n",
    "            clusters.append([x])\n",
    "        prev_x = x\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def get_results(char_probs: list, pn_histories: list, th: float = 0.5) -> list:\n",
    "    label_strs = []\n",
    "    for char_prob, pn_history in zip(char_probs, pn_histories):\n",
    "        pos_char_indices = np.where(char_prob > th)[0] + 1\n",
    "        if len(pos_char_indices) > 0 and pos_char_indices[0] == 1:\n",
    "            pos_char_indices = np.hstack([[0], pos_char_indices])\n",
    "        clustered_pos_char_indices = cluster_elements(xs=pos_char_indices)\n",
    "\n",
    "        for i in range(len(clustered_pos_char_indices)):\n",
    "            # 1文字目がspaceの場合\n",
    "            if len(clustered_pos_char_indices[i]) > 0:\n",
    "                target_idx = clustered_pos_char_indices[i][0] - 1\n",
    "                if target_idx > -1 and pn_history[target_idx] != \" \":\n",
    "                    clustered_pos_char_indices[i] = np.hstack(\n",
    "                        [[target_idx], clustered_pos_char_indices[i]]\n",
    "                    )\n",
    "            # 1文字目が\\r\\nの場合\n",
    "            if len(clustered_pos_char_indices[i]) > 0:\n",
    "                if clustered_pos_char_indices[i][0] > 0 and clustered_pos_char_indices[\n",
    "                    i\n",
    "                ][0] + 2 < len(pn_history):\n",
    "                    if (\n",
    "                        pn_history[\n",
    "                            clustered_pos_char_indices[i][\n",
    "                                0\n",
    "                            ] : clustered_pos_char_indices[i][0]\n",
    "                            + 2\n",
    "                        ]\n",
    "                        == \"\\r\\n\"\n",
    "                    ):\n",
    "                        clustered_pos_char_indices[i] = clustered_pos_char_indices[i][\n",
    "                            2:\n",
    "                        ]\n",
    "            # 最後の2文字が\\n-の場合\n",
    "            if len(clustered_pos_char_indices[i]) > 0:\n",
    "                target_idx = clustered_pos_char_indices[i][-1] - 2\n",
    "                if target_idx > 0 and pn_history[target_idx : target_idx + 2] == \"\\n-\":\n",
    "                    clustered_pos_char_indices[i] = clustered_pos_char_indices[i][:-2]\n",
    "\n",
    "        pos_char_spans = []\n",
    "        if len(clustered_pos_char_indices[0]) != 0:\n",
    "            for x in clustered_pos_char_indices:\n",
    "                if len(x) > 0:\n",
    "                    pos_char_spans.append([x[0], x[-1]])\n",
    "        label_strs.append(\";\".join([f\"{x[0]} {x[1]}\" for x in pos_char_spans]))\n",
    "\n",
    "    return label_strs\n",
    "\n",
    "\n",
    "def get_predictions(results: list) -> list:\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def get_result(\n",
    "    df_oof: DataFrame, tokenizer: PreTrainedTokenizer, max_len: int\n",
    ") -> tuple:\n",
    "    labels = create_labels_for_scoring(df_oof)\n",
    "    predictions = df_oof[[i for i in range(max_len)]].to_numpy()\n",
    "    char_probs = get_char_probs(df_oof[\"pn_history\"].to_numpy(), predictions, tokenizer)\n",
    "    pn_histories = df_oof[\"pn_history\"].to_list()\n",
    "\n",
    "    score = -100\n",
    "    for th in np.arange(0.3, 0.7, 0.005):\n",
    "        th = np.round(th, 4)\n",
    "        results = get_results(char_probs, pn_histories, th=th)\n",
    "        preds = get_predictions(results)\n",
    "        tmp_score = get_score(labels, preds)\n",
    "        if tmp_score > score:\n",
    "            best_th = th\n",
    "            score = tmp_score\n",
    "\n",
    "    return score, best_th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dc99c98"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T23:46:53.45023Z",
     "iopub.status.busy": "2022-04-26T23:46:53.449418Z",
     "iopub.status.idle": "2022-04-26T23:46:53.467497Z",
     "shell.execute_reply": "2022-04-26T23:46:53.466841Z",
     "shell.execute_reply.started": "2022-04-26T23:46:53.450185Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess_features(features: DataFrame) -> None:\n",
    "    features.loc[27, \"feature_text\"] = \"Last-Pap-smear-1-year-ago\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1650910208040,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "f93fc583",
    "outputId": "b46aba9f-5cc3-4be5-f4cc-5fb5f67cdbd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape: (14300, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[chest pressure]</td>\n",
       "      <td>[203 217]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[intermittent episodes, episode]</td>\n",
       "      <td>[70 91, 176 183]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[felt as if he were going to pass out]</td>\n",
       "      <td>[222 258]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num                              annotation          location\n",
       "0  00016_000         0      16            0          [dad with recent heart attcak]         [696 724]\n",
       "1  00016_001         0      16            1             [mom with \"thyroid disease]         [668 693]\n",
       "2  00016_002         0      16            2                        [chest pressure]         [203 217]\n",
       "3  00016_003         0      16            3        [intermittent episodes, episode]  [70 91, 176 183]\n",
       "4  00016_004         0      16            4  [felt as if he were going to pass out]         [222 258]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.shape: (143, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Chest-pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Lightheaded</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_num  case_num                                       feature_text\n",
       "0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n",
       "1            1         0                 Family-history-of-thyroid-disorder\n",
       "2            2         0                                     Chest-pressure\n",
       "3            3         0                              Intermittent-symptoms\n",
       "4            4         0                                        Lightheaded"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_notes.shape: (42146, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pn_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17-year-old male, has come to the student heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17 yo male with recurrent palpitations for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Dillon Cleveland is a 17 y.o. male patient wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a 17 yo m c/o palpitation started 3 mos ago; \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17yo male with no pmh here for evaluation of p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pn_num  case_num                                         pn_history\n",
       "0       0         0  17-year-old male, has come to the student heal...\n",
       "1       1         0  17 yo male with recurrent palpitations for the...\n",
       "2       2         0  Dillon Cleveland is a 17 y.o. male patient wit...\n",
       "3       3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n",
       "4       4         0  17yo male with no pmh here for evaluation of p..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "INPUT_DIR = Path(\"../../input/\")\n",
    "df_train = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "df_train[\"annotation\"] = df_train[\"annotation\"].map(lambda x: ast.literal_eval(x))\n",
    "df_train[\"location\"] = df_train[\"location\"].map(lambda x: ast.literal_eval(x))\n",
    "\n",
    "features = pd.read_csv(INPUT_DIR / \"features.csv\")\n",
    "preprocess_features(features)\n",
    "\n",
    "patient_notes = pd.read_csv(INPUT_DIR / \"patient_notes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650910208041,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "ce0adc5c",
    "outputId": "61045f17-e17e-4555-c577-198ec790424a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[chest pressure]</td>\n",
       "      <td>[203 217]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[intermittent episodes, episode]</td>\n",
       "      <td>[70 91, 176 183]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[felt as if he were going to pass out]</td>\n",
       "      <td>[222 258]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num                              annotation          location                                       feature_text                                         pn_history\n",
       "0  00016_000         0      16            0          [dad with recent heart attcak]         [696 724]  Family-history-of-MI-OR-Family-history-of-myoc...  HPI: 17yo M presents with palpitations. Patien...\n",
       "1  00016_001         0      16            1             [mom with \"thyroid disease]         [668 693]                 Family-history-of-thyroid-disorder  HPI: 17yo M presents with palpitations. Patien...\n",
       "2  00016_002         0      16            2                        [chest pressure]         [203 217]                                     Chest-pressure  HPI: 17yo M presents with palpitations. Patien...\n",
       "3  00016_003         0      16            3        [intermittent episodes, episode]  [70 91, 176 183]                              Intermittent-symptoms  HPI: 17yo M presents with palpitations. Patien...\n",
       "4  00016_004         0      16            4  [felt as if he were going to pass out]         [222 258]                                        Lightheaded  HPI: 17yo M presents with palpitations. Patien..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = df_train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "df_train = df_train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1650910208674,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "66337109"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "def correct_annotation(df_train:DataFrame) -> None:\n",
    "    df_train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "    df_train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "    df_train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "    df_train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "    df_train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "    df_train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "    df_train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "    df_train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "    df_train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "    df_train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "    df_train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "    df_train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "    df_train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "    df_train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "    df_train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "    df_train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "    df_train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "    df_train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "    df_train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "    df_train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "    df_train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "    df_train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "    df_train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "    df_train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "    df_train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "    df_train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "    df_train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "    df_train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "    df_train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "    df_train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "    df_train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "    df_train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "    df_train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "    df_train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "    df_train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "    df_train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "    df_train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "    df_train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "    df_train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "    df_train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "    df_train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "    df_train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "    df_train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "    df_train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "    df_train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "    df_train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "    df_train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "    df_train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "    df_train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "    df_train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "    df_train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "    df_train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "    df_train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "    df_train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "    df_train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "    df_train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "    df_train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "    df_train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "    df_train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "    df_train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "    df_train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "    df_train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "    df_train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "    df_train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "    df_train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "    df_train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "    df_train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "    df_train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "    df_train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "    df_train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "    df_train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "    df_train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "    df_train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "    df_train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "    df_train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "    df_train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "    df_train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "    df_train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "    df_train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "    df_train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "    df_train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "    df_train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "    df_train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "    df_train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1650910208674,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "ec3a7b5f",
    "outputId": "7d119c39-2b53-46c0-b5ca-52f72a6fc104"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8181\n",
       "0    4399\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['annotation_length'] = df_train['annotation'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_train = pd.read_csv('./pl_train_with_folds.csv')\n",
    "pl_train['location'] = pl_train['location'].fillna('[]')\n",
    "pl_train['location'] = pl_train['location'].map(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6c6437f"
   },
   "source": [
    "# CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650910208674,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "ed511f87",
    "outputId": "ffe7d0af-603d-44f5-f1cc-ba23c958d0ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "kf = GroupKFold(n_splits=config.n_folds)\n",
    "groups = df_train['pn_num'].to_numpy()\n",
    "df_train.loc[:, 'fold'] = -1\n",
    "for n, (train_index, val_index) in enumerate(kf.split(df_train, df_train['location'], groups)):\n",
    "    df_train.loc[val_index, 'fold'] = n\n",
    "display(df_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1650910208675,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "526e4ac9"
   },
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    display(df_train.groupby('fold').size())\n",
    "    df_train = df_train.sample(n=500, random_state=0).reset_index(drop=True)\n",
    "    display(df_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11e2590f"
   },
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3472,
     "status": "ok",
     "timestamp": 1650910212142,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "819ca57b",
    "outputId": "9d5b5c9e-0186-44f5-e974-4c83e30bf318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../output/exp096/non-leak/tokenizer/tokenizer_config.json',\n",
       " '../output/exp096/non-leak/tokenizer/special_tokens_map.json',\n",
       " '../output/exp096/non-leak/tokenizer/vocab.json',\n",
       " '../output/exp096/non-leak/tokenizer/merges.txt',\n",
       " '../output/exp096/non-leak/tokenizer/added_tokens.json',\n",
       " '../output/exp096/non-leak/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.tokenizer,\n",
    "    trim_offsets=False\n",
    ")\n",
    "tokenizer.save_pretrained(config.output_dir+'tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 26136,
     "status": "ok",
     "timestamp": 1650910238270,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "7d057ad3",
    "outputId": "92e42c7c-53d0-4ff7-e80f-0025d3808755"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pn_history max(lengths): 433\n",
      "feature_text max(lengths): 30\n",
      "max_len: 466\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "for text in patient_notes[\"pn_history\"].fillna(\"\").to_list():\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "pn_history_max_len = max(pn_history_lengths)\n",
    "logger.info(f\"pn_history max(lengths): {pn_history_max_len}\")\n",
    "\n",
    "features_lengths = []\n",
    "for text in features[\"feature_text\"].fillna(\"\").to_list():\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    features_lengths.append(length)\n",
    "feature_text_max_len = max(features_lengths)\n",
    "logger.info(f\"feature_text max(lengths): {feature_text_max_len}\")\n",
    "\n",
    "config.max_len = pn_history_max_len + feature_text_max_len + 3\n",
    "logger.info(f\"max_len: {config.max_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06f9f616"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650910238271,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "1387f638"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_len: int,\n",
    "        feature_text_max_len: int,\n",
    "        pn_history_max_len: int,\n",
    "        df: DataFrame,\n",
    "    ) -> None:\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.feature_text_max_len = feature_text_max_len\n",
    "        self.pn_history_max_len = pn_history_max_len\n",
    "        self.feature_texts = df[\"feature_text\"].to_numpy()\n",
    "        self.pn_historys = df[\"pn_history\"].to_numpy()\n",
    "        self.annotation_lengths = df[\"annotation_length\"].to_numpy()\n",
    "        self.locations = df[\"location\"].to_numpy()\n",
    "\n",
    "    def prepare_input_with_fixed_position(\n",
    "        self, pn_history: str, feature_text: str\n",
    "    ) -> dict:\n",
    "\n",
    "        pn_history_token = self.tokenizer(\n",
    "            pn_history,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.pn_history_max_len + 2,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "\n",
    "        feature_text_token = self.tokenizer(\n",
    "            feature_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.feature_text_max_len + 2,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in feature_text_token.items():\n",
    "            feature_text_token[k] = v[1:]\n",
    "\n",
    "        token = {\n",
    "            \"input_ids\": pn_history_token[\"input_ids\"]\n",
    "            + feature_text_token[\"input_ids\"],\n",
    "            \"attention_mask\": pn_history_token[\"attention_mask\"]\n",
    "            + feature_text_token[\"attention_mask\"],\n",
    "            #             'token_type_ids': pn_history_token['token_type_ids']+list(torch.ones_like(\n",
    "            #                 torch.tensor(feature_text_token['token_type_ids'], dtype=torch.long)\n",
    "            #                 ))\n",
    "        }\n",
    "        for k, v in token.items():\n",
    "            token[k] = torch.tensor(v[: self.max_len], dtype=torch.long)\n",
    "        return token\n",
    "\n",
    "    def create_label(\n",
    "        self, text: str, annotation_length: int, location_list: list\n",
    "    ) -> Tensor:\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping))\n",
    "        label[ignore_idxes] = -1\n",
    "        if annotation_length != 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start_idx = -1\n",
    "                    end_idx = -1\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    for idx in range(len(offset_mapping)):\n",
    "                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                            start_idx = idx - 1\n",
    "                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                            end_idx = idx + 1\n",
    "                    if start_idx == -1:\n",
    "                        start_idx = end_idx\n",
    "                    if (start_idx != -1) & (end_idx != -1):\n",
    "                        label[start_idx:end_idx] = 1\n",
    "\n",
    "        return torch.tensor(label[: self.max_len], dtype=torch.float)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item: int) -> tuple:\n",
    "\n",
    "        inputs = self.prepare_input_with_fixed_position(\n",
    "            self.pn_historys[item], self.feature_texts[item]\n",
    "        )\n",
    "        label = self.create_label(\n",
    "            self.pn_historys[item], self.annotation_lengths[item], self.locations[item]\n",
    "        )\n",
    "\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "006c4b6c"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650910238271,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "aab0ac97"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.optim import Optimizer\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "\n",
    "class CustomModel(Module):\n",
    "    def __init__(\n",
    "        self, model_name: str, config_path: str = None, pretrained: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(\n",
    "                model_name, output_hidden_states=True\n",
    "            )\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        self.initializer_range = 0.1\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "\n",
    "    def _init_weights(self, module: Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def feature(self, inputs: Tensor) -> Tensor:\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output\n",
    "\n",
    "\n",
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "        criterion: _Loss,\n",
    "        optimizer: Optimizer,\n",
    "        adv_param: str = \"weight\",\n",
    "        adv_lr: int = 1,\n",
    "        adv_eps: float = 0.2,\n",
    "        start_epoch: int = 0,\n",
    "        adv_step: int = 1,\n",
    "        scaler=None,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def attack_backward(self, inputs: Tensor, labels: Tensor, epoch: int) -> None:\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "        self._save()\n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds = self.model(inputs)\n",
    "                adv_loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "                adv_loss = torch.masked_select(\n",
    "                    adv_loss, labels.view(-1, 1) != -1\n",
    "                ).mean()\n",
    "                adv_loss = adv_loss.mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "        self._restore()\n",
    "\n",
    "    def _attack_step(self) -> None:\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if (\n",
    "                param.requires_grad\n",
    "                and param.grad is not None\n",
    "                and self.adv_param in name\n",
    "            ):\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]),\n",
    "                        self.backup_eps[name][1],\n",
    "                    )\n",
    "\n",
    "    def _save(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if (\n",
    "                param.requires_grad\n",
    "                and param.grad is not None\n",
    "                and self.adv_param in name\n",
    "            ):\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "250ef911"
   },
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650910238271,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "716ccfc0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from math import floor\n",
    "from torch import inference_mode\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val: float, n=1) -> None:\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s) -> str:\n",
    "    m = floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent) -> str:\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3139cba-ebd0-4faf-bb86-de5f7f032293"
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 1051,
     "status": "ok",
     "timestamp": 1650910239314,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "ef0f8033"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from logging import Logger\n",
    "import joblib\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch import cuda\n",
    "from transformers import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from wandb.sdk.wandb_config import Config\n",
    "\n",
    "\n",
    "def get_optimizer_params(\n",
    "    model: Module, encoder_lr: float, decoder_lr: float, weight_decay: float = 0.0\n",
    ") -> list:\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"lr\": encoder_lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"lr\": encoder_lr,\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "            \"lr\": decoder_lr,\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return optimizer_parameters\n",
    "\n",
    "\n",
    "def get_scheduler(\n",
    "    scheduler: str,\n",
    "    optimizer: Optimizer,\n",
    "    num_warmup_steps: int,\n",
    "    num_train_steps: int,\n",
    "    num_cycles: int,\n",
    ") -> _LRScheduler:\n",
    "\n",
    "    if scheduler == \"linear\":\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_train_steps,\n",
    "        )\n",
    "    elif scheduler == \"cosine\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_train_steps,\n",
    "            num_cycles=num_cycles,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Scheduler Name.\")\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, config: Config, tokenizer: PreTrainedTokenizer, logger: Logger\n",
    "    ) -> None:\n",
    "\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.logger = logger\n",
    "        self.device = torch.device(\"cuda\" if cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def log(self, data: dict, prefix: str = \"\") -> None:\n",
    "\n",
    "        min_str_length = min([len(k) for k in data.keys()])\n",
    "        n_same_char_seqs = 0\n",
    "        for i in range(min_str_length):\n",
    "            s = set([k[i] for k in data.keys()])\n",
    "            if len(s) == 1:\n",
    "                n_same_char_seqs += 1\n",
    "            else:\n",
    "                break\n",
    "        str_logs = [f\"{k}: {v}\" for k, v in data.items()]\n",
    "        s = \" \".join([l[n_same_char_seqs:].capitalize() for l in str_logs])\n",
    "        if prefix != \"\":\n",
    "            s = f\"{prefix} - {s}\"\n",
    "        self.logger.info(s)\n",
    "        wandb.log(data)\n",
    "\n",
    "    def compute_loss(\n",
    "        self, y_preds: Tensor, labels: Tensor, batch_size: int, loss_th: float\n",
    "    ) -> tuple:\n",
    "\n",
    "        loss = self.criterion(y_preds.squeeze(-1), labels.squeeze(-1))\n",
    "        samplewise_losses = []\n",
    "        for i in range(batch_size):\n",
    "            samplewise_losses.append(\n",
    "                torch.masked_select(loss[i], labels[i].squeeze(-1) != -1).mean()\n",
    "            )\n",
    "        loss = torch.stack(samplewise_losses)\n",
    "        loss_filter = torch.ones(batch_size, device=self.device)\n",
    "        if loss_th is not None:\n",
    "            mask = loss > loss_th\n",
    "            n_masked = mask.sum()\n",
    "            if n_masked > 0:\n",
    "                self.logger.info(f\"{n_masked} sample's loss was removed.\")\n",
    "            loss_filter[mask] = 0.0\n",
    "        else:\n",
    "            n_masked = 0\n",
    "\n",
    "        samplewise_losses = []\n",
    "        if loss_th is None:\n",
    "            for l in samplewise_losses:\n",
    "                samplewise_losses.append(l.item())\n",
    "\n",
    "        return (loss * loss_filter).sum() / (batch_size - n_masked), samplewise_losses\n",
    "\n",
    "    def train_with_eval(\n",
    "        self,\n",
    "        model: Module,\n",
    "        fold: int,\n",
    "        dls: tuple,\n",
    "        optimizer: Optimizer,\n",
    "        epoch: int,\n",
    "        scheduler: _LRScheduler,\n",
    "        loss_th: float,\n",
    "        valid_texts: list,\n",
    "        valid_labels: ndarray,\n",
    "        n_vl: int,\n",
    "        best_score: float,\n",
    "    ) -> tuple:\n",
    "\n",
    "        tr_dl, vl_dl = dls\n",
    "        model.train()\n",
    "        scaler = cuda.amp.GradScaler(enabled=self.config.apex)\n",
    "        awp = AWP(\n",
    "            model,\n",
    "            self.criterion,\n",
    "            optimizer,\n",
    "            adv_lr=self.config.adv_lr,\n",
    "            adv_eps=self.config.adv_eps,\n",
    "            start_epoch=self.config.adv_start_epoch,\n",
    "            scaler=scaler,\n",
    "        )\n",
    "\n",
    "        am = AverageMeter()\n",
    "        samplewise_losses = []\n",
    "        start = end = time.time()\n",
    "        global_step = 0\n",
    "        for step, (inputs, labels) in enumerate(tr_dl):\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            batch_size = labels.size(0)\n",
    "            with cuda.amp.autocast(enabled=self.config.apex):\n",
    "                y_preds = model(inputs)\n",
    "\n",
    "            loss, sl = self.compute_loss(\n",
    "                y_preds=y_preds, labels=labels, batch_size=batch_size, loss_th=loss_th\n",
    "            )\n",
    "            samplewise_losses += sl\n",
    "            if self.config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / self.config.gradient_accumulation_steps\n",
    "            am.update(loss.item(), batch_size)\n",
    "            scaler.scale(loss).backward()\n",
    "            awp.attack_backward(inputs, labels, epoch)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), self.config.max_grad_norm\n",
    "            )\n",
    "            if (step + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                if self.config.batch_scheduler:\n",
    "                    scheduler.step()\n",
    "            if step % self.config.print_freq == 0 or step == (len(tr_dl) - 1):\n",
    "                print(\n",
    "                    \"Epoch: [{0}][{1}/{2}] \"\n",
    "                    \"Elapsed {remain:s} \"\n",
    "                    \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                    \"Grad: {grad_norm:.4f}  \"\n",
    "                    \"LR: {lr:.8f}  \".format(\n",
    "                        epoch + 1,\n",
    "                        step,\n",
    "                        len(tr_dl),\n",
    "                        remain=timeSince(start, float(step + 1) / len(tr_dl)),\n",
    "                        loss=am,\n",
    "                        grad_norm=grad_norm,\n",
    "                        lr=scheduler.get_lr()[0],\n",
    "                    )\n",
    "                )\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"[fold{fold}] loss\": am.val,\n",
    "                    f\"[fold{fold}] lr\": scheduler.get_lr()[0],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if (step + 1) % self.config.n_eval_steps == 0:\n",
    "                model.eval()\n",
    "                avg_vl_loss, predictions = self.infer(model, vl_dl, n_vl)\n",
    "                score, best_th = self.evaluate(\n",
    "                    predictions=predictions,\n",
    "                    valid_texts=valid_texts,\n",
    "                    valid_labels=valid_labels,\n",
    "                    th_range=self.config.th_range,\n",
    "                    th_step=self.config.th_step,\n",
    "                )\n",
    "                self.log(\n",
    "                    {\n",
    "                        f\"[fold{fold}] epoch\": epoch + 1,\n",
    "                        f\"[fold{fold}] step\": step,\n",
    "                        f\"[fold{fold}] avg_val_loss\": avg_vl_loss,\n",
    "                        f\"[fold{fold}] score\": score,\n",
    "                        f\"[fold{fold}] best_th\": best_th,\n",
    "                    }\n",
    "                )\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    self.save_ckpt(fold=fold, model=model, predictions=predictions)\n",
    "                model.train()\n",
    "\n",
    "        return am.avg, best_score, samplewise_losses\n",
    "\n",
    "    @inference_mode()\n",
    "    def infer(self, model: Module, vl_dl: DataLoader, n_vl: int) -> tuple:\n",
    "\n",
    "        model.eval()\n",
    "        losses = AverageMeter()\n",
    "        preds = []\n",
    "        for inputs, labels in vl_dl:\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            batch_size = labels.size(0)\n",
    "            y_preds = model(inputs)\n",
    "            loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "            loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "            if self.config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / self.config.gradient_accumulation_steps\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            preds.append(y_preds.sigmoid().to(\"cpu\").numpy())\n",
    "        predictions = np.concatenate(preds).reshape((n_vl, self.config.max_len))\n",
    "\n",
    "        return losses.avg, predictions\n",
    "\n",
    "    def create_dl(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        feature_text_max_len: int,\n",
    "        pn_history_max_len: int,\n",
    "        is_train: bool,\n",
    "        seed: int,\n",
    "    ) -> DataLoader:\n",
    "\n",
    "        ds = TrainDataset(\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_len=self.config.max_len,\n",
    "            feature_text_max_len=feature_text_max_len,\n",
    "            pn_history_max_len=pn_history_max_len,\n",
    "            df=df,\n",
    "        )\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed + int(is_train))\n",
    "\n",
    "        return DataLoader(\n",
    "            ds,\n",
    "            batch_size=self.config.batch_size\n",
    "            if is_train\n",
    "            else self.config.batch_size * 2,\n",
    "            shuffle=is_train,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=is_train,\n",
    "            generator=g,\n",
    "        )\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        predictions: ndarray,\n",
    "        valid_texts: tuple,\n",
    "        valid_labels: ndarray,\n",
    "        th_range: list,\n",
    "        th_step: float = 0.005,\n",
    "    ) -> tuple:\n",
    "\n",
    "        char_probs = get_char_probs(valid_texts, predictions, self.tokenizer)\n",
    "        best_score = -100\n",
    "        for th in np.arange(th_range[0], th_range[1], th_step):\n",
    "            th = np.round(th, 4)\n",
    "            results = get_results(char_probs, valid_texts, th=th)\n",
    "            preds = get_predictions(results)\n",
    "            score = get_score(valid_labels, preds)\n",
    "            if best_score < score:\n",
    "                best_th = th\n",
    "                best_score = score\n",
    "        return best_score, best_th\n",
    "\n",
    "    def save_ckpt(\n",
    "        self, fold: int, model: Module, predictions: ndarray, epoch: int = None\n",
    "    ) -> None:\n",
    "\n",
    "        if epoch is None:\n",
    "            ep_suffix = \"\"\n",
    "        else:\n",
    "            ep_suffix = f\"_epoch{epoch}\"\n",
    "\n",
    "        torch.save(\n",
    "            {\"model\": model.state_dict(), \"predictions\": predictions},\n",
    "            f\"{self.config.output_dir}{self.config.ckpt_name}_fold{fold}{ep_suffix}_best.pth\",\n",
    "        )\n",
    "        self.logger.info(\"model has been saved.\")\n",
    "\n",
    "    def compute_loss_th(self, samplewise_losses: list, fold: int, epoch: int) -> float:\n",
    "\n",
    "        mu_loss = np.mean(samplewise_losses)\n",
    "        std_loss = np.std(samplewise_losses)\n",
    "        loss_th = mu_loss + std_loss * self.config.n_loss_removal_std\n",
    "        joblib.dump(\n",
    "            value=samplewise_losses,\n",
    "            filename=f\"samplewise_losses_f{fold}_e{epoch}.pkl\",\n",
    "            compress=3,\n",
    "        )\n",
    "\n",
    "        return loss_th\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        pl_df: DataFrame,\n",
    "        feature_text_max_len: int,\n",
    "        pn_history_max_len: int,\n",
    "    ) -> None:\n",
    "\n",
    "        oof_df = pd.DataFrame()\n",
    "        for f in range(self.config.n_folds):\n",
    "\n",
    "            self.logger.info(f\"========== fold: {f} training ==========\")\n",
    "\n",
    "            model = CustomModel(\n",
    "                model_name=self.config.model, config_path=None, pretrained=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            tr_df = df[df[\"fold\"] != f].reset_index(drop=True)\n",
    "            if self.config.pl_frac > 0.0:\n",
    "                tr_pl_df = pl_df.loc[pl_df[\"fold\"] == f].sample(\n",
    "                    frac=self.config.pl_frac, random_state=self.config.seed + 1\n",
    "                )\n",
    "                self.logger.info(f\"{len(tr_pl_df)} pseudo labeled data was sampled.\")\n",
    "                tr_df = pd.concat((tr_df, tr_pl_df)).sample(\n",
    "                    frac=1.0, random_state=self.config.seed\n",
    "                )\n",
    "            tr_dl = self.create_dl(\n",
    "                df=tr_df,\n",
    "                feature_text_max_len=feature_text_max_len,\n",
    "                pn_history_max_len=pn_history_max_len,\n",
    "                is_train=True,\n",
    "                seed=self.config.seed,\n",
    "            )\n",
    "            num_train_steps = int(\n",
    "                len(tr_df) / self.config.batch_size * self.config.epochs\n",
    "            )\n",
    "\n",
    "            vl_df = df[df[\"fold\"] == f].reset_index(drop=True)\n",
    "            vl_dl = self.create_dl(\n",
    "                df=vl_df,\n",
    "                feature_text_max_len=feature_text_max_len,\n",
    "                pn_history_max_len=pn_history_max_len,\n",
    "                is_train=False,\n",
    "                seed=self.config.seed,\n",
    "            )\n",
    "            valid_texts = vl_df[\"pn_history\"].to_numpy()\n",
    "            valid_labels = create_labels_for_scoring(vl_df)\n",
    "\n",
    "            optimizer_parameters = get_optimizer_params(\n",
    "                model,\n",
    "                encoder_lr=self.config.encoder_lr,\n",
    "                decoder_lr=self.config.decoder_lr,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "            )\n",
    "            optimizer = AdamW(\n",
    "                optimizer_parameters,\n",
    "                lr=self.config.encoder_lr,\n",
    "                eps=self.config.eps,\n",
    "                betas=self.config.betas,\n",
    "            )\n",
    "            scheduler = get_scheduler(\n",
    "                scheduler=self.config.scheduler,\n",
    "                optimizer=optimizer,\n",
    "                num_warmup_steps=self.config.num_warmup_steps,\n",
    "                num_train_steps=num_train_steps,\n",
    "                num_cycles=self.config.num_cycles,\n",
    "            )\n",
    "\n",
    "            best_score = 0\n",
    "            for epoch in range(self.config.epochs):\n",
    "\n",
    "                if epoch == self.config.loss_removal_start_ep:\n",
    "                    loss_th = self.compute_loss_th(\n",
    "                        samplewise_losses=samplewise_losses, fold=f, epoch=epoch\n",
    "                    )\n",
    "                    self.logger.info(f\"Loss th: {loss_th}\")\n",
    "\n",
    "                (\n",
    "                    avg_tr_loss,\n",
    "                    stepwise_best_score,\n",
    "                    samplewise_losses,\n",
    "                ) = self.train_with_eval(\n",
    "                    model,\n",
    "                    f,\n",
    "                    (tr_dl, vl_dl),\n",
    "                    optimizer,\n",
    "                    epoch,\n",
    "                    scheduler,\n",
    "                    loss_th if epoch >= self.config.loss_removal_start_ep else None,\n",
    "                    valid_texts,\n",
    "                    valid_labels,\n",
    "                    len(vl_df),\n",
    "                    best_score,\n",
    "                )\n",
    "\n",
    "                avg_vl_loss, predictions = self.infer(model, vl_dl, len(vl_df))\n",
    "                score, best_th = self.evaluate(\n",
    "                    predictions=predictions,\n",
    "                    valid_texts=valid_texts,\n",
    "                    valid_labels=valid_labels,\n",
    "                    th_range=self.config.th_range,\n",
    "                    th_step=self.config.th_step,\n",
    "                )\n",
    "\n",
    "                self.log(\n",
    "                    {\n",
    "                        f\"[fold{f}] epoch\": epoch + 1,\n",
    "                        f\"[fold{f}] avg_train_loss\": avg_tr_loss,\n",
    "                        f\"[fold{f}] avg_val_loss\": avg_vl_loss,\n",
    "                        f\"[fold{f}] score\": score,\n",
    "                        f\"[fold{f}] best_th\": best_th,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if score > stepwise_best_score:\n",
    "                    best_score = score\n",
    "\n",
    "                self.save_ckpt(\n",
    "                    fold=f, model=model, predictions=predictions, epoch=epoch\n",
    "                )\n",
    "\n",
    "            predictions = torch.load(\n",
    "                f\"{self.config.output_dir}{self.config.ckpt_name}_fold{f}_best.pth\",\n",
    "                map_location=torch.device(\"cpu\"),\n",
    "            )[\"predictions\"]\n",
    "            vl_df[[i for i in range(self.config.max_len)]] = predictions\n",
    "            oof_df = pd.concat([oof_df, vl_df])\n",
    "\n",
    "            self.logger.info(f\"========== fold: {f} result ==========\")\n",
    "\n",
    "            score, th = get_result(vl_df, self.tokenizer, self.config.max_len)\n",
    "            self.log(\n",
    "                {f\"[fold{f}] overall score\": score, f\"[fold{f}] overall best th\": th}\n",
    "            )\n",
    "            oof_df.to_pickle(f\"{self.config.output_dir}oof_df_fold{f}.pkl\")\n",
    "\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        self.logger.info(f\"========== CV ==========\")\n",
    "        score, th = get_result(oof_df, self.tokenizer, self.config.max_len)\n",
    "        self.log({f\"overall score\": score, f\"overall best th\": th})\n",
    "        oof_df.to_pickle(self.config.output_dir + \"oof_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db38bc60",
    "outputId": "1e4c4719-1e76-4cdb-e979-4f10b39b0896"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Some weights of the model checkpoint at ../input/roberta-large-self-supervised-learning-9epoch were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/roberta-large-self-supervised-learning-9epoch and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "153147 pseudo labeled data was sampled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/13656] Elapsed 0m 1s (remain 416m 11s) Loss: 3.8356(3.8356) Grad: inf  LR: 0.00002500  \n",
      "Epoch: [1][1000/13656] Elapsed 5m 19s (remain 67m 17s) Loss: 0.0070(0.0420) Grad: 1140.2079  LR: 0.00002499  \n",
      "Epoch: [1][2000/13656] Elapsed 10m 36s (remain 61m 50s) Loss: 0.0019(0.0254) Grad: 383.1989  LR: 0.00002496  \n",
      "Epoch: [1][3000/13656] Elapsed 15m 54s (remain 56m 29s) Loss: 0.0015(0.0192) Grad: 434.3897  LR: 0.00002492  \n",
      "Epoch: [1][4000/13656] Elapsed 21m 12s (remain 51m 10s) Loss: 0.0009(0.0160) Grad: 525.9743  LR: 0.00002485  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 4999 Avg_val_loss: 0.014202040127654384 Score: 0.88323747344422 Best_th: 0.4\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][5000/13656] Elapsed 27m 39s (remain 47m 51s) Loss: 0.0031(0.0139) Grad: 855.8945  LR: 0.00002477  \n",
      "Epoch: [1][6000/13656] Elapsed 32m 57s (remain 42m 1s) Loss: 0.0038(0.0126) Grad: 1816.6116  LR: 0.00002467  \n",
      "Epoch: [1][7000/13656] Elapsed 38m 14s (remain 36m 21s) Loss: 0.0022(0.0115) Grad: 3744.0078  LR: 0.00002455  \n",
      "Epoch: [1][8000/13656] Elapsed 43m 32s (remain 30m 46s) Loss: 0.0361(0.0107) Grad: 12919.3018  LR: 0.00002442  \n",
      "Epoch: [1][9000/13656] Elapsed 48m 50s (remain 25m 15s) Loss: 0.0033(0.0101) Grad: 10314.8721  LR: 0.00002426  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 9999 Avg_val_loss: 0.013903780655635821 Score: 0.8842404164694747 Best_th: 0.6\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][10000/13656] Elapsed 55m 27s (remain 20m 16s) Loss: 0.0043(0.0095) Grad: 19610.0801  LR: 0.00002409  \n",
      "Epoch: [1][11000/13656] Elapsed 60m 45s (remain 14m 39s) Loss: 0.0025(0.0091) Grad: 20611.0039  LR: 0.00002390  \n",
      "Epoch: [1][12000/13656] Elapsed 66m 3s (remain 9m 6s) Loss: 0.0007(0.0087) Grad: 5864.2539  LR: 0.00002370  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][13000/13656] Elapsed 71m 20s (remain 3m 35s) Loss: 0.0003(0.0084) Grad: 2494.4326  LR: 0.00002348  \n",
      "Epoch: [1][13655/13656] Elapsed 74m 48s (remain 0m 0s) Loss: 0.0021(0.0082) Grad: 4540.9014  LR: 0.00002333  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Avg_train_loss: 0.00821489075635273 Avg_val_loss: 0.018825264070052784 Score: 0.884778728380624 Best_th: 0.46\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/13656] Elapsed 0m 0s (remain 128m 56s) Loss: 0.0091(0.0091) Grad: 13423.8887  LR: 0.00002333  \n",
      "Epoch: [2][1000/13656] Elapsed 5m 18s (remain 67m 4s) Loss: 0.0034(0.0038) Grad: 9737.2393  LR: 0.00002308  \n",
      "Epoch: [2][2000/13656] Elapsed 10m 35s (remain 61m 44s) Loss: 0.0002(0.0037) Grad: 3934.7253  LR: 0.00002281  \n",
      "Epoch: [2][3000/13656] Elapsed 15m 53s (remain 56m 25s) Loss: 0.0050(0.0038) Grad: 39883.5625  LR: 0.00002254  \n",
      "Epoch: [2][4000/13656] Elapsed 21m 11s (remain 51m 8s) Loss: 0.0034(0.0037) Grad: 96839.3594  LR: 0.00002224  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Step: 4999 Avg_val_loss: 0.020039487043981785 Score: 0.8845323628476386 Best_th: 0.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][5000/13656] Elapsed 27m 36s (remain 47m 47s) Loss: 0.0014(0.0037) Grad: 50451.5234  LR: 0.00002194  \n",
      "Epoch: [2][6000/13656] Elapsed 32m 54s (remain 41m 58s) Loss: 0.0025(0.0037) Grad: 39261.1289  LR: 0.00002161  \n",
      "Epoch: [2][7000/13656] Elapsed 38m 12s (remain 36m 19s) Loss: 0.0028(0.0037) Grad: 58709.4531  LR: 0.00002128  \n",
      "Epoch: [2][8000/13656] Elapsed 43m 30s (remain 30m 45s) Loss: 0.0003(0.0037) Grad: 1909.3300  LR: 0.00002093  \n",
      "Epoch: [2][9000/13656] Elapsed 48m 48s (remain 25m 14s) Loss: 0.0010(0.0038) Grad: 3523.1426  LR: 0.00002057  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Step: 9999 Avg_val_loss: 0.018787393141277395 Score: 0.8855743264460214 Best_th: 0.255\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][10000/13656] Elapsed 55m 25s (remain 20m 15s) Loss: 0.0012(0.0038) Grad: 3678.6914  LR: 0.00002020  \n",
      "Epoch: [2][11000/13656] Elapsed 60m 43s (remain 14m 39s) Loss: 0.0021(0.0038) Grad: 9306.9922  LR: 0.00001982  \n",
      "Epoch: [2][12000/13656] Elapsed 66m 1s (remain 9m 6s) Loss: 0.0017(0.0037) Grad: 5163.6904  LR: 0.00001942  \n",
      "Epoch: [2][13000/13656] Elapsed 71m 19s (remain 3m 35s) Loss: 0.0000(0.0038) Grad: 42.4909  LR: 0.00001902  \n",
      "Epoch: [2][13655/13656] Elapsed 74m 47s (remain 0m 0s) Loss: 0.0019(0.0038) Grad: 18120.9629  LR: 0.00001875  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Avg_train_loss: 0.0037691382251168358 Avg_val_loss: 0.017920165205882355 Score: 0.8830692140345812 Best_th: 0.285\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/13656] Elapsed 0m 0s (remain 127m 51s) Loss: 0.0023(0.0023) Grad: 9619.8076  LR: 0.00001875  \n",
      "Epoch: [3][1000/13656] Elapsed 5m 18s (remain 67m 6s) Loss: 0.0000(0.0030) Grad: 174.1079  LR: 0.00001833  \n",
      "Epoch: [3][2000/13656] Elapsed 10m 36s (remain 61m 46s) Loss: 0.0540(0.0031) Grad: 32606.1152  LR: 0.00001790  \n",
      "Epoch: [3][3000/13656] Elapsed 15m 54s (remain 56m 27s) Loss: 0.0080(0.0031) Grad: 8480.2021  LR: 0.00001747  \n",
      "Epoch: [3][4000/13656] Elapsed 21m 12s (remain 51m 10s) Loss: 0.0014(0.0031) Grad: 6362.7905  LR: 0.00001702  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Step: 4999 Avg_val_loss: 0.018591364617557168 Score: 0.8874497909664728 Best_th: 0.25\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][5000/13656] Elapsed 27m 49s (remain 48m 9s) Loss: 0.0057(0.0031) Grad: 32587.4512  LR: 0.00001657  \n",
      "Epoch: [3][6000/13656] Elapsed 33m 7s (remain 42m 15s) Loss: 0.0030(0.0031) Grad: 197105.6250  LR: 0.00001612  \n",
      "Epoch: [3][7000/13656] Elapsed 38m 25s (remain 36m 31s) Loss: 0.0001(0.0030) Grad: 10488.8438  LR: 0.00001566  \n",
      "Epoch: [3][8000/13656] Elapsed 43m 43s (remain 30m 54s) Loss: 0.0016(0.0030) Grad: 9940.6934  LR: 0.00001519  \n",
      "Epoch: [3][9000/13656] Elapsed 49m 1s (remain 25m 21s) Loss: 0.0020(0.0030) Grad: 2380.6086  LR: 0.00001472  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Step: 9999 Avg_val_loss: 0.01696092901165252 Score: 0.8864375677704563 Best_th: 0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][10000/13656] Elapsed 55m 26s (remain 20m 15s) Loss: 0.0002(0.0030) Grad: 969.2994  LR: 0.00001425  \n",
      "Epoch: [3][11000/13656] Elapsed 60m 44s (remain 14m 39s) Loss: 0.0058(0.0030) Grad: 21349.9746  LR: 0.00001377  \n",
      "Epoch: [3][12000/13656] Elapsed 66m 2s (remain 9m 6s) Loss: 0.0000(0.0030) Grad: 681.5616  LR: 0.00001329  \n",
      "Epoch: [3][13000/13656] Elapsed 71m 20s (remain 3m 35s) Loss: 0.0049(0.0030) Grad: 28889.2617  LR: 0.00001281  \n",
      "Epoch: [3][13655/13656] Elapsed 74m 48s (remain 0m 0s) Loss: 0.0010(0.0030) Grad: 26468.6855  LR: 0.00001250  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Avg_train_loss: 0.0029630403700284854 Avg_val_loss: 0.021269874523094222 Score: 0.8875015555647736 Best_th: 0.265\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/13656] Elapsed 0m 0s (remain 127m 55s) Loss: 0.0057(0.0057) Grad: 16258.9844  LR: 0.00001250  \n",
      "Epoch: [4][1000/13656] Elapsed 5m 18s (remain 67m 7s) Loss: 0.0001(0.0023) Grad: 352.5764  LR: 0.00001202  \n",
      "Epoch: [4][2000/13656] Elapsed 10m 36s (remain 61m 46s) Loss: 0.0015(0.0023) Grad: 31486.2363  LR: 0.00001154  \n",
      "Epoch: [4][3000/13656] Elapsed 15m 54s (remain 56m 28s) Loss: 0.0039(0.0023) Grad: 51523.2305  LR: 0.00001106  \n",
      "Epoch: [4][4000/13656] Elapsed 21m 12s (remain 51m 10s) Loss: 0.0001(0.0023) Grad: 7733.4028  LR: 0.00001059  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "Epoch: 4 Step: 4999 Avg_val_loss: 0.022354164338602288 Score: 0.8886508299704623 Best_th: 0.46\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][5000/13656] Elapsed 27m 48s (remain 48m 8s) Loss: 0.0008(0.0023) Grad: 65936.7812  LR: 0.00001012  \n",
      "Epoch: [4][6000/13656] Elapsed 33m 6s (remain 42m 14s) Loss: 0.0070(0.0023) Grad: 102511.2266  LR: 0.00000965  \n",
      "Epoch: [4][7000/13656] Elapsed 38m 24s (remain 36m 30s) Loss: 0.0000(0.0023) Grad: 25.0012  LR: 0.00000918  \n",
      "Epoch: [4][8000/13656] Elapsed 43m 42s (remain 30m 53s) Loss: 0.0008(0.0023) Grad: 58400.5586  LR: 0.00000873  \n",
      "Epoch: [4][9000/13656] Elapsed 49m 0s (remain 25m 20s) Loss: 0.0019(0.0023) Grad: 11750.3936  LR: 0.00000827  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Step: 9999 Avg_val_loss: 0.021141241359104355 Score: 0.8884329208895522 Best_th: 0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][10000/13656] Elapsed 55m 26s (remain 20m 15s) Loss: 0.0013(0.0023) Grad: 8450.2793  LR: 0.00000782  \n",
      "Epoch: [4][11000/13656] Elapsed 60m 44s (remain 14m 39s) Loss: 0.0032(0.0022) Grad: 14292.5410  LR: 0.00000738  \n",
      "Epoch: [4][12000/13656] Elapsed 66m 2s (remain 9m 6s) Loss: 0.0003(0.0022) Grad: 4259.1152  LR: 0.00000695  \n",
      "Epoch: [4][13000/13656] Elapsed 71m 20s (remain 3m 35s) Loss: 0.0000(0.0022) Grad: 87.0360  LR: 0.00000652  \n",
      "Epoch: [4][13655/13656] Elapsed 74m 48s (remain 0m 0s) Loss: 0.0002(0.0022) Grad: 12193.9023  LR: 0.00000625  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Avg_train_loss: 0.002241653030469413 Avg_val_loss: 0.023162963242348206 Score: 0.8874166675333589 Best_th: 0.315\n",
      "model has been saved.\n",
      "Loss th: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/13656] Elapsed 0m 0s (remain 129m 32s) Loss: 0.0013(0.0013) Grad: 1466.6405  LR: 0.00000625  \n",
      "Epoch: [5][1000/13656] Elapsed 5m 18s (remain 67m 8s) Loss: 0.0008(0.0018) Grad: 10687.3320  LR: 0.00000584  \n",
      "Epoch: [5][2000/13656] Elapsed 10m 36s (remain 61m 49s) Loss: 0.0017(0.0017) Grad: 28395.7715  LR: 0.00000544  \n",
      "Epoch: [5][3000/13656] Elapsed 15m 55s (remain 56m 31s) Loss: 0.0022(0.0017) Grad: 20291.3262  LR: 0.00000505  \n",
      "Epoch: [5][4000/13656] Elapsed 21m 13s (remain 51m 13s) Loss: 0.0000(0.0017) Grad: 515.0031  LR: 0.00000467  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Step: 4999 Avg_val_loss: 0.025187118932201 Score: 0.8867951997670598 Best_th: 0.285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][5000/13656] Elapsed 27m 39s (remain 47m 51s) Loss: 0.0000(0.0018) Grad: 18.9422  LR: 0.00000430  \n",
      "Epoch: [5][6000/13656] Elapsed 32m 57s (remain 42m 1s) Loss: 0.0000(0.0018) Grad: 112.2714  LR: 0.00000395  \n",
      "Epoch: [5][7000/13656] Elapsed 38m 15s (remain 36m 21s) Loss: 0.0002(0.0018) Grad: 23956.8672  LR: 0.00000360  \n",
      "Epoch: [5][8000/13656] Elapsed 43m 33s (remain 30m 47s) Loss: 0.0000(0.0018) Grad: 2489.3157  LR: 0.00000327  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][9000/13656] Elapsed 48m 51s (remain 25m 16s) Loss: 0.0046(0.0018) Grad: 607601.9375  LR: 0.00000296  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Step: 9999 Avg_val_loss: 0.022587331487973046 Score: 0.8869727820486183 Best_th: 0.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][10000/13656] Elapsed 55m 17s (remain 20m 12s) Loss: 0.0000(0.0018) Grad: 30.5150  LR: 0.00000265  \n",
      "Epoch: [5][11000/13656] Elapsed 60m 35s (remain 14m 37s) Loss: 0.0000(0.0018) Grad: 70.1236  LR: 0.00000237  \n",
      "Epoch: [5][12000/13656] Elapsed 65m 54s (remain 9m 5s) Loss: 0.0018(0.0018) Grad: 18279.8945  LR: 0.00000209  \n",
      "Epoch: [5][13000/13656] Elapsed 71m 12s (remain 3m 35s) Loss: 0.0004(0.0018) Grad: 16659.6934  LR: 0.00000184  \n",
      "Epoch: [5][13655/13656] Elapsed 74m 40s (remain 0m 0s) Loss: 0.0012(0.0018) Grad: 2772.9197  LR: 0.00000167  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Avg_train_loss: 0.001773986100543657 Avg_val_loss: 0.024135945802424234 Score: 0.887910627517754 Best_th: 0.25\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/13656] Elapsed 0m 0s (remain 135m 49s) Loss: 0.0003(0.0003) Grad: 8044.1021  LR: 0.00000167  \n",
      "Epoch: [6][1000/13656] Elapsed 5m 19s (remain 67m 12s) Loss: 0.0042(0.0014) Grad: 19481.4551  LR: 0.00000144  \n",
      "Epoch: [6][2000/13656] Elapsed 10m 37s (remain 61m 50s) Loss: 0.0012(0.0014) Grad: 2774.9893  LR: 0.00000123  \n",
      "Epoch: [6][3000/13656] Elapsed 15m 55s (remain 56m 31s) Loss: 0.0000(0.0014) Grad: 728.6712  LR: 0.00000103  \n",
      "Epoch: [6][4000/13656] Elapsed 21m 13s (remain 51m 12s) Loss: 0.0000(0.0015) Grad: 171.1012  LR: 0.00000085  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Step: 4999 Avg_val_loss: 0.02330448452734968 Score: 0.8879324649464864 Best_th: 0.25\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][5000/13656] Elapsed 27m 51s (remain 48m 12s) Loss: 0.0013(0.0015) Grad: 48327.2148  LR: 0.00000068  \n",
      "Epoch: [6][6000/13656] Elapsed 33m 9s (remain 42m 17s) Loss: 0.0000(0.0015) Grad: 136.5405  LR: 0.00000053  \n",
      "Epoch: [6][7000/13656] Elapsed 38m 27s (remain 36m 33s) Loss: 0.0000(0.0015) Grad: 161.5989  LR: 0.00000040  \n",
      "Epoch: [6][8000/13656] Elapsed 43m 45s (remain 30m 55s) Loss: 0.0001(0.0015) Grad: 2226.0266  LR: 0.00000029  \n",
      "Epoch: [6][9000/13656] Elapsed 49m 4s (remain 25m 22s) Loss: 0.0000(0.0015) Grad: 210.8673  LR: 0.00000020  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Step: 9999 Avg_val_loss: 0.024639160139225683 Score: 0.8878286088502199 Best_th: 0.295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][10000/13656] Elapsed 55m 29s (remain 20m 16s) Loss: 0.0027(0.0015) Grad: 9307.8867  LR: 0.00000012  \n",
      "Epoch: [6][11000/13656] Elapsed 60m 47s (remain 14m 40s) Loss: 0.0000(0.0015) Grad: 2210.9309  LR: 0.00000006  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][12000/13656] Elapsed 66m 5s (remain 9m 6s) Loss: 0.0000(0.0015) Grad: 105.0568  LR: 0.00000003  \n",
      "Epoch: [6][13000/13656] Elapsed 71m 24s (remain 3m 35s) Loss: 0.0021(0.0015) Grad: 208582.3281  LR: 0.00000000  \n",
      "Epoch: [6][13655/13656] Elapsed 74m 52s (remain 0m 0s) Loss: 0.0000(0.0015) Grad: 1035.7400  LR: 0.00000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Avg_train_loss: 0.0015139481698912785 Avg_val_loss: 0.024707308787872063 Score: 0.8877570112887466 Best_th: 0.29\n",
      "model has been saved.\n",
      "========== fold: 0 result ==========\n",
      "Score: 0.887729653337767 Best th: 0.32\n",
      "========== fold: 1 training ==========\n",
      "Some weights of the model checkpoint at ../input/roberta-large-self-supervised-learning-9epoch were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/roberta-large-self-supervised-learning-9epoch and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "153155 pseudo labeled data was sampled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/13656] Elapsed 0m 0s (remain 126m 9s) Loss: 0.7402(0.7402) Grad: inf  LR: 0.00002500  \n",
      "Epoch: [1][1000/13656] Elapsed 5m 18s (remain 67m 11s) Loss: 0.0047(0.0201) Grad: 1010.5608  LR: 0.00002499  \n",
      "Epoch: [1][2000/13656] Elapsed 10m 37s (remain 61m 50s) Loss: 0.0155(0.0140) Grad: 9731.7061  LR: 0.00002496  \n",
      "Epoch: [1][3000/13656] Elapsed 15m 55s (remain 56m 31s) Loss: 0.0034(0.0116) Grad: 710.2477  LR: 0.00002492  \n",
      "Epoch: [1][4000/13656] Elapsed 21m 13s (remain 51m 12s) Loss: 0.0087(0.0104) Grad: 1363.5912  LR: 0.00002485  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 4999 Avg_val_loss: 0.01346043304637908 Score: 0.8738036085203437 Best_th: 0.37\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][5000/13656] Elapsed 27m 41s (remain 47m 55s) Loss: 0.0019(0.0095) Grad: 431.2208  LR: 0.00002477  \n",
      "Epoch: [1][6000/13656] Elapsed 32m 59s (remain 42m 5s) Loss: 0.0124(0.0089) Grad: 1344.6136  LR: 0.00002467  \n",
      "Epoch: [1][7000/13656] Elapsed 38m 18s (remain 36m 24s) Loss: 0.0004(0.0084) Grad: 156.6679  LR: 0.00002455  \n",
      "Epoch: [1][8000/13656] Elapsed 43m 36s (remain 30m 49s) Loss: 0.0001(0.0079) Grad: 61.3435  LR: 0.00002442  \n",
      "Epoch: [1][9000/13656] Elapsed 48m 54s (remain 25m 17s) Loss: 0.0030(0.0076) Grad: 1446.7478  LR: 0.00002426  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 9999 Avg_val_loss: 0.017271542960120092 Score: 0.8833362300763717 Best_th: 0.58\n",
      "model has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][10000/13656] Elapsed 55m 32s (remain 20m 17s) Loss: 0.0003(0.0072) Grad: 629.2804  LR: 0.00002409  \n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    logger=logger)\n",
    "\n",
    "trainer.run(\n",
    "    df=df_train,\n",
    "    pl_df=pl_train,\n",
    "    feature_text_max_len=feature_text_max_len, \n",
    "    pn_history_max_len=pn_history_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5a96081-3aa7-4948-be95-67e31d3bd7e6"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8ac591e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
