{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired https://www.kaggle.com/code/yasufuminakama/nbme-deberta-base-baseline-train etc... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7426d7d"
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1366,
     "status": "ok",
     "timestamp": 1650910196447,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "797404f7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.core.display import display\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import yaml\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import pandas as pd\n",
    "import torch\n",
    "from logging import Logger, getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "import wandb\n",
    "from wandb.sdk.wandb_config import Config\n",
    "\n",
    "\n",
    "def init_pandas() -> None:\n",
    "\n",
    "    pd.set_option(\"display.max_rows\", 500)\n",
    "    pd.set_option(\"display.max_columns\", 500)\n",
    "    pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "\n",
    "def get_logger(filename: str) -> Logger:\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def init_wandb(wandb_key: str) -> Config:\n",
    "    secret_value_0 = wandb_key\n",
    "    wandb.login(key=secret_value_0)\n",
    "\n",
    "    loader = yaml.SafeLoader\n",
    "    loader.add_implicit_resolver(\n",
    "        \"tag:yaml.org,2002:float\",\n",
    "        re.compile(\n",
    "            \"\"\"^(?:\n",
    "         [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)?\n",
    "        |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)\n",
    "        |\\\\.[0-9_]+(?:[eE][-+][0-9]+)?\n",
    "        |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\\\.[0-9_]*\n",
    "        |[-+]?\\\\.(?:inf|Inf|INF)\n",
    "        |\\\\.(?:nan|NaN|NAN))$\"\"\",\n",
    "            re.X,\n",
    "        ),\n",
    "        list(\"-+0123456789.\"),\n",
    "    )\n",
    "    with open(f\"./config.yml\") as f:\n",
    "        param = yaml.load(f, Loader=loader)\n",
    "    wandb.init(project=param[\"project\"], config=param)\n",
    "    wandb.config.update(param)\n",
    "    print(f\"run name: {wandb.run.name}\")\n",
    "    return wandb.config\n",
    "\n",
    "\n",
    "def mk_output_dir(path: str) -> None:\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7061,
     "status": "ok",
     "timestamp": 1650910203504,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "73494049-1b8a-4400-a39d-18f47f6c69de",
    "outputId": "662ee360-c60e-467d-80cf-101596e16c89"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from getpass import getpass\n",
    "\n",
    "wandb_key = getpass()\n",
    "config = init_wandb(wandb_key=wandb_key)\n",
    "mk_output_dir(path=config.output_dir)\n",
    "logger = get_logger(filename=config.output_dir + \"train\")\n",
    "seed_everything(seed=config.seed)\n",
    "init_pandas()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "575ae408"
   },
   "source": [
    "# Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1650910203932,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "8771dcad"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def get_score(y_true: ndarray, y_pred: ndarray) -> float:\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def micro_f1(preds: list, truths: list) -> float:\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans: list, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(\n",
    "            np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0\n",
    "        )\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "\n",
    "    return micro_f1(bin_preds, bin_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3294,
     "status": "ok",
     "timestamp": 1650910207225,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "c20fdb1b"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from numpy import ndarray\n",
    "from pandas import DataFrame\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def create_labels_for_scoring(df: DataFrame):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(\n",
    "                f'[[\"{new_lst}\"]]'\n",
    "            )\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(\n",
    "    texts: list, predictions: ndarray, tokenizer: PreTrainedTokenizer\n",
    ") -> list:\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, add_special_tokens=True, return_offsets_mapping=True)\n",
    "        prev_pred = 0\n",
    "        prev_end = -1\n",
    "        for offset_mapping, pred in zip(encoded[\"offset_mapping\"], prediction):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "            if start != prev_end:\n",
    "                results[i][prev_end:start] = (pred + prev_pred) / 2\n",
    "            prev_pred = pred\n",
    "            prev_end = end\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def cluster_elements(xs: list) -> list:\n",
    "    clusters = [[]]\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        return clusters\n",
    "\n",
    "    prev_x = xs[0] - 1\n",
    "    for x in xs:\n",
    "        if x == prev_x + 1:\n",
    "            clusters[-1].append(x)\n",
    "        else:\n",
    "            clusters.append([x])\n",
    "        prev_x = x\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def get_results(char_probs: list, pn_histories: list, th: float = 0.5) -> list:\n",
    "    label_strs = []\n",
    "    for char_prob, pn_history in zip(char_probs, pn_histories):\n",
    "        pos_char_indices = np.where(char_prob > th)[0] + 1\n",
    "        if len(pos_char_indices) > 0 and pos_char_indices[0] == 1:\n",
    "            pos_char_indices = np.hstack([[0], pos_char_indices])\n",
    "        clustered_pos_char_indices = cluster_elements(xs=pos_char_indices)\n",
    "\n",
    "        for i in range(len(clustered_pos_char_indices)):\n",
    "            # 1文字目がspaceの場合\n",
    "            if len(clustered_pos_char_indices[i]) > 0:\n",
    "                target_idx = clustered_pos_char_indices[i][0] - 1\n",
    "                if target_idx > -1 and pn_history[target_idx] != \" \":\n",
    "                    clustered_pos_char_indices[i] = np.hstack(\n",
    "                        [[target_idx], clustered_pos_char_indices[i]]\n",
    "                    )\n",
    "            # 1文字目が\\r\\nの場合\n",
    "            if len(clustered_pos_char_indices[i]) > 0:\n",
    "                if clustered_pos_char_indices[i][0] > 0 and clustered_pos_char_indices[\n",
    "                    i\n",
    "                ][0] + 2 < len(pn_history):\n",
    "                    if (\n",
    "                        pn_history[\n",
    "                            clustered_pos_char_indices[i][\n",
    "                                0\n",
    "                            ] : clustered_pos_char_indices[i][0]\n",
    "                            + 2\n",
    "                        ]\n",
    "                        == \"\\r\\n\"\n",
    "                    ):\n",
    "                        clustered_pos_char_indices[i] = clustered_pos_char_indices[i][\n",
    "                            2:\n",
    "                        ]\n",
    "            # 最後の2文字が\\n-の場合\n",
    "            if len(clustered_pos_char_indices[i]) > 0:\n",
    "                target_idx = clustered_pos_char_indices[i][-1] - 2\n",
    "                if target_idx > 0 and pn_history[target_idx : target_idx + 2] == \"\\n-\":\n",
    "                    clustered_pos_char_indices[i] = clustered_pos_char_indices[i][:-2]\n",
    "\n",
    "        pos_char_spans = []\n",
    "        if len(clustered_pos_char_indices[0]) != 0:\n",
    "            for x in clustered_pos_char_indices:\n",
    "                if len(x) > 0:\n",
    "                    pos_char_spans.append([x[0], x[-1]])\n",
    "        label_strs.append(\";\".join([f\"{x[0]} {x[1]}\" for x in pos_char_spans]))\n",
    "\n",
    "    return label_strs\n",
    "\n",
    "\n",
    "def get_predictions(results: list) -> list:\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def get_result(\n",
    "    df_oof: DataFrame, tokenizer: PreTrainedTokenizer, max_len: int\n",
    ") -> tuple:\n",
    "    labels = create_labels_for_scoring(df_oof)\n",
    "    predictions = df_oof[[i for i in range(max_len)]].to_numpy()\n",
    "    char_probs = get_char_probs(df_oof[\"pn_history\"].to_numpy(), predictions, tokenizer)\n",
    "    pn_histories = df_oof[\"pn_history\"].to_list()\n",
    "\n",
    "    score = -100\n",
    "    for th in np.arange(0.3, 0.7, 0.005):\n",
    "        th = np.round(th, 4)\n",
    "        results = get_results(char_probs, pn_histories, th=th)\n",
    "        preds = get_predictions(results)\n",
    "        tmp_score = get_score(labels, preds)\n",
    "        if tmp_score > score:\n",
    "            best_th = th\n",
    "            score = tmp_score\n",
    "\n",
    "    return score, best_th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dc99c98"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T23:46:53.45023Z",
     "iopub.status.busy": "2022-04-26T23:46:53.449418Z",
     "iopub.status.idle": "2022-04-26T23:46:53.467497Z",
     "shell.execute_reply": "2022-04-26T23:46:53.466841Z",
     "shell.execute_reply.started": "2022-04-26T23:46:53.450185Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess_features(features: DataFrame) -> None:\n",
    "    features.loc[27, \"feature_text\"] = \"Last-Pap-smear-1-year-ago\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1650910208040,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "f93fc583",
    "outputId": "b46aba9f-5cc3-4be5-f4cc-5fb5f67cdbd7"
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = Path(\"../../input/\")\n",
    "df_train = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "df_train[\"annotation\"] = df_train[\"annotation\"].map(lambda x: ast.literal_eval(x))\n",
    "df_train[\"location\"] = df_train[\"location\"].map(lambda x: ast.literal_eval(x))\n",
    "\n",
    "features = pd.read_csv(INPUT_DIR / \"features.csv\")\n",
    "preprocess_features(features)\n",
    "\n",
    "patient_notes = pd.read_csv(INPUT_DIR / \"patient_notes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650910208041,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "ce0adc5c",
    "outputId": "61045f17-e17e-4555-c577-198ec790424a"
   },
   "outputs": [],
   "source": [
    "df_train = df_train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "df_train = df_train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1650910208674,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "66337109"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "def correct_annotation(df_train:DataFrame) -> None:\n",
    "    df_train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "    df_train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "    df_train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "    df_train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "    df_train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "    df_train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "    df_train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "    df_train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "    df_train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "    df_train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "    df_train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "    df_train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "    df_train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "    df_train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "    df_train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "    df_train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "    df_train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "    df_train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "    df_train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "    df_train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "    df_train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "    df_train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "    df_train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "    df_train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "    df_train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "    df_train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "    df_train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "    df_train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "    df_train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "    df_train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "    df_train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "    df_train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "    df_train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "    df_train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "    df_train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "    df_train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "    df_train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "    df_train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "    df_train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "    df_train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "    df_train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "    df_train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "    df_train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "    df_train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "    df_train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "    df_train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "    df_train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "    df_train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "    df_train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "    df_train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "    df_train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "    df_train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "    df_train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "    df_train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "    df_train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "    df_train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "    df_train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "    df_train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "    df_train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "    df_train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "    df_train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "    df_train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "    df_train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "    df_train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "    df_train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "    df_train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "    df_train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "    df_train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "    df_train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "    df_train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "    df_train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "    df_train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "    df_train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "    df_train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "    df_train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "    df_train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "    df_train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "    df_train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "    df_train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "    df_train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "    df_train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "    df_train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "    df_train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "    df_train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1650910208674,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "ec3a7b5f",
    "outputId": "7d119c39-2b53-46c0-b5ca-52f72a6fc104"
   },
   "outputs": [],
   "source": [
    "df_train['annotation_length'] = df_train['annotation'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_train = pd.read_csv('./pl_train_with_folds.csv')\n",
    "pl_train['location'] = pl_train['location'].fillna('[]')\n",
    "pl_train['location'] = pl_train['location'].map(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6c6437f"
   },
   "source": [
    "# CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650910208674,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "ed511f87",
    "outputId": "ffe7d0af-603d-44f5-f1cc-ba23c958d0ca"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "kf = GroupKFold(n_splits=config.n_folds)\n",
    "groups = df_train['pn_num'].to_numpy()\n",
    "df_train.loc[:, 'fold'] = -1\n",
    "for n, (train_index, val_index) in enumerate(kf.split(df_train, df_train['location'], groups)):\n",
    "    df_train.loc[val_index, 'fold'] = n\n",
    "display(df_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1650910208675,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "526e4ac9"
   },
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    display(df_train.groupby('fold').size())\n",
    "    df_train = df_train.sample(n=500, random_state=0).reset_index(drop=True)\n",
    "    display(df_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11e2590f"
   },
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3472,
     "status": "ok",
     "timestamp": 1650910212142,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "819ca57b",
    "outputId": "9d5b5c9e-0186-44f5-e974-4c83e30bf318"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.tokenizer,\n",
    "    trim_offsets=False\n",
    ")\n",
    "tokenizer.save_pretrained(config.output_dir+'tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26136,
     "status": "ok",
     "timestamp": 1650910238270,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "7d057ad3",
    "outputId": "92e42c7c-53d0-4ff7-e80f-0025d3808755"
   },
   "outputs": [],
   "source": [
    "pn_history_lengths = []\n",
    "for text in patient_notes[\"pn_history\"].fillna(\"\").to_list():\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    pn_history_lengths.append(length)\n",
    "pn_history_max_len = max(pn_history_lengths)\n",
    "logger.info(f\"pn_history max(lengths): {pn_history_max_len}\")\n",
    "\n",
    "features_lengths = []\n",
    "for text in features[\"feature_text\"].fillna(\"\").to_list():\n",
    "    length = len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "    features_lengths.append(length)\n",
    "feature_text_max_len = max(features_lengths)\n",
    "logger.info(f\"feature_text max(lengths): {feature_text_max_len}\")\n",
    "\n",
    "config.max_len = pn_history_max_len + feature_text_max_len + 3\n",
    "logger.info(f\"max_len: {config.max_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06f9f616"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650910238271,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "1387f638"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_len: int,\n",
    "        feature_text_max_len: int,\n",
    "        pn_history_max_len: int,\n",
    "        df: DataFrame,\n",
    "    ) -> None:\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.feature_text_max_len = feature_text_max_len\n",
    "        self.pn_history_max_len = pn_history_max_len\n",
    "        self.feature_texts = df[\"feature_text\"].to_numpy()\n",
    "        self.pn_historys = df[\"pn_history\"].to_numpy()\n",
    "        self.annotation_lengths = df[\"annotation_length\"].to_numpy()\n",
    "        self.locations = df[\"location\"].to_numpy()\n",
    "\n",
    "    def prepare_input_with_fixed_position(\n",
    "        self, pn_history: str, feature_text: str\n",
    "    ) -> dict:\n",
    "\n",
    "        pn_history_token = self.tokenizer(\n",
    "            pn_history,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.pn_history_max_len + 2,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "\n",
    "        feature_text_token = self.tokenizer(\n",
    "            feature_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.feature_text_max_len + 2,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in feature_text_token.items():\n",
    "            feature_text_token[k] = v[1:]\n",
    "\n",
    "        token = {\n",
    "            \"input_ids\": pn_history_token[\"input_ids\"]\n",
    "            + feature_text_token[\"input_ids\"],\n",
    "            \"attention_mask\": pn_history_token[\"attention_mask\"]\n",
    "            + feature_text_token[\"attention_mask\"],\n",
    "            #             'token_type_ids': pn_history_token['token_type_ids']+list(torch.ones_like(\n",
    "            #                 torch.tensor(feature_text_token['token_type_ids'], dtype=torch.long)\n",
    "            #                 ))\n",
    "        }\n",
    "        for k, v in token.items():\n",
    "            token[k] = torch.tensor(v[: self.max_len], dtype=torch.long)\n",
    "        return token\n",
    "\n",
    "    def create_label(\n",
    "        self, text: str, annotation_length: int, location_list: list\n",
    "    ) -> Tensor:\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        offset_mapping = encoded[\"offset_mapping\"]\n",
    "        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping))\n",
    "        label[ignore_idxes] = -1\n",
    "        if annotation_length != 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(\";\")]:\n",
    "                    start_idx = -1\n",
    "                    end_idx = -1\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    for idx in range(len(offset_mapping)):\n",
    "                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                            start_idx = idx - 1\n",
    "                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                            end_idx = idx + 1\n",
    "                    if start_idx == -1:\n",
    "                        start_idx = end_idx\n",
    "                    if (start_idx != -1) & (end_idx != -1):\n",
    "                        label[start_idx:end_idx] = 1\n",
    "\n",
    "        return torch.tensor(label[: self.max_len], dtype=torch.float)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item: int) -> tuple:\n",
    "\n",
    "        inputs = self.prepare_input_with_fixed_position(\n",
    "            self.pn_historys[item], self.feature_texts[item]\n",
    "        )\n",
    "        label = self.create_label(\n",
    "            self.pn_historys[item], self.annotation_lengths[item], self.locations[item]\n",
    "        )\n",
    "\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "006c4b6c"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650910238271,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "aab0ac97"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.optim import Optimizer\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "\n",
    "class CustomModel(Module):\n",
    "    def __init__(\n",
    "        self, model_name: str, config_path: str = None, pretrained: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(\n",
    "                model_name, output_hidden_states=True\n",
    "            )\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        self.initializer_range = 0.1\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "\n",
    "    def _init_weights(self, module: Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def feature(self, inputs: Tensor) -> Tensor:\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output\n",
    "\n",
    "\n",
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "        criterion: _Loss,\n",
    "        optimizer: Optimizer,\n",
    "        adv_param: str = \"weight\",\n",
    "        adv_lr: int = 1,\n",
    "        adv_eps: float = 0.2,\n",
    "        start_epoch: int = 0,\n",
    "        adv_step: int = 1,\n",
    "        scaler=None,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def attack_backward(self, inputs: Tensor, labels: Tensor, epoch: int) -> None:\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "        self._save()\n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds = self.model(inputs)\n",
    "                adv_loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "                adv_loss = torch.masked_select(\n",
    "                    adv_loss, labels.view(-1, 1) != -1\n",
    "                ).mean()\n",
    "                adv_loss = adv_loss.mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "        self._restore()\n",
    "\n",
    "    def _attack_step(self) -> None:\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if (\n",
    "                param.requires_grad\n",
    "                and param.grad is not None\n",
    "                and self.adv_param in name\n",
    "            ):\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]),\n",
    "                        self.backup_eps[name][1],\n",
    "                    )\n",
    "\n",
    "    def _save(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if (\n",
    "                param.requires_grad\n",
    "                and param.grad is not None\n",
    "                and self.adv_param in name\n",
    "            ):\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "250ef911"
   },
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650910238271,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "716ccfc0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from math import floor\n",
    "from torch import inference_mode\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val: float, n=1) -> None:\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s) -> str:\n",
    "    m = floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent) -> str:\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3139cba-ebd0-4faf-bb86-de5f7f032293"
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1051,
     "status": "ok",
     "timestamp": 1650910239314,
     "user": {
      "displayName": "Ryosuke SAKURAI",
      "userId": "08091800370318882441"
     },
     "user_tz": -540
    },
    "id": "ef0f8033"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from logging import Logger\n",
    "import joblib\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch import cuda\n",
    "from transformers import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from wandb.sdk.wandb_config import Config\n",
    "\n",
    "\n",
    "def get_optimizer_params(\n",
    "    model: Module, encoder_lr: float, decoder_lr: float, weight_decay: float = 0.0\n",
    ") -> list:\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"lr\": encoder_lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"lr\": encoder_lr,\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "            \"lr\": decoder_lr,\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return optimizer_parameters\n",
    "\n",
    "\n",
    "def get_scheduler(\n",
    "    scheduler: str,\n",
    "    optimizer: Optimizer,\n",
    "    num_warmup_steps: int,\n",
    "    num_train_steps: int,\n",
    "    num_cycles: int,\n",
    ") -> _LRScheduler:\n",
    "\n",
    "    if scheduler == \"linear\":\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_train_steps,\n",
    "        )\n",
    "    elif scheduler == \"cosine\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_train_steps,\n",
    "            num_cycles=num_cycles,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Scheduler Name.\")\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, config: Config, tokenizer: PreTrainedTokenizer, logger: Logger\n",
    "    ) -> None:\n",
    "\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.logger = logger\n",
    "        self.device = torch.device(\"cuda\" if cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def log(self, data: dict, prefix: str = \"\") -> None:\n",
    "\n",
    "        min_str_length = min([len(k) for k in data.keys()])\n",
    "        n_same_char_seqs = 0\n",
    "        for i in range(min_str_length):\n",
    "            s = set([k[i] for k in data.keys()])\n",
    "            if len(s) == 1:\n",
    "                n_same_char_seqs += 1\n",
    "            else:\n",
    "                break\n",
    "        str_logs = [f\"{k}: {v}\" for k, v in data.items()]\n",
    "        s = \" \".join([l[n_same_char_seqs:].capitalize() for l in str_logs])\n",
    "        if prefix != \"\":\n",
    "            s = f\"{prefix} - {s}\"\n",
    "        self.logger.info(s)\n",
    "        wandb.log(data)\n",
    "\n",
    "    def compute_loss(\n",
    "        self, y_preds: Tensor, labels: Tensor, batch_size: int, loss_th: float\n",
    "    ) -> tuple:\n",
    "\n",
    "        loss = self.criterion(y_preds.squeeze(-1), labels.squeeze(-1))\n",
    "        samplewise_losses = []\n",
    "        for i in range(batch_size):\n",
    "            samplewise_losses.append(\n",
    "                torch.masked_select(loss[i], labels[i].squeeze(-1) != -1).mean()\n",
    "            )\n",
    "        loss = torch.stack(samplewise_losses)\n",
    "        loss_filter = torch.ones(batch_size, device=self.device)\n",
    "        if loss_th is not None:\n",
    "            mask = loss > loss_th\n",
    "            n_masked = mask.sum()\n",
    "            if n_masked > 0:\n",
    "                self.logger.info(f\"{n_masked} sample's loss was removed.\")\n",
    "            loss_filter[mask] = 0.0\n",
    "        else:\n",
    "            n_masked = 0\n",
    "\n",
    "        samplewise_losses = []\n",
    "        if loss_th is None:\n",
    "            for l in samplewise_losses:\n",
    "                samplewise_losses.append(l.item())\n",
    "\n",
    "        return (loss * loss_filter).sum() / (batch_size - n_masked), samplewise_losses\n",
    "\n",
    "    def train_with_eval(\n",
    "        self,\n",
    "        model: Module,\n",
    "        fold: int,\n",
    "        dls: tuple,\n",
    "        optimizer: Optimizer,\n",
    "        epoch: int,\n",
    "        scheduler: _LRScheduler,\n",
    "        loss_th: float,\n",
    "        valid_texts: list,\n",
    "        valid_labels: ndarray,\n",
    "        n_vl: int,\n",
    "        best_score: float,\n",
    "    ) -> tuple:\n",
    "\n",
    "        tr_dl, vl_dl = dls\n",
    "        model.train()\n",
    "        scaler = cuda.amp.GradScaler(enabled=self.config.apex)\n",
    "        awp = AWP(\n",
    "            model,\n",
    "            self.criterion,\n",
    "            optimizer,\n",
    "            adv_lr=self.config.adv_lr,\n",
    "            adv_eps=self.config.adv_eps,\n",
    "            start_epoch=self.config.adv_start_epoch,\n",
    "            scaler=scaler,\n",
    "        )\n",
    "\n",
    "        am = AverageMeter()\n",
    "        samplewise_losses = []\n",
    "        start = end = time.time()\n",
    "        global_step = 0\n",
    "        for step, (inputs, labels) in enumerate(tr_dl):\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            batch_size = labels.size(0)\n",
    "            with cuda.amp.autocast(enabled=self.config.apex):\n",
    "                y_preds = model(inputs)\n",
    "\n",
    "            loss, sl = self.compute_loss(\n",
    "                y_preds=y_preds, labels=labels, batch_size=batch_size, loss_th=loss_th\n",
    "            )\n",
    "            samplewise_losses += sl\n",
    "            if self.config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / self.config.gradient_accumulation_steps\n",
    "            am.update(loss.item(), batch_size)\n",
    "            scaler.scale(loss).backward()\n",
    "            awp.attack_backward(inputs, labels, epoch)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), self.config.max_grad_norm\n",
    "            )\n",
    "            if (step + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                if self.config.batch_scheduler:\n",
    "                    scheduler.step()\n",
    "            if step % self.config.print_freq == 0 or step == (len(tr_dl) - 1):\n",
    "                print(\n",
    "                    \"Epoch: [{0}][{1}/{2}] \"\n",
    "                    \"Elapsed {remain:s} \"\n",
    "                    \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
    "                    \"Grad: {grad_norm:.4f}  \"\n",
    "                    \"LR: {lr:.8f}  \".format(\n",
    "                        epoch + 1,\n",
    "                        step,\n",
    "                        len(tr_dl),\n",
    "                        remain=timeSince(start, float(step + 1) / len(tr_dl)),\n",
    "                        loss=am,\n",
    "                        grad_norm=grad_norm,\n",
    "                        lr=scheduler.get_lr()[0],\n",
    "                    )\n",
    "                )\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"[fold{fold}] loss\": am.val,\n",
    "                    f\"[fold{fold}] lr\": scheduler.get_lr()[0],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if (step + 1) % self.config.n_eval_steps == 0:\n",
    "                model.eval()\n",
    "                avg_vl_loss, predictions = self.infer(model, vl_dl, n_vl)\n",
    "                score, best_th = self.evaluate(\n",
    "                    predictions=predictions,\n",
    "                    valid_texts=valid_texts,\n",
    "                    valid_labels=valid_labels,\n",
    "                    th_range=self.config.th_range,\n",
    "                    th_step=self.config.th_step,\n",
    "                )\n",
    "                self.log(\n",
    "                    {\n",
    "                        f\"[fold{fold}] epoch\": epoch + 1,\n",
    "                        f\"[fold{fold}] step\": step,\n",
    "                        f\"[fold{fold}] avg_val_loss\": avg_vl_loss,\n",
    "                        f\"[fold{fold}] score\": score,\n",
    "                        f\"[fold{fold}] best_th\": best_th,\n",
    "                    }\n",
    "                )\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    self.save_ckpt(fold=fold, model=model, predictions=predictions)\n",
    "                model.train()\n",
    "\n",
    "        return am.avg, best_score, samplewise_losses\n",
    "\n",
    "    @inference_mode()\n",
    "    def infer(self, model: Module, vl_dl: DataLoader, n_vl: int) -> tuple:\n",
    "\n",
    "        model.eval()\n",
    "        losses = AverageMeter()\n",
    "        preds = []\n",
    "        for inputs, labels in vl_dl:\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            batch_size = labels.size(0)\n",
    "            y_preds = model(inputs)\n",
    "            loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "            loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "            if self.config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / self.config.gradient_accumulation_steps\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            preds.append(y_preds.sigmoid().to(\"cpu\").numpy())\n",
    "        predictions = np.concatenate(preds).reshape((n_vl, self.config.max_len))\n",
    "\n",
    "        return losses.avg, predictions\n",
    "\n",
    "    def create_dl(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        feature_text_max_len: int,\n",
    "        pn_history_max_len: int,\n",
    "        is_train: bool,\n",
    "        seed: int,\n",
    "    ) -> DataLoader:\n",
    "\n",
    "        ds = TrainDataset(\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_len=self.config.max_len,\n",
    "            feature_text_max_len=feature_text_max_len,\n",
    "            pn_history_max_len=pn_history_max_len,\n",
    "            df=df,\n",
    "        )\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed + int(is_train))\n",
    "\n",
    "        return DataLoader(\n",
    "            ds,\n",
    "            batch_size=self.config.batch_size\n",
    "            if is_train\n",
    "            else self.config.batch_size * 2,\n",
    "            shuffle=is_train,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=is_train,\n",
    "            generator=g,\n",
    "        )\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        predictions: ndarray,\n",
    "        valid_texts: tuple,\n",
    "        valid_labels: ndarray,\n",
    "        th_range: list,\n",
    "        th_step: float = 0.005,\n",
    "    ) -> tuple:\n",
    "\n",
    "        char_probs = get_char_probs(valid_texts, predictions, self.tokenizer)\n",
    "        best_score = -100\n",
    "        for th in np.arange(th_range[0], th_range[1], th_step):\n",
    "            th = np.round(th, 4)\n",
    "            results = get_results(char_probs, valid_texts, th=th)\n",
    "            preds = get_predictions(results)\n",
    "            score = get_score(valid_labels, preds)\n",
    "            if best_score < score:\n",
    "                best_th = th\n",
    "                best_score = score\n",
    "        return best_score, best_th\n",
    "\n",
    "    def save_ckpt(\n",
    "        self, fold: int, model: Module, predictions: ndarray, epoch: int = None\n",
    "    ) -> None:\n",
    "\n",
    "        if epoch is None:\n",
    "            ep_suffix = \"\"\n",
    "        else:\n",
    "            ep_suffix = f\"_epoch{epoch}\"\n",
    "\n",
    "        torch.save(\n",
    "            {\"model\": model.state_dict(), \"predictions\": predictions},\n",
    "            f\"{self.config.output_dir}{self.config.ckpt_name}_fold{fold}{ep_suffix}_best.pth\",\n",
    "        )\n",
    "        self.logger.info(\"model has been saved.\")\n",
    "\n",
    "    def compute_loss_th(self, samplewise_losses: list, fold: int, epoch: int) -> float:\n",
    "\n",
    "        mu_loss = np.mean(samplewise_losses)\n",
    "        std_loss = np.std(samplewise_losses)\n",
    "        loss_th = mu_loss + std_loss * self.config.n_loss_removal_std\n",
    "        joblib.dump(\n",
    "            value=samplewise_losses,\n",
    "            filename=f\"samplewise_losses_f{fold}_e{epoch}.pkl\",\n",
    "            compress=3,\n",
    "        )\n",
    "\n",
    "        return loss_th\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        pl_df: DataFrame,\n",
    "        feature_text_max_len: int,\n",
    "        pn_history_max_len: int,\n",
    "    ) -> None:\n",
    "\n",
    "        oof_df = pd.DataFrame()\n",
    "        for f in range(self.config.n_folds):\n",
    "\n",
    "            self.logger.info(f\"========== fold: {f} training ==========\")\n",
    "\n",
    "            model = CustomModel(\n",
    "                model_name=self.config.model, config_path=None, pretrained=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            tr_df = df[df[\"fold\"] != f].reset_index(drop=True)\n",
    "            if self.config.pl_frac > 0.0:\n",
    "                tr_pl_df = pl_df.loc[pl_df[\"fold\"] == f].sample(\n",
    "                    frac=self.config.pl_frac, random_state=self.config.seed + 1\n",
    "                )\n",
    "                self.logger.info(f\"{len(tr_pl_df)} pseudo labeled data was sampled.\")\n",
    "                tr_df = pd.concat((tr_df, tr_pl_df)).sample(\n",
    "                    frac=1.0, random_state=self.config.seed\n",
    "                )\n",
    "            tr_dl = self.create_dl(\n",
    "                df=tr_df,\n",
    "                feature_text_max_len=feature_text_max_len,\n",
    "                pn_history_max_len=pn_history_max_len,\n",
    "                is_train=True,\n",
    "                seed=self.config.seed,\n",
    "            )\n",
    "            num_train_steps = int(\n",
    "                len(tr_df) / self.config.batch_size * self.config.epochs\n",
    "            )\n",
    "\n",
    "            vl_df = df[df[\"fold\"] == f].reset_index(drop=True)\n",
    "            vl_dl = self.create_dl(\n",
    "                df=vl_df,\n",
    "                feature_text_max_len=feature_text_max_len,\n",
    "                pn_history_max_len=pn_history_max_len,\n",
    "                is_train=False,\n",
    "                seed=self.config.seed,\n",
    "            )\n",
    "            valid_texts = vl_df[\"pn_history\"].to_numpy()\n",
    "            valid_labels = create_labels_for_scoring(vl_df)\n",
    "\n",
    "            optimizer_parameters = get_optimizer_params(\n",
    "                model,\n",
    "                encoder_lr=self.config.encoder_lr,\n",
    "                decoder_lr=self.config.decoder_lr,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "            )\n",
    "            optimizer = AdamW(\n",
    "                optimizer_parameters,\n",
    "                lr=self.config.encoder_lr,\n",
    "                eps=self.config.eps,\n",
    "                betas=self.config.betas,\n",
    "            )\n",
    "            scheduler = get_scheduler(\n",
    "                scheduler=self.config.scheduler,\n",
    "                optimizer=optimizer,\n",
    "                num_warmup_steps=self.config.num_warmup_steps,\n",
    "                num_train_steps=num_train_steps,\n",
    "                num_cycles=self.config.num_cycles,\n",
    "            )\n",
    "\n",
    "            best_score = 0\n",
    "            for epoch in range(self.config.epochs):\n",
    "\n",
    "                if epoch == self.config.loss_removal_start_ep:\n",
    "                    loss_th = self.compute_loss_th(\n",
    "                        samplewise_losses=samplewise_losses, fold=f, epoch=epoch\n",
    "                    )\n",
    "                    self.logger.info(f\"Loss th: {loss_th}\")\n",
    "\n",
    "                (\n",
    "                    avg_tr_loss,\n",
    "                    stepwise_best_score,\n",
    "                    samplewise_losses,\n",
    "                ) = self.train_with_eval(\n",
    "                    model,\n",
    "                    f,\n",
    "                    (tr_dl, vl_dl),\n",
    "                    optimizer,\n",
    "                    epoch,\n",
    "                    scheduler,\n",
    "                    loss_th if epoch >= self.config.loss_removal_start_ep else None,\n",
    "                    valid_texts,\n",
    "                    valid_labels,\n",
    "                    len(vl_df),\n",
    "                    best_score,\n",
    "                )\n",
    "\n",
    "                avg_vl_loss, predictions = self.infer(model, vl_dl, len(vl_df))\n",
    "                score, best_th = self.evaluate(\n",
    "                    predictions=predictions,\n",
    "                    valid_texts=valid_texts,\n",
    "                    valid_labels=valid_labels,\n",
    "                    th_range=self.config.th_range,\n",
    "                    th_step=self.config.th_step,\n",
    "                )\n",
    "\n",
    "                self.log(\n",
    "                    {\n",
    "                        f\"[fold{f}] epoch\": epoch + 1,\n",
    "                        f\"[fold{f}] avg_train_loss\": avg_tr_loss,\n",
    "                        f\"[fold{f}] avg_val_loss\": avg_vl_loss,\n",
    "                        f\"[fold{f}] score\": score,\n",
    "                        f\"[fold{f}] best_th\": best_th,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if score > stepwise_best_score:\n",
    "                    best_score = score\n",
    "\n",
    "                self.save_ckpt(\n",
    "                    fold=f, model=model, predictions=predictions, epoch=epoch\n",
    "                )\n",
    "\n",
    "            predictions = torch.load(\n",
    "                f\"{self.config.output_dir}{self.config.ckpt_name}_fold{f}_best.pth\",\n",
    "                map_location=torch.device(\"cpu\"),\n",
    "            )[\"predictions\"]\n",
    "            vl_df[[i for i in range(self.config.max_len)]] = predictions\n",
    "            oof_df = pd.concat([oof_df, vl_df])\n",
    "\n",
    "            self.logger.info(f\"========== fold: {f} result ==========\")\n",
    "\n",
    "            score, th = get_result(vl_df, self.tokenizer, self.config.max_len)\n",
    "            self.log(\n",
    "                {f\"[fold{f}] overall score\": score, f\"[fold{f}] overall best th\": th}\n",
    "            )\n",
    "            oof_df.to_pickle(f\"{self.config.output_dir}oof_df_fold{f}.pkl\")\n",
    "\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        self.logger.info(f\"========== CV ==========\")\n",
    "        score, th = get_result(oof_df, self.tokenizer, self.config.max_len)\n",
    "        self.log({f\"overall score\": score, f\"overall best th\": th})\n",
    "        oof_df.to_pickle(self.config.output_dir + \"oof_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db38bc60",
    "outputId": "1e4c4719-1e76-4cdb-e979-4f10b39b0896"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    logger=logger)\n",
    "\n",
    "trainer.run(\n",
    "    df=df_train,\n",
    "    pl_df=pl_train,\n",
    "    feature_text_max_len=feature_text_max_len, \n",
    "    pn_history_max_len=pn_history_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5a96081-3aa7-4948-be95-67e31d3bd7e6"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8ac591e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
