{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Ic8N2I-_AuR_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1650983915697,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "Ic8N2I-_AuR_",
    "outputId": "6db84cd6-f006-4893-eb90-46d85ed9e1ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 27 05:20:19 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "FLpl3saHCT0D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20887,
     "status": "ok",
     "timestamp": 1651020917203,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "FLpl3saHCT0D",
    "outputId": "fabb1056-bb47-4d55-dde5-0d5b54fe7c3d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7426d7d",
   "metadata": {
    "id": "f7426d7d"
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "i8FIgXo4CcyN",
   "metadata": {
    "id": "i8FIgXo4CcyN"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "E0idovjsE4L6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1650983920969,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "E0idovjsE4L6",
    "outputId": "aab28d55-0dd8-49c8-ea0a-448233b61ef2"
   },
   "outputs": [],
   "source": [
    "# %cd \"/content/drive/MyDrive/Colab Notebooks/nbme/code/exp076\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797404f7",
   "metadata": {
    "id": "797404f7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import yaml\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import pandas as pd\n",
    "import torch\n",
    "from logging import Logger, getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from wandb.sdk.wandb_config import Config\n",
    "\n",
    "def init_pandas() -> None:    \n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "def get_logger(filename:str) -> Logger:\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "def seed_everything(seed:int=42) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def init_wandb(wandb_key:str) -> Config:\n",
    "    #from kaggle_secrets import UserSecretsClient\n",
    "    #user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = wandb_key\n",
    "    wandb.login(key=secret_value_0)\n",
    "\n",
    "    INPUT_DIR = Path(\"../input\") #'../input/raiii-nbme'\n",
    "    loader = yaml.SafeLoader\n",
    "    loader.add_implicit_resolver(\n",
    "        u'tag:yaml.org,2002:float',\n",
    "        re.compile(u'''^(?:\n",
    "         [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)?\n",
    "        |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)\n",
    "        |\\\\.[0-9_]+(?:[eE][-+][0-9]+)?\n",
    "        |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\\\.[0-9_]*\n",
    "        |[-+]?\\\\.(?:inf|Inf|INF)\n",
    "        |\\\\.(?:nan|NaN|NAN))$''', re.X),\n",
    "        list(u'-+0123456789.'))\n",
    "    with open(INPUT_DIR / 'exp076_config.yml') as f:\n",
    "        param = yaml.load(f, Loader=loader)\n",
    "    wandb.init(\n",
    "        project=param['project'],\n",
    "        config=param\n",
    "    )\n",
    "    wandb.config.update(param)\n",
    "    print(f'run name: {wandb.run.name}')    \n",
    "    return wandb.config\n",
    "\n",
    "def mk_output_dir(path:str) -> None:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bQ6e-ctMDbHn",
   "metadata": {
    "id": "bQ6e-ctMDbHn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "wandb_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73494049-1b8a-4400-a39d-18f47f6c69de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 6575,
     "status": "ok",
     "timestamp": 1650983928575,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "73494049-1b8a-4400-a39d-18f47f6c69de",
    "outputId": "5233fbf1-9a3e-4bea-ae81-def2840cea3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmpeg\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/workspace/notebook/wandb/run-20220427_052026-zuo6ev33</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mpeg/NBME-ScoreClinicalPatientNotes/runs/zuo6ev33\" target=\"_blank\">quiet-snow-26</a></strong> to <a href=\"https://wandb.ai/mpeg/NBME-ScoreClinicalPatientNotes\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run name: quiet-snow-26\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "#wandb_key = \"\"\n",
    "config = init_wandb(wandb_key=wandb_key)\n",
    "mk_output_dir(path=config.output_dir)\n",
    "LOGGER = get_logger(\n",
    "    filename=config.output_dir+'train'\n",
    ")\n",
    "seed_everything(seed=config.seed)\n",
    "init_pandas()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ae408",
   "metadata": {
    "id": "575ae408"
   },
   "source": [
    "# Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8771dcad",
   "metadata": {
    "id": "8771dcad"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_score(y_true:ndarray, y_pred:ndarray) -> float:\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def micro_f1(preds:list, truths:list) -> float:\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans:list, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3hjXfEl6FVXg",
   "metadata": {
    "id": "3hjXfEl6FVXg"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers[sentencepiece] > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c20fdb1b",
   "metadata": {
    "id": "c20fdb1b"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import ast\n",
    "from pandas import DataFrame\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "def create_labels_for_scoring(df:DataFrame):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "#↓こっちのほうがちょっといい\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        prev_pred = 0\n",
    "        prev_end = -1\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "            if start != prev_end:\n",
    "                results[i][prev_end:start] = (pred+prev_pred)/2\n",
    "            prev_pred = pred\n",
    "            prev_end = end\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def get_result(df_oof:DataFrame, tokenizer:PreTrainedTokenizer, max_len:int) -> None:\n",
    "    labels = create_labels_for_scoring(df_oof)\n",
    "    predictions = df_oof[[i for i in range(max_len)]].values\n",
    "    char_probs = get_char_probs(df_oof['pn_history'].values, predictions, tokenizer)\n",
    "    \n",
    "    score=-100\n",
    "    for th in np.arange(0.3,0.7,0.005):\n",
    "        th = np.round(th,4)\n",
    "        results = get_results(char_probs, th=th)\n",
    "        preds = get_predictions(results)\n",
    "        tmp_score = get_score(labels, preds)\n",
    "        if tmp_score > score:\n",
    "            best_th=th\n",
    "            score=tmp_score\n",
    "    LOGGER.info(f'Score: {score:<.4f} Best threshold:: {best_th}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc99c98",
   "metadata": {
    "id": "7dc99c98"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f93fc583",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "executionInfo": {
     "elapsed": 2110,
     "status": "ok",
     "timestamp": 1650983968701,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "f93fc583",
    "outputId": "c89675d1-7d3d-4de3-bb97-b85918dc2488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape: (14300, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[chest pressure]</td>\n",
       "      <td>[203 217]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[intermittent episodes, episode]</td>\n",
       "      <td>[70 91, 176 183]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[felt as if he were going to pass out]</td>\n",
       "      <td>[222 258]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num                              annotation          location\n",
       "0  00016_000         0      16            0          [dad with recent heart attcak]         [696 724]\n",
       "1  00016_001         0      16            1             [mom with \"thyroid disease]         [668 693]\n",
       "2  00016_002         0      16            2                        [chest pressure]         [203 217]\n",
       "3  00016_003         0      16            3        [intermittent episodes, episode]  [70 91, 176 183]\n",
       "4  00016_004         0      16            4  [felt as if he were going to pass out]         [222 258]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.shape: (143, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Chest-pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Lightheaded</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_num  case_num                                       feature_text\n",
       "0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n",
       "1            1         0                 Family-history-of-thyroid-disorder\n",
       "2            2         0                                     Chest-pressure\n",
       "3            3         0                              Intermittent-symptoms\n",
       "4            4         0                                        Lightheaded"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_notes.shape: (42146, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pn_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17-year-old male, has come to the student heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17 yo male with recurrent palpitations for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Dillon Cleveland is a 17 y.o. male patient wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a 17 yo m c/o palpitation started 3 mos ago; \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17yo male with no pmh here for evaluation of p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pn_num  case_num                                         pn_history\n",
       "0       0         0  17-year-old male, has come to the student heal...\n",
       "1       1         0  17 yo male with recurrent palpitations for the...\n",
       "2       2         0  Dillon Cleveland is a 17 y.o. male patient wit...\n",
       "3       3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n",
       "4       4         0  17yo male with no pmh here for evaluation of p..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_train['annotation'] = df_train['annotation'].map(lambda x: ast.literal_eval(x))\n",
    "df_train['location'] = df_train['location'].map(lambda x: ast.literal_eval(x))\n",
    "\n",
    "features = pd.read_csv('../input/features.csv')\n",
    "features = preprocess_features(features)\n",
    "\n",
    "patient_notes = pd.read_csv('../input/patient_notes.csv')\n",
    "\n",
    "print(f\"df_train.shape: {df_train.shape}\")\n",
    "display(df_train.head())\n",
    "print(f\"features.shape: {features.shape}\")\n",
    "display(features.head())\n",
    "print(f\"patient_notes.shape: {patient_notes.shape}\")\n",
    "display(patient_notes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce0adc5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 868,
     "status": "ok",
     "timestamp": 1650983969564,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "ce0adc5c",
    "outputId": "f1fe2104-4cfd-4a21-ba7d-de4955073dcb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[chest pressure]</td>\n",
       "      <td>[203 217]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[intermittent episodes, episode]</td>\n",
       "      <td>[70 91, 176 183]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[felt as if he were going to pass out]</td>\n",
       "      <td>[222 258]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num                              annotation          location                                       feature_text                                         pn_history\n",
       "0  00016_000         0      16            0          [dad with recent heart attcak]         [696 724]  Family-history-of-MI-OR-Family-history-of-myoc...  HPI: 17yo M presents with palpitations. Patien...\n",
       "1  00016_001         0      16            1             [mom with \"thyroid disease]         [668 693]                 Family-history-of-thyroid-disorder  HPI: 17yo M presents with palpitations. Patien...\n",
       "2  00016_002         0      16            2                        [chest pressure]         [203 217]                                     Chest-pressure  HPI: 17yo M presents with palpitations. Patien...\n",
       "3  00016_003         0      16            3        [intermittent episodes, episode]  [70 91, 176 183]                              Intermittent-symptoms  HPI: 17yo M presents with palpitations. Patien...\n",
       "4  00016_004         0      16            4  [felt as if he were going to pass out]         [222 258]                                        Lightheaded  HPI: 17yo M presents with palpitations. Patien..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = df_train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "df_train = df_train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66337109",
   "metadata": {
    "id": "66337109"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "def correct_annotation(df_train:DataFrame) -> None:\n",
    "    df_train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "    df_train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "    df_train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "    df_train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "    df_train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "    df_train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "    df_train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "    df_train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "    df_train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "    df_train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "    df_train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "    df_train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "    df_train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "    df_train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "    df_train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "    df_train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "    df_train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "    df_train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "    df_train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "    df_train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "    df_train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "    df_train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "    df_train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "    df_train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "    df_train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "    df_train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "    df_train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "    df_train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "    df_train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "    df_train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "    df_train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "    df_train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "    df_train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "    df_train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "    df_train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "    df_train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "    df_train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "    df_train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "    df_train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "    df_train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "    df_train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "    df_train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "    df_train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "    df_train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "    df_train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "    df_train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "    df_train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "    df_train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "    df_train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "    df_train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "    df_train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "    df_train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "    df_train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "    df_train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "    df_train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "    df_train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "    df_train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "    df_train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "    df_train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "    df_train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "    df_train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "    df_train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "    df_train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "    df_train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "    df_train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "    df_train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "    df_train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "    df_train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "    df_train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "    df_train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "    df_train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "    df_train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "    df_train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "    df_train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "    df_train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "    df_train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "    df_train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "    df_train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "    df_train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "    df_train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "    df_train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "    df_train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "    df_train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "    df_train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec3a7b5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "executionInfo": {
     "elapsed": 1006,
     "status": "ok",
     "timestamp": 1650983971924,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "ec3a7b5f",
    "outputId": "755b13cb-9f7b-4e52-f753-6eeaac17aa92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8181\n",
       "0    4399\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['annotation_length'] = df_train['annotation'].map(lambda x: len(x))\n",
    "display(df_train['annotation_length'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6437f",
   "metadata": {
    "id": "e6c6437f"
   },
   "source": [
    "# CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed511f87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "executionInfo": {
     "elapsed": 1005,
     "status": "ok",
     "timestamp": 1650983972923,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "ed511f87",
    "outputId": "3fd2fb7b-8954-49b4-e449-dea063fa0956"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    3575\n",
       "1    3575\n",
       "2    3575\n",
       "3    3575\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "kf = GroupKFold(n_splits=config.n_folds)\n",
    "groups = df_train['pn_num'].to_numpy()\n",
    "df_train.loc[:, 'fold'] = -1\n",
    "for n, (train_index, val_index) in enumerate(kf.split(df_train, df_train['location'], groups)):\n",
    "    df_train.loc[val_index, 'fold'] = n\n",
    "display(df_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "526e4ac9",
   "metadata": {
    "id": "526e4ac9"
   },
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    display(df_train.groupby('fold').size())\n",
    "    df_train = df_train.sample(n=500, random_state=0).reset_index(drop=True)\n",
    "    display(df_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2590f",
   "metadata": {
    "id": "11e2590f"
   },
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "BR6m_itarTmJ",
   "metadata": {
    "id": "BR6m_itarTmJ"
   },
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "\n",
    "if 'deberta' in config.model:\n",
    "\n",
    "    transformers_path = Path(transformers.__file__[:-12])\n",
    "\n",
    "    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n",
    "\n",
    "    convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "    conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "    if conversion_path.exists():\n",
    "        conversion_path.unlink()\n",
    "\n",
    "    shutil.copy(convert_file, transformers_path)\n",
    "    deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "    for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py', \"deberta__init__.py\"]:\n",
    "        if str(filename).startswith(\"deberta\"):\n",
    "            filepath = deberta_v2_path/str(filename).replace(\"deberta\", \"\")\n",
    "        else:\n",
    "            filepath = deberta_v2_path/filename\n",
    "        if filepath.exists():\n",
    "            filepath.unlink()\n",
    "\n",
    "        shutil.copy(input_dir/filename, filepath)\n",
    "    from transformers.models.deberta_v2 import DebertaV2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "819ca57b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4885,
     "status": "ok",
     "timestamp": 1650983979611,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "819ca57b",
    "outputId": "fa3b8bbf-4b81-47fb-bd30-b421ae28f2f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../output/exp076/tokenizer/tokenizer_config.json',\n",
       " '../output/exp076/tokenizer/special_tokens_map.json',\n",
       " '../output/exp076/tokenizer/vocab.json',\n",
       " '../output/exp076/tokenizer/merges.txt',\n",
       " '../output/exp076/tokenizer/added_tokens.json',\n",
       " '../output/exp076/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "if 'deberta' in config.model:\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(config.model)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('roberta-large', trim_offsets=False)\n",
    "tokenizer.save_pretrained(config.output_dir+'tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d057ad3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20191,
     "status": "ok",
     "timestamp": 1650983999790,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "7d057ad3",
    "outputId": "ae55669f-3d68-488d-a3fd-46f212a3786d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pn_history max(lengths): 433\n",
      "feature_text max(lengths): 30\n",
      "max_len: 466\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "for text in patient_notes['pn_history'].fillna(\"\").to_list():\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    pn_history_lengths.append(length)\n",
    "pn_history_max_len = max(pn_history_lengths)\n",
    "LOGGER.info(f'pn_history max(lengths): {pn_history_max_len}')\n",
    "\n",
    "features_lengths = []\n",
    "for text in features['feature_text'].fillna(\"\").to_list():\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    features_lengths.append(length)\n",
    "feature_text_max_len = max(features_lengths)\n",
    "LOGGER.info(f'feature_text max(lengths): {feature_text_max_len}')\n",
    "\n",
    "config.max_len = pn_history_max_len+feature_text_max_len + 3\n",
    "LOGGER.info(f\"max_len: {config.max_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9f616",
   "metadata": {
    "id": "06f9f616"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1387f638",
   "metadata": {
    "id": "1387f638"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer:PreTrainedTokenizer, \n",
    "        max_len:int,\n",
    "        feature_text_max_len:int, \n",
    "        pn_history_max_len:int, \n",
    "        df:DataFrame\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.feature_text_max_len = feature_text_max_len\n",
    "        self.pn_history_max_len = pn_history_max_len\n",
    "        self.feature_texts = df['feature_text'].to_numpy()\n",
    "        self.pn_historys = df['pn_history'].to_numpy()\n",
    "        self.annotation_lengths = df['annotation_length'].to_numpy()\n",
    "        self.locations = df['location'].to_numpy()\n",
    "\n",
    "    def prepare_input_with_fixed_position(self, pn_history:str, feature_text:str) -> dict:\n",
    "\n",
    "        pn_history_token = self.tokenizer(\n",
    "            pn_history, \n",
    "            add_special_tokens=True,\n",
    "            max_length=self.pn_history_max_len+2, \n",
    "            padding='max_length',\n",
    "            return_offsets_mapping=False)\n",
    "        \n",
    "        feature_text_token = self.tokenizer(\n",
    "            feature_text, \n",
    "            add_special_tokens=True,\n",
    "            max_length=self.feature_text_max_len+2, \n",
    "            padding='max_length',\n",
    "            return_offsets_mapping=False)\n",
    "        for k,v in feature_text_token.items():\n",
    "            feature_text_token[k] = v[1:]\n",
    "\n",
    "        token = {\n",
    "            'input_ids': pn_history_token['input_ids']+feature_text_token['input_ids'],\n",
    "            'attention_mask': pn_history_token['attention_mask']+feature_text_token['attention_mask'],\n",
    "            #'token_type_ids': pn_history_token['token_type_ids']+torch.ones_like(feature_text_token['token_type_ids'])\n",
    "        }\n",
    "        for k, v in token.items():\n",
    "            token[k] = torch.tensor(v[:self.max_len], dtype=torch.long)\n",
    "        return token\n",
    "    \n",
    "    def prepare_input(self, text:str, feature_text:str) -> dict:\n",
    "        token = self.tokenizer(text, feature_text, \n",
    "                               add_special_tokens=True,\n",
    "                               max_length=self.max_len,\n",
    "                               padding=\"max_length\",\n",
    "                               return_offsets_mapping=False)\n",
    "        for k, v in token.items():\n",
    "            token[k] = torch.tensor(v[:self.max_len], dtype=torch.long)\n",
    "        return token\n",
    "    \n",
    "    def create_label(self, text:str, annotation_length:int, location_list:list) -> Tensor:\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True)\n",
    "        offset_mapping = encoded['offset_mapping']\n",
    "        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping))\n",
    "        label[ignore_idxes] = -1\n",
    "        if annotation_length != 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(';')]:\n",
    "                    start_idx = -1\n",
    "                    end_idx = -1\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    for idx in range(len(offset_mapping)):\n",
    "                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                            start_idx = idx - 1\n",
    "                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                            end_idx = idx + 1\n",
    "                    if start_idx == -1:\n",
    "                        start_idx = end_idx\n",
    "                    if (start_idx != -1) & (end_idx != -1):\n",
    "                        label[start_idx:end_idx] = 1\n",
    "        return torch.tensor(label[:self.max_len], dtype=torch.float)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item:int) -> tuple:\n",
    "        inputs = self.prepare_input_with_fixed_position(\n",
    "            self.pn_historys[item],\n",
    "            self.feature_texts[item])\n",
    "        label = self.create_label(\n",
    "            self.pn_historys[item], \n",
    "            self.annotation_lengths[item], \n",
    "            self.locations[item])\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e253111b-eb72-4462-a8d7-10b57aaf84e4",
   "metadata": {
    "id": "e253111b-eb72-4462-a8d7-10b57aaf84e4"
   },
   "outputs": [],
   "source": [
    "ds = TrainDataset(\n",
    "    tokenizer=tokenizer, \n",
    "    max_len=config.max_len,\n",
    "    feature_text_max_len=feature_text_max_len, \n",
    "    pn_history_max_len=pn_history_max_len, \n",
    "    df=df_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c4b6c",
   "metadata": {
    "id": "006c4b6c"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "o6CRrkIpgvzs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1212,
     "status": "ok",
     "timestamp": 1651020929535,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "o6CRrkIpgvzs",
    "outputId": "aeda6eca-a7e7-49a1-d6b6-d0f5b028b2de"
   },
   "outputs": [],
   "source": [
    "# %cd \"/content/drive/MyDrive/Colab Notebooks/nbme/code/exp076\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "kh6iyZMqhIgY",
   "metadata": {
    "executionInfo": {
     "elapsed": 7805,
     "status": "ok",
     "timestamp": 1651020976094,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "kh6iyZMqhIgY"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers[sentencepiece] > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "KXDxA1lSgmY6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34867,
     "status": "ok",
     "timestamp": 1651021027981,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "KXDxA1lSgmY6",
    "outputId": "3d7740e2-e34b-4bfe-c2da-1cb9458f507d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained('../output/roberta-large-self-supervised-learning-9epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "uL1tA3_whM-Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1651021059509,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "uL1tA3_whM-Z",
    "outputId": "740c255c-111d-4de9-a1a6-823449028849"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.modeling_roberta.RobertaModel"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "_Xl_4ScSHhLw",
   "metadata": {
    "id": "_Xl_4ScSHhLw"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "\n",
    "class CustomModel(Module):\n",
    "    def __init__(\n",
    "        self, model_name: str, config_path: str = None, pretrained: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(\n",
    "                model_name, output_hidden_states=True\n",
    "            )\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "\n",
    "    def _init_weights(self, module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def feature(self, inputs: Tensor) -> Tensor:\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output\n",
    "\n",
    "\n",
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        adv_param=\"weight\",\n",
    "        adv_lr=1,\n",
    "        adv_eps=0.2,\n",
    "        start_epoch=0,\n",
    "        adv_step=1,\n",
    "        scaler=None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def attack_backward(self, inputs, labels, epoch):\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "\n",
    "        self._save()\n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_preds = self.model(inputs)\n",
    "                adv_loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "                adv_loss = torch.masked_select(\n",
    "                    adv_loss, labels.view(-1, 1) != -1\n",
    "                ).mean()\n",
    "                adv_loss = adv_loss.mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "\n",
    "        self._restore()\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if (\n",
    "                param.requires_grad\n",
    "                and param.grad is not None\n",
    "                and self.adv_param in name\n",
    "            ):\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]),\n",
    "                        self.backup_eps[name][1],\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if (\n",
    "                param.requires_grad\n",
    "                and param.grad is not None\n",
    "                and self.adv_param in name\n",
    "            ):\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(\n",
    "        self,\n",
    "    ):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250ef911",
   "metadata": {
    "id": "250ef911"
   },
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "716ccfc0",
   "metadata": {
    "id": "716ccfc0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from math import floor\n",
    "from torch import inference_mode\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val:float, n=1) -> None:\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s) -> str:\n",
    "    m = floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent) -> str:\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3139cba-ebd0-4faf-bb86-de5f7f032293",
   "metadata": {
    "id": "a3139cba-ebd0-4faf-bb86-de5f7f032293"
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef0f8033",
   "metadata": {
    "id": "ef0f8033"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.optim import AdamW\n",
    "from torch import cuda\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "from wandb.sdk.wandb_config import Config\n",
    "\n",
    "def get_optimizer_params(model:Module, encoder_lr:float, decoder_lr:float, weight_decay:float=0.0) -> list:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "        'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "def get_scheduler(scheduler:str, optimizer, num_warmup_steps:int, num_train_steps:int, num_cycles:int):\n",
    "    if scheduler=='linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=num_warmup_steps, \n",
    "            num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif scheduler=='cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=num_warmup_steps, \n",
    "            num_training_steps=num_train_steps, \n",
    "            num_cycles=num_cycles\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('Invalid Scheduler Name.')\n",
    "    return scheduler\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, config:Config, tokenizer:PreTrainedTokenizer) -> None:\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.device = torch.device('cuda' if cuda.is_available() else 'cpu')\n",
    "\n",
    "    def train(self, model:Module, fold:int, tr_dl:DataLoader, optimizer, epoch:int, scheduler):\n",
    "        model.train()\n",
    "        scaler = cuda.amp.GradScaler(enabled=self.config.apex)\n",
    "        awp = AWP(model,\n",
    "              self.criterion,\n",
    "              optimizer,\n",
    "              adv_lr=config.adv_lr,\n",
    "              adv_eps=config.adv_eps,\n",
    "              start_epoch=config.adv_start_epoch,\n",
    "              scaler=scaler\n",
    "             )\n",
    "        losses = AverageMeter()\n",
    "        start = end = time.time()\n",
    "        global_step = 0\n",
    "        for step, (inputs, labels) in enumerate(tr_dl):\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            batch_size = labels.size(0)\n",
    "            with cuda.amp.autocast(enabled=self.config.apex):\n",
    "                y_preds = model(inputs)\n",
    "            loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "            loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "            if self.config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / self.config.gradient_accumulation_steps\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            scaler.scale(loss).backward()\n",
    "            awp.attack_backward(inputs, labels, epoch)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), self.config.max_grad_norm)\n",
    "            if (step + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                if self.config.batch_scheduler:\n",
    "                    scheduler.step()\n",
    "            end = time.time()\n",
    "            if step % self.config.print_freq == 0 or step == (len(tr_dl)-1):\n",
    "                print('Epoch: [{0}][{1}/{2}] '\n",
    "                    'Elapsed {remain:s} '\n",
    "                    'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                    'Grad: {grad_norm:.4f}  '\n",
    "                    'LR: {lr:.8f}  '\n",
    "                    .format(epoch+1, step, len(tr_dl), \n",
    "                            remain=timeSince(start, float(step+1)/len(tr_dl)),\n",
    "                            loss=losses,\n",
    "                            grad_norm=grad_norm,\n",
    "                            lr=scheduler.get_lr()[0]))\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                    f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "        return losses.avg\n",
    "\n",
    "    @inference_mode()\n",
    "    def validate(self, model:Module, vl_dl:DataLoader) -> tuple:\n",
    "        model.eval()\n",
    "        losses = AverageMeter()\n",
    "        preds = []\n",
    "        start = end = time.time()\n",
    "        for step, (inputs, labels) in enumerate(vl_dl):\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            batch_size = labels.size(0)\n",
    "            y_preds = model(inputs)\n",
    "            loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "            loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "            if self.config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / self.config.gradient_accumulation_steps\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "            end = time.time()\n",
    "            if step % self.config.print_freq == 0 or step == (len(vl_dl)-1):\n",
    "                print('EVAL: [{0}/{1}] '\n",
    "                    'Elapsed {remain:s} '\n",
    "                    'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                    .format(step, len(vl_dl),\n",
    "                            loss=losses,\n",
    "                            remain=timeSince(start, float(step+1)/len(vl_dl))))\n",
    "        return losses.avg, np.concatenate(preds)\n",
    "\n",
    "    def create_dl(self, df:DataFrame, feature_text_max_len:int, pn_history_max_len:int, is_train:bool) -> DataLoader:\n",
    "        ds = TrainDataset(\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_len=self.config.max_len,\n",
    "            feature_text_max_len=feature_text_max_len,\n",
    "            pn_history_max_len=pn_history_max_len,\n",
    "            df=df)\n",
    "        return DataLoader(\n",
    "            ds,\n",
    "            batch_size=self.config.batch_size if is_train else self.config.batch_size * 2,\n",
    "            shuffle=is_train,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True, \n",
    "            drop_last=is_train)\n",
    "    \n",
    "    def log_epoch_result(self, f:int, ep:int, avg_tr_loss:float, avg_vl_loss:float, elapsed:float, score:float, best_th:float) -> None:\n",
    "        LOGGER.info(\n",
    "            f'Epoch {ep} - avg_train_loss: {avg_tr_loss:.4f}  avg_val_loss: {avg_vl_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(\n",
    "            f'Epoch {ep} - Score: {score:.4f} for th={best_th}')\n",
    "        wandb.log(\n",
    "            {\n",
    "                f\"[fold{f}] epoch\": ep, \n",
    "                f\"[fold{f}] avg_train_loss\": avg_tr_loss, \n",
    "                f\"[fold{f}] avg_val_loss\": avg_vl_loss,\n",
    "                f\"[fold{f}] score\": score,\n",
    "                f\"[fold{f}] best_th\": best_th\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        df:DataFrame,\n",
    "        feature_text_max_len:int, \n",
    "        pn_history_max_len:int) -> None:\n",
    "    \n",
    "        oof_df = pd.DataFrame()\n",
    "        for f in range(self.config.n_folds):\n",
    "            LOGGER.info(f\"========== fold: {f} training ==========\")\n",
    "            \n",
    "            model = CustomModel(\n",
    "                self.config.model, \n",
    "                config_path=None, \n",
    "                pretrained=True).to(self.device)\n",
    "\n",
    "            tr_df = df[df['fold'] != f].reset_index(drop=True)\n",
    "            tr_dl = self.create_dl(\n",
    "                df=tr_df, \n",
    "                feature_text_max_len=feature_text_max_len, \n",
    "                pn_history_max_len=pn_history_max_len, \n",
    "                is_train=True)\n",
    "            num_train_steps = int(len(tr_df) / self.config.batch_size * self.config.epochs)\n",
    "            \n",
    "            vl_df = df[df['fold'] == f].reset_index(drop=True)\n",
    "            vl_dl = self.create_dl(\n",
    "                df=vl_df, \n",
    "                feature_text_max_len=feature_text_max_len, \n",
    "                pn_history_max_len=pn_history_max_len, \n",
    "                is_train=False)\n",
    "            valid_texts = vl_df['pn_history'].to_numpy()\n",
    "            valid_labels = create_labels_for_scoring(vl_df)\n",
    "\n",
    "            optimizer_parameters = get_optimizer_params(\n",
    "                model,\n",
    "                encoder_lr=self.config.encoder_lr, \n",
    "                decoder_lr=self.config.decoder_lr,\n",
    "                weight_decay=self.config.weight_decay)\n",
    "            optimizer = AdamW(\n",
    "                optimizer_parameters, \n",
    "                lr=self.config.encoder_lr, \n",
    "                eps=self.config.eps, \n",
    "                betas=self.config.betas)\n",
    "            scheduler = get_scheduler(\n",
    "                scheduler=self.config.scheduler, \n",
    "                optimizer=optimizer, \n",
    "                num_warmup_steps=self.config.num_warmup_steps,\n",
    "                num_train_steps=num_train_steps,\n",
    "                num_cycles=self.config.num_cycles)\n",
    "            \n",
    "            best_score = 0\n",
    "\n",
    "            for epoch in range(self.config.epochs):\n",
    "                \n",
    "                start_time = time.time()\n",
    "\n",
    "                avg_tr_loss = self.train(\n",
    "                    model,\n",
    "                    f, \n",
    "                    tr_dl, \n",
    "                    optimizer, \n",
    "                    epoch, \n",
    "                    scheduler)\n",
    "\n",
    "                # eval\n",
    "                avg_vl_loss, predictions = self.validate(\n",
    "                    model, \n",
    "                    vl_dl\n",
    "                )\n",
    "                predictions = predictions.reshape(\n",
    "                    (len(vl_df), \n",
    "                    self.config.max_len))\n",
    "                \n",
    "                # scoring\n",
    "                char_probs = get_char_probs(\n",
    "                    valid_texts, \n",
    "                    predictions, \n",
    "                    self.tokenizer)\n",
    "                # ここをしきい値で探索した最適な値にする\n",
    "                score=-100\n",
    "                for th in np.arange(0.3,0.7,0.005):\n",
    "                    th = np.round(th,4)\n",
    "                    results = get_results(char_probs, th=th)\n",
    "                    preds = get_predictions(results)\n",
    "                    tmp_score = get_score(valid_labels, preds)\n",
    "                    if tmp_score > score:\n",
    "                        best_th=th\n",
    "                        score=tmp_score\n",
    "                \n",
    "                self.log_epoch_result(\n",
    "                    f=f,\n",
    "                    ep=epoch+1, \n",
    "                    avg_tr_loss=avg_tr_loss, \n",
    "                    avg_vl_loss=avg_vl_loss, \n",
    "                    elapsed=time.time() - start_time, \n",
    "                    score=score, \n",
    "                    best_th=best_th)\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    LOGGER.info(f'Epoch {epoch+1} - Save Score: {best_score:.4f} Model')\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            'model': model.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                        f'{self.config.output_dir}{self.config.ckpt_name}_fold{f}_best.pth')\n",
    "\n",
    "            predictions = torch.load(\n",
    "                f'{self.config.output_dir}{self.config.ckpt_name}_fold{f}_best.pth', \n",
    "                map_location=torch.device('cpu'))['predictions']\n",
    "            vl_df[[i for i in range(self.config.max_len)]] = predictions\n",
    "            oof_df = pd.concat([oof_df, vl_df])\n",
    "            LOGGER.info(f\"========== fold: {f} result ==========\")\n",
    "            get_result(vl_df, self.tokenizer, self.config.max_len)\n",
    "            oof_df.to_pickle(f'{self.config.output_dir}oof_df_fold{f}.pkl')\n",
    "            wandb.alert(\n",
    "                title=f\"fold{f} Finished\", \n",
    "                text=f'{self.config.model} has finished its fold{f} running.'\n",
    "            )\n",
    "        \n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df, self.tokenizer, self.config.max_len)\n",
    "        oof_df.to_pickle(self.config.output_dir+'oof_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "Aur_3U8GR8Hd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1650984005911,
     "user": {
      "displayName": "Taro Masuda",
      "userId": "07311423760250806954"
     },
     "user_tz": -540
    },
    "id": "Aur_3U8GR8Hd",
    "outputId": "a178df09-ac55-404c-b5ae-32578303162a"
   },
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db38bc60",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "db38bc60",
    "outputId": "283bcced-9a53-496d-9625-3ffd47fb34be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Some weights of the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/536] Elapsed 0m 1s (remain 17m 21s) Loss: 1.0100(1.0100) Grad: inf  LR: 0.00002500  \n",
      "Epoch: [1][100/536] Elapsed 0m 48s (remain 3m 27s) Loss: 0.0175(0.1018) Grad: 1823.3350  LR: 0.00002494  \n",
      "Epoch: [1][200/536] Elapsed 1m 34s (remain 2m 37s) Loss: 0.0192(0.0605) Grad: 2266.5237  LR: 0.00002476  \n",
      "Epoch: [1][300/536] Elapsed 2m 21s (remain 1m 50s) Loss: 0.0055(0.0452) Grad: 595.0005  LR: 0.00002446  \n",
      "Epoch: [1][400/536] Elapsed 3m 7s (remain 1m 3s) Loss: 0.0121(0.0375) Grad: 1397.3009  LR: 0.00002405  \n",
      "Epoch: [1][500/536] Elapsed 3m 53s (remain 0m 16s) Loss: 0.0167(0.0328) Grad: 1205.3877  LR: 0.00002353  \n",
      "Epoch: [1][535/536] Elapsed 4m 10s (remain 0m 0s) Loss: 0.0136(0.0315) Grad: 1594.2166  LR: 0.00002333  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 1m 3s) Loss: 0.0072(0.0072) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0062(0.0120) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0315  avg_val_loss: 0.0120  time: 312s\n",
      "Epoch 1 - Score: 0.8455 for th=0.425\n",
      "Epoch 1 - Save Score: 0.8455 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/536] Elapsed 0m 0s (remain 6m 10s) Loss: 0.0082(0.0082) Grad: 26633.9746  LR: 0.00002332  \n",
      "Epoch: [2][100/536] Elapsed 0m 47s (remain 3m 23s) Loss: 0.0166(0.0096) Grad: 18584.1074  LR: 0.00002266  \n",
      "Epoch: [2][200/536] Elapsed 1m 33s (remain 2m 36s) Loss: 0.0098(0.0094) Grad: 20383.0352  LR: 0.00002190  \n",
      "Epoch: [2][300/536] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0113(0.0095) Grad: 99047.1250  LR: 0.00002105  \n",
      "Epoch: [2][400/536] Elapsed 3m 6s (remain 1m 2s) Loss: 0.0054(0.0090) Grad: 10198.6309  LR: 0.00002012  \n",
      "Epoch: [2][500/536] Elapsed 3m 53s (remain 0m 16s) Loss: 0.0042(0.0087) Grad: 26291.7109  LR: 0.00001912  \n",
      "Epoch: [2][535/536] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0067(0.0086) Grad: 14682.9043  LR: 0.00001875  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 43s) Loss: 0.0054(0.0054) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0040(0.0120) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0086  avg_val_loss: 0.0120  time: 310s\n",
      "Epoch 2 - Score: 0.8685 for th=0.44\n",
      "Epoch 2 - Save Score: 0.8685 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/536] Elapsed 0m 0s (remain 6m 16s) Loss: 0.0056(0.0056) Grad: 8955.5449  LR: 0.00001874  \n",
      "Epoch: [3][100/536] Elapsed 0m 47s (remain 3m 23s) Loss: 0.0042(0.0072) Grad: 18308.7227  LR: 0.00001766  \n",
      "Epoch: [3][200/536] Elapsed 1m 33s (remain 2m 36s) Loss: 0.0112(0.0070) Grad: 29037.3203  LR: 0.00001652  \n",
      "Epoch: [3][300/536] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0142(0.0071) Grad: 24376.8105  LR: 0.00001535  \n",
      "Epoch: [3][400/536] Elapsed 3m 6s (remain 1m 2s) Loss: 0.0038(0.0072) Grad: 11914.4951  LR: 0.00001415  \n",
      "Epoch: [3][500/536] Elapsed 3m 52s (remain 0m 16s) Loss: 0.0020(0.0074) Grad: 6756.7559  LR: 0.00001293  \n",
      "Epoch: [3][535/536] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0017(0.0074) Grad: 6487.9727  LR: 0.00001251  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 42s) Loss: 0.0066(0.0066) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0037(0.0120) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0074  avg_val_loss: 0.0120  time: 310s\n",
      "Epoch 3 - Score: 0.8713 for th=0.69\n",
      "Epoch 3 - Save Score: 0.8713 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/536] Elapsed 0m 1s (remain 10m 37s) Loss: 0.0041(0.0041) Grad: 27950.6270  LR: 0.00001249  \n",
      "Epoch: [4][100/536] Elapsed 1m 36s (remain 6m 53s) Loss: 0.0083(0.0061) Grad: 18009.5449  LR: 0.00001128  \n",
      "Epoch: [4][200/536] Elapsed 3m 10s (remain 5m 18s) Loss: 0.0025(0.0068) Grad: 11856.3486  LR: 0.00001007  \n",
      "Epoch: [4][300/536] Elapsed 4m 45s (remain 3m 43s) Loss: 0.0058(0.0070) Grad: 16031.7510  LR: 0.00000888  \n",
      "Epoch: [4][400/536] Elapsed 6m 20s (remain 2m 8s) Loss: 0.0049(0.0071) Grad: 13918.9570  LR: 0.00000773  \n",
      "Epoch: [4][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0182(0.0073) Grad: 40378.0547  LR: 0.00000663  \n",
      "Epoch: [4][535/536] Elapsed 8m 28s (remain 0m 0s) Loss: 0.0123(0.0074) Grad: 33532.6055  LR: 0.00000626  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 43s) Loss: 0.0064(0.0064) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0040(0.0102) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0074  avg_val_loss: 0.0102  time: 570s\n",
      "Epoch 4 - Score: 0.8711 for th=0.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/536] Elapsed 0m 1s (remain 10m 48s) Loss: 0.0057(0.0057) Grad: 16562.6641  LR: 0.00000625  \n",
      "Epoch: [5][100/536] Elapsed 1m 36s (remain 6m 53s) Loss: 0.0065(0.0079) Grad: 16298.3857  LR: 0.00000522  \n",
      "Epoch: [5][200/536] Elapsed 3m 10s (remain 5m 18s) Loss: 0.0057(0.0078) Grad: 11654.2705  LR: 0.00000426  \n",
      "Epoch: [5][300/536] Elapsed 4m 45s (remain 3m 43s) Loss: 0.0058(0.0077) Grad: 15684.2666  LR: 0.00000339  \n",
      "Epoch: [5][400/536] Elapsed 6m 20s (remain 2m 8s) Loss: 0.0066(0.0076) Grad: 18909.7441  LR: 0.00000260  \n",
      "Epoch: [5][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0085(0.0074) Grad: 20489.3184  LR: 0.00000190  \n",
      "Epoch: [5][535/536] Elapsed 8m 28s (remain 0m 0s) Loss: 0.0051(0.0074) Grad: 13307.4805  LR: 0.00000168  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 43s) Loss: 0.0064(0.0064) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0040(0.0103) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0074  avg_val_loss: 0.0103  time: 570s\n",
      "Epoch 5 - Score: 0.8714 for th=0.43\n",
      "Epoch 5 - Save Score: 0.8714 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/536] Elapsed 0m 1s (remain 10m 50s) Loss: 0.0078(0.0078) Grad: 13731.1475  LR: 0.00000167  \n",
      "Epoch: [6][100/536] Elapsed 1m 36s (remain 6m 53s) Loss: 0.0049(0.0076) Grad: 24871.3574  LR: 0.00000112  \n",
      "Epoch: [6][200/536] Elapsed 3m 11s (remain 5m 18s) Loss: 0.0102(0.0077) Grad: 18878.1973  LR: 0.00000067  \n",
      "Epoch: [6][300/536] Elapsed 4m 45s (remain 3m 43s) Loss: 0.0062(0.0075) Grad: 5232.0850  LR: 0.00000033  \n",
      "Epoch: [6][400/536] Elapsed 6m 20s (remain 2m 8s) Loss: 0.0060(0.0074) Grad: 2670.3279  LR: 0.00000011  \n",
      "Epoch: [6][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0031(0.0073) Grad: 7684.1743  LR: 0.00000001  \n",
      "Epoch: [6][535/536] Elapsed 8m 28s (remain 0m 0s) Loss: 0.0046(0.0073) Grad: 4979.0625  LR: 0.00000000  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 43s) Loss: 0.0065(0.0065) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0040(0.0103) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0073  avg_val_loss: 0.0103  time: 570s\n",
      "Epoch 6 - Score: 0.8714 for th=0.48\n",
      "========== fold: 0 result ==========\n",
      "Score: 0.8714 Best threshold:: 0.43\n",
      "========== fold: 1 training ==========\n",
      "Some weights of the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/536] Elapsed 0m 0s (remain 6m 6s) Loss: 0.6834(0.6834) Grad: inf  LR: 0.00002500  \n",
      "Epoch: [1][100/536] Elapsed 0m 47s (remain 3m 22s) Loss: 0.0215(0.0855) Grad: 2322.1880  LR: 0.00002494  \n",
      "Epoch: [1][200/536] Elapsed 1m 33s (remain 2m 35s) Loss: 0.0067(0.0517) Grad: 1320.7733  LR: 0.00002476  \n",
      "Epoch: [1][300/536] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0228(0.0396) Grad: 1902.7981  LR: 0.00002446  \n",
      "Epoch: [1][400/536] Elapsed 3m 6s (remain 1m 2s) Loss: 0.0117(0.0332) Grad: 1902.3706  LR: 0.00002405  \n",
      "Epoch: [1][500/536] Elapsed 3m 53s (remain 0m 16s) Loss: 0.0079(0.0292) Grad: 1072.1298  LR: 0.00002353  \n",
      "Epoch: [1][535/536] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0129(0.0282) Grad: 2309.6772  LR: 0.00002333  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 46s) Loss: 0.0103(0.0103) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0085(0.0127) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0282  avg_val_loss: 0.0127  time: 312s\n",
      "Epoch 1 - Score: 0.8536 for th=0.48\n",
      "Epoch 1 - Save Score: 0.8536 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/536] Elapsed 0m 0s (remain 6m 28s) Loss: 0.0096(0.0096) Grad: 14121.2314  LR: 0.00002332  \n",
      "Epoch: [2][100/536] Elapsed 0m 47s (remain 3m 24s) Loss: 0.0109(0.0093) Grad: 18464.1543  LR: 0.00002266  \n",
      "Epoch: [2][200/536] Elapsed 1m 33s (remain 2m 36s) Loss: 0.0113(0.0088) Grad: 32455.6387  LR: 0.00002190  \n",
      "Epoch: [2][300/536] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0153(0.0088) Grad: 14081.5410  LR: 0.00002105  \n",
      "Epoch: [2][400/536] Elapsed 3m 7s (remain 1m 3s) Loss: 0.0049(0.0087) Grad: 6729.8667  LR: 0.00002012  \n",
      "Epoch: [2][500/536] Elapsed 3m 54s (remain 0m 16s) Loss: 0.0218(0.0086) Grad: 23131.8926  LR: 0.00001912  \n",
      "Epoch: [2][535/536] Elapsed 4m 10s (remain 0m 0s) Loss: 0.0052(0.0086) Grad: 6627.4390  LR: 0.00001875  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 45s) Loss: 0.0087(0.0087) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0054(0.0129) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0086  avg_val_loss: 0.0129  time: 312s\n",
      "Epoch 2 - Score: 0.8698 for th=0.54\n",
      "Epoch 2 - Save Score: 0.8698 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/536] Elapsed 0m 0s (remain 6m 34s) Loss: 0.0117(0.0117) Grad: 28640.3066  LR: 0.00001874  \n",
      "Epoch: [3][100/536] Elapsed 0m 47s (remain 3m 23s) Loss: 0.0109(0.0070) Grad: 69227.7031  LR: 0.00001766  \n",
      "Epoch: [3][200/536] Elapsed 1m 34s (remain 2m 36s) Loss: 0.0103(0.0068) Grad: 9242.8389  LR: 0.00001652  \n",
      "Epoch: [3][300/536] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0090(0.0070) Grad: 20590.9922  LR: 0.00001535  \n",
      "Epoch: [3][400/536] Elapsed 3m 7s (remain 1m 3s) Loss: 0.0142(0.0073) Grad: 22723.6035  LR: 0.00001415  \n",
      "Epoch: [3][500/536] Elapsed 3m 53s (remain 0m 16s) Loss: 0.0111(0.0072) Grad: 11569.3740  LR: 0.00001293  \n",
      "Epoch: [3][535/536] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0043(0.0071) Grad: 8940.6592  LR: 0.00001251  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 45s) Loss: 0.0079(0.0079) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0063(0.0133) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0071  avg_val_loss: 0.0133  time: 311s\n",
      "Epoch 3 - Score: 0.8734 for th=0.44\n",
      "Epoch 3 - Save Score: 0.8734 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/536] Elapsed 0m 1s (remain 10m 51s) Loss: 0.0003(0.0003) Grad: 10610.1562  LR: 0.00001249  \n",
      "Epoch: [4][100/536] Elapsed 1m 36s (remain 6m 53s) Loss: 0.0059(0.0061) Grad: 25984.9629  LR: 0.00001128  \n",
      "Epoch: [4][200/536] Elapsed 3m 10s (remain 5m 18s) Loss: 0.0049(0.0063) Grad: 23973.0352  LR: 0.00001007  \n",
      "Epoch: [4][300/536] Elapsed 4m 45s (remain 3m 43s) Loss: 0.0024(0.0065) Grad: 12727.7061  LR: 0.00000888  \n",
      "Epoch: [4][400/536] Elapsed 6m 20s (remain 2m 8s) Loss: 0.0039(0.0065) Grad: 6477.3750  LR: 0.00000773  \n",
      "Epoch: [4][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0078(0.0067) Grad: 11489.7334  LR: 0.00000663  \n",
      "Epoch: [4][535/536] Elapsed 8m 28s (remain 0m 0s) Loss: 0.0062(0.0067) Grad: 5159.6460  LR: 0.00000626  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 45s) Loss: 0.0080(0.0080) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0060(0.0116) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0067  avg_val_loss: 0.0116  time: 570s\n",
      "Epoch 4 - Score: 0.8729 for th=0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/536] Elapsed 0m 1s (remain 10m 53s) Loss: 0.0042(0.0042) Grad: 11215.3828  LR: 0.00000625  \n",
      "Epoch: [5][100/536] Elapsed 1m 36s (remain 6m 54s) Loss: 0.0063(0.0070) Grad: 11363.3701  LR: 0.00000522  \n",
      "Epoch: [5][200/536] Elapsed 3m 11s (remain 5m 18s) Loss: 0.0086(0.0070) Grad: 8985.7451  LR: 0.00000426  \n",
      "Epoch: [5][300/536] Elapsed 4m 45s (remain 3m 43s) Loss: 0.0082(0.0070) Grad: 13181.6045  LR: 0.00000339  \n",
      "Epoch: [5][400/536] Elapsed 6m 20s (remain 2m 8s) Loss: 0.0029(0.0070) Grad: 6567.2407  LR: 0.00000260  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0048(0.0069) Grad: 21793.7109  LR: 0.00000190  \n",
      "Epoch: [5][535/536] Elapsed 8m 28s (remain 0m 0s) Loss: 0.0037(0.0069) Grad: 11456.5762  LR: 0.00000168  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 44s) Loss: 0.0079(0.0079) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0059(0.0115) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0069  avg_val_loss: 0.0115  time: 570s\n",
      "Epoch 5 - Score: 0.8741 for th=0.405\n",
      "Epoch 5 - Save Score: 0.8741 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/536] Elapsed 0m 1s (remain 10m 57s) Loss: 0.0030(0.0030) Grad: 16280.0947  LR: 0.00000167  \n",
      "Epoch: [6][100/536] Elapsed 1m 36s (remain 6m 54s) Loss: 0.0123(0.0069) Grad: 17403.9707  LR: 0.00000112  \n",
      "Epoch: [6][200/536] Elapsed 3m 10s (remain 5m 18s) Loss: 0.0038(0.0068) Grad: 10376.6797  LR: 0.00000067  \n",
      "Epoch: [6][300/536] Elapsed 4m 45s (remain 3m 43s) Loss: 0.0085(0.0070) Grad: 10646.8438  LR: 0.00000033  \n",
      "Epoch: [6][400/536] Elapsed 6m 20s (remain 2m 8s) Loss: 0.0090(0.0069) Grad: 11348.4824  LR: 0.00000011  \n",
      "Epoch: [6][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0032(0.0068) Grad: 14211.6279  LR: 0.00000001  \n",
      "Epoch: [6][535/536] Elapsed 8m 28s (remain 0m 0s) Loss: 0.0099(0.0068) Grad: 24715.3594  LR: 0.00000000  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 45s) Loss: 0.0084(0.0084) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0057(0.0114) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0068  avg_val_loss: 0.0114  time: 570s\n",
      "Epoch 6 - Score: 0.8742 for th=0.46\n",
      "Epoch 6 - Save Score: 0.8742 Model\n",
      "========== fold: 1 result ==========\n",
      "Score: 0.8742 Best threshold:: 0.46\n",
      "========== fold: 2 training ==========\n",
      "Some weights of the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/536] Elapsed 0m 0s (remain 6m 34s) Loss: 0.5208(0.5208) Grad: inf  LR: 0.00002500  \n",
      "Epoch: [1][100/536] Elapsed 0m 47s (remain 3m 23s) Loss: 0.0086(0.0703) Grad: 887.0882  LR: 0.00002494  \n",
      "Epoch: [1][200/536] Elapsed 1m 33s (remain 2m 36s) Loss: 0.0156(0.0444) Grad: 2012.3204  LR: 0.00002476  \n",
      "Epoch: [1][300/536] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0075(0.0349) Grad: 4807.9292  LR: 0.00002446  \n",
      "Epoch: [1][400/536] Elapsed 3m 6s (remain 1m 2s) Loss: 0.0183(0.0299) Grad: 2409.5039  LR: 0.00002405  \n",
      "Epoch: [1][500/536] Elapsed 3m 53s (remain 0m 16s) Loss: 0.0093(0.0265) Grad: 808.6677  LR: 0.00002353  \n",
      "Epoch: [1][535/536] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0154(0.0256) Grad: 1916.4004  LR: 0.00002333  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 46s) Loss: 0.0115(0.0115) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0032(0.0132) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0256  avg_val_loss: 0.0132  time: 311s\n",
      "Epoch 1 - Score: 0.8510 for th=0.3\n",
      "Epoch 1 - Save Score: 0.8510 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/536] Elapsed 0m 0s (remain 6m 56s) Loss: 0.0153(0.0153) Grad: 26106.6484  LR: 0.00002332  \n",
      "Epoch: [2][100/536] Elapsed 0m 47s (remain 3m 23s) Loss: 0.0051(0.0099) Grad: 21137.3945  LR: 0.00002266  \n",
      "Epoch: [2][200/536] Elapsed 1m 33s (remain 2m 36s) Loss: 0.0065(0.0097) Grad: 28581.5176  LR: 0.00002190  \n",
      "Epoch: [2][300/536] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0091(0.0097) Grad: 37987.3320  LR: 0.00002105  \n",
      "Epoch: [2][400/536] Elapsed 3m 6s (remain 1m 2s) Loss: 0.0014(0.0095) Grad: 4227.0176  LR: 0.00002012  \n",
      "Epoch: [2][500/536] Elapsed 3m 53s (remain 0m 16s) Loss: 0.0111(0.0093) Grad: 12070.3203  LR: 0.00001912  \n",
      "Epoch: [2][535/536] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0049(0.0093) Grad: 38372.8086  LR: 0.00001875  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 44s) Loss: 0.0106(0.0106) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0028(0.0110) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0093  avg_val_loss: 0.0110  time: 311s\n",
      "Epoch 2 - Score: 0.8709 for th=0.36\n",
      "Epoch 2 - Save Score: 0.8709 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/536] Elapsed 0m 0s (remain 7m 3s) Loss: 0.0175(0.0175) Grad: 23301.1348  LR: 0.00001874  \n",
      "Epoch: [3][100/536] Elapsed 0m 47s (remain 3m 23s) Loss: 0.0153(0.0084) Grad: 23306.2070  LR: 0.00001766  \n",
      "Epoch: [3][200/536] Elapsed 1m 33s (remain 2m 36s) Loss: 0.0043(0.0079) Grad: 14913.9502  LR: 0.00001652  \n",
      "Epoch: [3][300/536] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0057(0.0080) Grad: 13773.3398  LR: 0.00001535  \n",
      "Epoch: [3][400/536] Elapsed 3m 6s (remain 1m 2s) Loss: 0.0060(0.0079) Grad: 15805.5195  LR: 0.00001415  \n",
      "Epoch: [3][500/536] Elapsed 3m 53s (remain 0m 16s) Loss: 0.0095(0.0079) Grad: 5287.1934  LR: 0.00001293  \n",
      "Epoch: [3][535/536] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0078(0.0078) Grad: 13772.1709  LR: 0.00001251  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 46s) Loss: 0.0097(0.0097) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0024(0.0113) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0078  avg_val_loss: 0.0113  time: 311s\n",
      "Epoch 3 - Score: 0.8722 for th=0.46\n",
      "Epoch 3 - Save Score: 0.8722 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/536] Elapsed 0m 1s (remain 11m 19s) Loss: 0.0080(0.0080) Grad: 46201.9219  LR: 0.00001249  \n",
      "Epoch: [4][100/536] Elapsed 1m 36s (remain 6m 54s) Loss: 0.0125(0.0074) Grad: 22007.4141  LR: 0.00001128  \n",
      "Epoch: [4][200/536] Elapsed 3m 11s (remain 5m 18s) Loss: 0.0055(0.0075) Grad: 8330.0840  LR: 0.00001007  \n",
      "Epoch: [4][300/536] Elapsed 4m 46s (remain 3m 43s) Loss: 0.0071(0.0073) Grad: 12745.3447  LR: 0.00000888  \n",
      "Epoch: [4][400/536] Elapsed 6m 20s (remain 2m 8s) Loss: 0.0089(0.0075) Grad: 9868.7256  LR: 0.00000773  \n",
      "Epoch: [4][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0107(0.0076) Grad: 17172.0156  LR: 0.00000663  \n",
      "Epoch: [4][535/536] Elapsed 8m 28s (remain 0m 0s) Loss: 0.0049(0.0076) Grad: 4214.0073  LR: 0.00000626  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 46s) Loss: 0.0089(0.0089) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0028(0.0102) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0076  avg_val_loss: 0.0102  time: 571s\n",
      "Epoch 4 - Score: 0.8764 for th=0.395\n",
      "Epoch 4 - Save Score: 0.8764 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/536] Elapsed 0m 1s (remain 11m 12s) Loss: 0.0038(0.0038) Grad: 12542.9844  LR: 0.00000625  \n",
      "Epoch: [5][100/536] Elapsed 1m 36s (remain 6m 54s) Loss: 0.0078(0.0078) Grad: 23899.9160  LR: 0.00000522  \n",
      "Epoch: [5][200/536] Elapsed 3m 11s (remain 5m 18s) Loss: 0.0255(0.0081) Grad: 25413.6191  LR: 0.00000426  \n",
      "Epoch: [5][300/536] Elapsed 4m 46s (remain 3m 43s) Loss: 0.0034(0.0080) Grad: 8632.0850  LR: 0.00000339  \n",
      "Epoch: [5][400/536] Elapsed 6m 20s (remain 2m 8s) Loss: 0.0075(0.0077) Grad: 9481.4736  LR: 0.00000260  \n",
      "Epoch: [5][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0090(0.0076) Grad: 25134.0977  LR: 0.00000190  \n",
      "Epoch: [5][535/536] Elapsed 8m 28s (remain 0m 0s) Loss: 0.0045(0.0076) Grad: 11855.4316  LR: 0.00000168  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 45s) Loss: 0.0090(0.0090) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0028(0.0100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0076  avg_val_loss: 0.0100  time: 571s\n",
      "Epoch 5 - Score: 0.8774 for th=0.445\n",
      "Epoch 5 - Save Score: 0.8774 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/536] Elapsed 0m 1s (remain 11m 28s) Loss: 0.0078(0.0078) Grad: 19204.2344  LR: 0.00000167  \n",
      "Epoch: [6][100/536] Elapsed 1m 36s (remain 6m 54s) Loss: 0.0074(0.0071) Grad: 13970.5771  LR: 0.00000112  \n",
      "Epoch: [6][200/536] Elapsed 3m 11s (remain 5m 18s) Loss: 0.0079(0.0074) Grad: 26371.4375  LR: 0.00000067  \n",
      "Epoch: [6][300/536] Elapsed 4m 46s (remain 3m 43s) Loss: 0.0067(0.0075) Grad: 4257.5078  LR: 0.00000033  \n",
      "Epoch: [6][400/536] Elapsed 6m 20s (remain 2m 8s) Loss: 0.0032(0.0076) Grad: 4372.7925  LR: 0.00000011  \n",
      "Epoch: [6][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0132(0.0075) Grad: 12269.8691  LR: 0.00000001  \n",
      "Epoch: [6][535/536] Elapsed 8m 29s (remain 0m 0s) Loss: 0.0040(0.0075) Grad: 5337.6777  LR: 0.00000000  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 46s) Loss: 0.0090(0.0090) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0027(0.0100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0075  avg_val_loss: 0.0100  time: 571s\n",
      "Epoch 6 - Score: 0.8779 for th=0.475\n",
      "Epoch 6 - Save Score: 0.8779 Model\n",
      "========== fold: 2 result ==========\n",
      "Score: 0.8779 Best threshold:: 0.475\n",
      "========== fold: 3 training ==========\n",
      "Some weights of the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/536] Elapsed 0m 0s (remain 6m 34s) Loss: 0.8639(0.8639) Grad: inf  LR: 0.00002500  \n",
      "Epoch: [1][100/536] Elapsed 0m 47s (remain 3m 23s) Loss: 0.0219(0.0992) Grad: 2306.2725  LR: 0.00002494  \n",
      "Epoch: [1][200/536] Elapsed 1m 33s (remain 2m 36s) Loss: 0.0105(0.0596) Grad: 1351.3083  LR: 0.00002476  \n",
      "Epoch: [1][300/536] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0080(0.0448) Grad: 1068.5028  LR: 0.00002446  \n",
      "Epoch: [1][400/536] Elapsed 3m 6s (remain 1m 2s) Loss: 0.0327(0.0371) Grad: 5284.2358  LR: 0.00002405  \n",
      "Epoch: [1][500/536] Elapsed 3m 53s (remain 0m 16s) Loss: 0.0102(0.0326) Grad: 2643.2878  LR: 0.00002353  \n",
      "Epoch: [1][535/536] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0100(0.0312) Grad: 1106.7229  LR: 0.00002333  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 48s) Loss: 0.0109(0.0109) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0030(0.0120) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0312  avg_val_loss: 0.0120  time: 312s\n",
      "Epoch 1 - Score: 0.8593 for th=0.6\n",
      "Epoch 1 - Save Score: 0.8593 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/536] Elapsed 0m 0s (remain 7m 10s) Loss: 0.0087(0.0087) Grad: 19818.9727  LR: 0.00002332  \n",
      "Epoch: [2][100/536] Elapsed 0m 47s (remain 3m 23s) Loss: 0.0141(0.0100) Grad: 17341.0215  LR: 0.00002266  \n",
      "Epoch: [2][200/536] Elapsed 1m 33s (remain 2m 36s) Loss: 0.0251(0.0101) Grad: 79657.7031  LR: 0.00002190  \n",
      "Epoch: [2][300/536] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0119(0.0097) Grad: 44633.3945  LR: 0.00002105  \n",
      "Epoch: [2][400/536] Elapsed 3m 6s (remain 1m 2s) Loss: 0.0115(0.0095) Grad: 15747.3145  LR: 0.00002012  \n",
      "Epoch: [2][500/536] Elapsed 3m 53s (remain 0m 16s) Loss: 0.0117(0.0093) Grad: 27500.1973  LR: 0.00001912  \n",
      "Epoch: [2][535/536] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0033(0.0093) Grad: 21817.3672  LR: 0.00001875  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 45s) Loss: 0.0103(0.0103) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0025(0.0104) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0093  avg_val_loss: 0.0104  time: 311s\n",
      "Epoch 2 - Score: 0.8719 for th=0.55\n",
      "Epoch 2 - Save Score: 0.8719 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/536] Elapsed 0m 0s (remain 7m 9s) Loss: 0.0134(0.0134) Grad: 39109.5078  LR: 0.00001874  \n",
      "Epoch: [3][100/536] Elapsed 0m 47s (remain 3m 23s) Loss: 0.0041(0.0079) Grad: 10171.5596  LR: 0.00001766  \n",
      "Epoch: [3][200/536] Elapsed 1m 33s (remain 2m 36s) Loss: 0.0061(0.0081) Grad: 16644.5430  LR: 0.00001652  \n",
      "Epoch: [3][300/536] Elapsed 2m 20s (remain 1m 49s) Loss: 0.0060(0.0082) Grad: 5233.3794  LR: 0.00001535  \n",
      "Epoch: [3][400/536] Elapsed 3m 6s (remain 1m 2s) Loss: 0.0038(0.0079) Grad: 9373.9385  LR: 0.00001415  \n",
      "Epoch: [3][500/536] Elapsed 3m 53s (remain 0m 16s) Loss: 0.0061(0.0081) Grad: 8099.7939  LR: 0.00001293  \n",
      "Epoch: [3][535/536] Elapsed 4m 9s (remain 0m 0s) Loss: 0.0104(0.0080) Grad: 7092.3174  LR: 0.00001251  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 45s) Loss: 0.0073(0.0073) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0024(0.0106) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0080  avg_val_loss: 0.0106  time: 311s\n",
      "Epoch 3 - Score: 0.8782 for th=0.51\n",
      "Epoch 3 - Save Score: 0.8782 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/536] Elapsed 0m 1s (remain 11m 17s) Loss: 0.0013(0.0013) Grad: 55194.2383  LR: 0.00001249  \n",
      "Epoch: [4][100/536] Elapsed 1m 36s (remain 6m 54s) Loss: 0.0169(0.0067) Grad: 39086.2617  LR: 0.00001128  \n",
      "Epoch: [4][200/536] Elapsed 3m 11s (remain 5m 18s) Loss: 0.0068(0.0071) Grad: 27038.4961  LR: 0.00001007  \n",
      "Epoch: [4][300/536] Elapsed 4m 46s (remain 3m 43s) Loss: 0.0086(0.0072) Grad: 28683.6152  LR: 0.00000888  \n",
      "Epoch: [4][400/536] Elapsed 6m 21s (remain 2m 8s) Loss: 0.0160(0.0072) Grad: 27045.0566  LR: 0.00000773  \n",
      "Epoch: [4][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0093(0.0072) Grad: 22729.2715  LR: 0.00000663  \n",
      "Epoch: [4][535/536] Elapsed 8m 29s (remain 0m 0s) Loss: 0.0080(0.0072) Grad: 14457.7959  LR: 0.00000626  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 46s) Loss: 0.0066(0.0066) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0022(0.0094) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0072  avg_val_loss: 0.0094  time: 571s\n",
      "Epoch 4 - Score: 0.8774 for th=0.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/536] Elapsed 0m 1s (remain 11m 31s) Loss: 0.0040(0.0040) Grad: 11441.0498  LR: 0.00000625  \n",
      "Epoch: [5][100/536] Elapsed 1m 36s (remain 6m 54s) Loss: 0.0115(0.0077) Grad: 7952.9248  LR: 0.00000522  \n",
      "Epoch: [5][200/536] Elapsed 3m 11s (remain 5m 18s) Loss: 0.0067(0.0074) Grad: 10385.5156  LR: 0.00000426  \n",
      "Epoch: [5][300/536] Elapsed 4m 46s (remain 3m 43s) Loss: 0.0079(0.0075) Grad: 5481.4585  LR: 0.00000339  \n",
      "Epoch: [5][400/536] Elapsed 6m 21s (remain 2m 8s) Loss: 0.0100(0.0076) Grad: 21617.3652  LR: 0.00000260  \n",
      "Epoch: [5][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0107(0.0075) Grad: 6883.7407  LR: 0.00000190  \n",
      "Epoch: [5][535/536] Elapsed 8m 29s (remain 0m 0s) Loss: 0.0137(0.0074) Grad: 23283.0840  LR: 0.00000168  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 45s) Loss: 0.0065(0.0065) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0022(0.0094) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0074  avg_val_loss: 0.0094  time: 571s\n",
      "Epoch 5 - Score: 0.8773 for th=0.435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/536] Elapsed 0m 1s (remain 11m 21s) Loss: 0.0059(0.0059) Grad: 65134.3828  LR: 0.00000167  \n",
      "Epoch: [6][100/536] Elapsed 1m 36s (remain 6m 54s) Loss: 0.0093(0.0075) Grad: 14710.8936  LR: 0.00000112  \n",
      "Epoch: [6][200/536] Elapsed 3m 11s (remain 5m 18s) Loss: 0.0093(0.0074) Grad: 12717.3047  LR: 0.00000067  \n",
      "Epoch: [6][300/536] Elapsed 4m 46s (remain 3m 43s) Loss: 0.0058(0.0073) Grad: 30583.7031  LR: 0.00000033  \n",
      "Epoch: [6][400/536] Elapsed 6m 20s (remain 2m 8s) Loss: 0.0192(0.0074) Grad: 13572.1455  LR: 0.00000011  \n",
      "Epoch: [6][500/536] Elapsed 7m 55s (remain 0m 33s) Loss: 0.0081(0.0074) Grad: 16094.4346  LR: 0.00000001  \n",
      "Epoch: [6][535/536] Elapsed 8m 29s (remain 0m 0s) Loss: 0.0056(0.0074) Grad: 26682.3398  LR: 0.00000000  \n",
      "EVAL: [0/90] Elapsed 0m 0s (remain 0m 46s) Loss: 0.0065(0.0065) \n",
      "EVAL: [89/90] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0023(0.0094) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0074  avg_val_loss: 0.0094  time: 571s\n",
      "Epoch 6 - Score: 0.8775 for th=0.45\n",
      "========== fold: 3 result ==========\n",
      "Score: 0.8782 Best threshold:: 0.51\n",
      "========== CV ==========\n",
      "Score: 0.8749 Best threshold:: 0.48\n"
     ]
    }
   ],
   "source": [
    "config.model = '../output/roberta-large-self-supervised-learning-9epoch'\n",
    "trainer = Trainer(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.run(\n",
    "    df=df_train,\n",
    "    feature_text_max_len=feature_text_max_len, \n",
    "    pn_history_max_len=pn_history_max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5a96081-3aa7-4948-be95-67e31d3bd7e6",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "f5a96081-3aa7-4948-be95-67e31d3bd7e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncommit_msg = \\'\"run_name: \\' + wandb.run.name[:wandb.run.name.rfind(\\'-\\')] + \\'\"\\'\\nwandb.finish()\\n!git config --global user.email \"taro.masuda.jp@gmail.com\"\\n!git config --global user.name \"taro-masuda\"\\n!git add baseline-train.ipynb\\n!git add ../input/raiii-nbme/config.yml\\n!git commit -m $commit_msg\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "commit_msg = '\"run_name: ' + wandb.run.name[:wandb.run.name.rfind('-')] + '\"'\n",
    "wandb.finish()\n",
    "!git config --global user.email \"taro.masuda.jp@gmail.com\"\n",
    "!git config --global user.name \"taro-masuda\"\n",
    "!git add baseline-train.ipynb\n",
    "!git add ../input/raiii-nbme/config.yml\n",
    "!git commit -m $commit_msg\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "Fzcomkp8UBKT",
   "metadata": {
    "id": "Fzcomkp8UBKT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../output/roberta-large-self-supervised-learning-9epoch and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "            model = CustomModel(\n",
    "                config.model, \n",
    "                config_path=None, \n",
    "                pretrained=True).to(torch.device('cuda' if cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "474209ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.config, \"../output/exp076/config.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4f4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "exp076_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-02T05:40:40.926468",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
