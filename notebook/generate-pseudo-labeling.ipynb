{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forked from skraiii's notebook https://www.kaggle.com/skraiii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T08:25:16.693859Z",
     "iopub.status.busy": "2022-04-18T08:25:16.693542Z",
     "iopub.status.idle": "2022-04-18T08:25:16.699611Z",
     "shell.execute_reply": "2022-04-18T08:25:16.699056Z",
     "shell.execute_reply.started": "2022-04-18T08:25:16.693824Z"
    }
   },
   "source": [
    "# Pseudo Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T14:29:01.523327Z",
     "iopub.status.busy": "2022-04-25T14:29:01.521984Z",
     "iopub.status.idle": "2022-04-25T14:29:02.404595Z",
     "shell.execute_reply": "2022-04-25T14:29:02.40366Z",
     "shell.execute_reply.started": "2022-04-25T14:29:01.523271Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIR = Path(\"../../input\")\n",
    "test = pd.read_csv(INPUT_DIR / \"test.csv\")\n",
    "train = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "patient_notes = pd.read_csv(INPUT_DIR / \"patient_notes.csv\")\n",
    "features = pd.read_csv(INPUT_DIR / \"features.csv\")\n",
    "oof = pd.read_pickle(INPUT_DIR / \"exp038-nbme-microsoft-deberta-v3-large/oof_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T13:40:46.785463Z",
     "iopub.status.busy": "2022-04-25T13:40:46.785134Z",
     "iopub.status.idle": "2022-04-25T13:40:46.79368Z",
     "shell.execute_reply": "2022-04-25T13:40:46.792738Z",
     "shell.execute_reply.started": "2022-04-25T13:40:46.785428Z"
    }
   },
   "outputs": [],
   "source": [
    "pn_nums = patient_notes[\"pn_num\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T14:10:10.833271Z",
     "iopub.status.busy": "2022-04-25T14:10:10.832576Z",
     "iopub.status.idle": "2022-04-25T14:10:12.774147Z",
     "shell.execute_reply": "2022-04-25T14:10:12.772783Z",
     "shell.execute_reply.started": "2022-04-25T14:10:10.833227Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "unique_train_pn_num = train.pn_num.unique().tolist()\n",
    "mask_not_in_train = [True if x not in unique_train_pn_num else False for x in pn_nums]\n",
    "ssl_df = patient_notes.loc[mask_not_in_train].copy()\n",
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "ssl_df[\"fold\"] = -1\n",
    "for f, (t, v) in enumerate(skf.split(ssl_df.pn_history, ssl_df.case_num)):\n",
    "    ssl_df.iloc[v, -1] = f\n",
    "ssl_df.to_csv(\"corpus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T14:32:11.111702Z",
     "iopub.status.busy": "2022-04-25T14:32:11.110568Z",
     "iopub.status.idle": "2022-04-25T14:32:33.559689Z",
     "shell.execute_reply": "2022-04-25T14:32:33.558482Z",
     "shell.execute_reply.started": "2022-04-25T14:32:11.111629Z"
    }
   },
   "outputs": [],
   "source": [
    "features.merge(ssl_df, on=\"case_num\").to_csv(\"pl_train\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:02:08.336027Z",
     "iopub.status.idle": "2022-04-21T17:02:08.336548Z",
     "shell.execute_reply": "2022-04-21T17:02:08.336357Z",
     "shell.execute_reply.started": "2022-04-21T17:02:08.336335Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def get_score(y_true: ndarray, y_pred: ndarray) -> float:\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def micro_f1(preds: list, truths: list) -> float:\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans: list, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(\n",
    "            np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0\n",
    "        )\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:02:08.337565Z",
     "iopub.status.idle": "2022-04-21T17:02:08.338227Z",
     "shell.execute_reply": "2022-04-21T17:02:08.337983Z",
     "shell.execute_reply.started": "2022-04-21T17:02:08.337956Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def create_label(\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_len,\n",
    "    text: str,\n",
    "    annotation_length: int,\n",
    "    location_list: list,\n",
    ") -> Tensor:\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    offset_mapping = encoded[\"offset_mapping\"]\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1\n",
    "    if annotation_length != 0:\n",
    "        for location in location_list:\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                        start_idx = idx - 1\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        end_idx = idx + 1\n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1\n",
    "    return torch.tensor(label[:max_len], dtype=torch.float)\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer) -> list:\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, add_special_tokens=True, return_offsets_mapping=True)\n",
    "        prev_pred = 0\n",
    "        prev_end = -1\n",
    "        for idx, (offset_mapping, pred) in enumerate(\n",
    "            zip(encoded[\"offset_mapping\"], prediction)\n",
    "        ):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "            if start != prev_end:\n",
    "                results[i][prev_end:start] = (pred + prev_pred) / 2\n",
    "            prev_pred = pred\n",
    "            prev_end = end\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs: list, pn_histories: list, th: float = 0.5) -> list:\n",
    "    label_strs = []\n",
    "    for char_prob, pn_history in zip(char_probs, pn_histories):\n",
    "        pos_char_indices = np.where(char_prob > th)[0] + 1\n",
    "        if len(pos_char_indices) > 0 and pos_char_indices[0] == 1:\n",
    "            pos_char_indices = np.hstack([[0], pos_char_indices])\n",
    "        clustered_pos_char_indices = cluster_elements(xs=pos_char_indices)\n",
    "\n",
    "        for i in range(len(clustered_pos_char_indices)):\n",
    "            if len(clustered_pos_char_indices[i]) > 0:\n",
    "\n",
    "                # 1文字目がspaceの場合\n",
    "                target_idx = clustered_pos_char_indices[i][0] - 1\n",
    "                if target_idx > -1 and pn_history[target_idx] != \" \":\n",
    "                    clustered_pos_char_indices[i] = np.hstack(\n",
    "                        [[target_idx], clustered_pos_char_indices[i]]\n",
    "                    )\n",
    "\n",
    "                # 1文字目が\\r\\nの場合\n",
    "                if clustered_pos_char_indices[i][0] > 0 and clustered_pos_char_indices[\n",
    "                    i\n",
    "                ][0] + 2 < len(pn_history):\n",
    "                    if (\n",
    "                        pn_history[\n",
    "                            clustered_pos_char_indices[i][\n",
    "                                0\n",
    "                            ] : clustered_pos_char_indices[i][0]\n",
    "                            + 2\n",
    "                        ]\n",
    "                        == \"\\r\\n\"\n",
    "                    ):\n",
    "                        clustered_pos_char_indices[i] = clustered_pos_char_indices[i][\n",
    "                            2:\n",
    "                        ]\n",
    "\n",
    "                # 最後の2文字が\\n-の場合\n",
    "                target_idx = clustered_pos_char_indices[i][-1] - 2\n",
    "                if target_idx > 0 and pn_history[target_idx : target_idx + 2] == \"\\n-\":\n",
    "                    clustered_pos_char_indices[i] = clustered_pos_char_indices[i][:-2]\n",
    "\n",
    "        pos_char_spans = []\n",
    "        if len(clustered_pos_char_indices[0]) != 0:\n",
    "            for x in clustered_pos_char_indices:\n",
    "                if len(x) > 0:\n",
    "                    pos_char_spans.append([x[0], x[-1]])\n",
    "\n",
    "        label_strs.append(\";\".join([f\"{x[0]} {x[1]}\" for x in pos_char_spans]))\n",
    "\n",
    "    return label_strs\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def cluster_elements(xs: list) -> list:\n",
    "    clusters = [[]]\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        return clusters\n",
    "\n",
    "    prev_x = xs[0] - 1\n",
    "    for x in xs:\n",
    "        if x == prev_x + 1:\n",
    "            clusters[-1].append(x)\n",
    "        else:\n",
    "            clusters.append([x])\n",
    "        prev_x = x\n",
    "    return clusters\n",
    "\n",
    "\n",
    "import ast\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def get_result(\n",
    "    oof_df: DataFrame, tokenizer: PreTrainedTokenizer, max_len: int\n",
    ") -> tuple:\n",
    "    labels = create_labels_for_scoring(oof_df)\n",
    "    predictions = oof_df[[i for i in range(max_len)]].to_numpy()\n",
    "    char_probs = get_char_probs(oof_df[\"pn_history\"].to_numpy(), predictions, tokenizer)\n",
    "    pn_histories = oof_df[\"pn_history\"].to_list()\n",
    "\n",
    "    score = -100\n",
    "    for th in np.arange(0.3, 0.7, 0.005):\n",
    "        th = np.round(th, 4)\n",
    "        results = get_results(char_probs, pn_histories, th=th)\n",
    "        preds = get_predictions(results)\n",
    "        tmp_score = get_score(labels, preds)\n",
    "        if tmp_score > score:\n",
    "            best_th = th\n",
    "            score = tmp_score\n",
    "    print(f\"Score: {score:<.4f} Best threshold:: {best_th}\")\n",
    "    return score, best_th\n",
    "\n",
    "\n",
    "def create_labels_for_scoring(df: DataFrame):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df[\"location_for_create_labels\"] = [ast.literal_eval(f\"[]\")] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, \"location\"]\n",
    "        if lst:\n",
    "            new_lst = \";\".join(lst)\n",
    "            df.loc[i, \"location_for_create_labels\"] = ast.literal_eval(\n",
    "                f'[[\"{new_lst}\"]]'\n",
    "            )\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df[\"location_for_create_labels\"].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(\";\")]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:02:08.339398Z",
     "iopub.status.idle": "2022-04-21T17:02:08.339892Z",
     "shell.execute_reply": "2022-04-21T17:02:08.339725Z",
     "shell.execute_reply.started": "2022-04-21T17:02:08.339691Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "train = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/train.csv\")\n",
    "train[\"annotation\"] = train[\"annotation\"].map(lambda x: literal_eval(x))\n",
    "train[\"location\"] = train[\"location\"].map(lambda x: literal_eval(x))\n",
    "train[\"annotation_length\"] = train[\"annotation\"].map(lambda x: len(x))\n",
    "train = train.sort_values(by=\"id\").reset_index()\n",
    "mask = train[\"annotation_length\"] == 0\n",
    "\n",
    "oof = pd.read_pickle(\"../input/exp038-nbme-microsoft-deberta-v3-large/oof_df.pkl\")\n",
    "oof = oof.sort_values(by=\"id\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:02:08.341322Z",
     "iopub.status.idle": "2022-04-21T17:02:08.341671Z",
     "shell.execute_reply": "2022-04-21T17:02:08.341491Z",
     "shell.execute_reply.started": "2022-04-21T17:02:08.341466Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"id\",\n",
    "    \"case_num\",\n",
    "    \"pn_num\",\n",
    "    \"feature_num\",\n",
    "    \"annotation\",\n",
    "    \"location\",\n",
    "    \"feature_text\",\n",
    "    \"pn_history\",\n",
    "    \"annotation_length\",\n",
    "    \"fold\",\n",
    "]\n",
    "oof.loc[mask, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T17:02:08.342609Z",
     "iopub.status.idle": "2022-04-21T17:02:08.342927Z",
     "shell.execute_reply": "2022-04-21T17:02:08.342781Z",
     "shell.execute_reply.started": "2022-04-21T17:02:08.34276Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 354\n",
    "\n",
    "pn_histories = oof.loc[mask, \"pn_history\"].to_list()\n",
    "char_probs = get_char_probs(\n",
    "    pn_histories, oof.loc[mask, range(max_len)].to_numpy(), deberta_tokenizer\n",
    ")\n",
    "\n",
    "th = 0.5\n",
    "results = get_results(char_probs, pn_histories, th=th)\n",
    "preds = get_predictions(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_locations = [str([str(y)[1:-2].replace(\",\", \"\") for y in x]) for x in preds]\n",
    "\n",
    "print(len(pl_locations))\n",
    "\n",
    "oof.loc[mask, \"location\"] = pl_locations\n",
    "x = oof.loc[mask]\n",
    "x[x.location != \"[]\"].iloc[:, :11].to_csv(\"pl_train.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
