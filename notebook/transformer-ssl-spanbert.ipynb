{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Pre-training\n",
    "\n",
    "Besides the training data of a target task, we can further pre-train a transformer on the data from the same domain.\n",
    "\n",
    "![image.png](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-32381-3_16/MediaObjects/489562_1_En_16_Fig1_HTML.png)\n",
    "\n",
    "The Transformer models are pre-trained on the general domain corpus. For a text classification task / regression task in a specific domain, such as Readability Assesment, its data\n",
    "distribution may be different from a transformer trained on a different corpus e.g. RoBERTa trained on BookCorpus, Wiki, CC-News, OpenWebText, Stories. Therefore the idea is, we can further pre-train the transformer with masked language model and next sentence prediction tasks on the domain-specific data. Three further pretraining approaches are performed:\n",
    "\n",
    "1) `Within-task pre-training (ITPT)`, in which transformer is further pre-trained on the training data of a target task. `This Kernel.`\n",
    "\n",
    "2) `In-domain pre-training (IDPT)`, in which the pretraining data is obtained from the same domain of a target task. For example, there are several different sentiment classification tasks, which have a similar data distribution. We can further pre-train the transformer on the combined training data from these tasks.\n",
    "\n",
    "3) `Cross-domain pre-training (CDPT)`, in which the pretraining data is obtained from both the same and other different domains to a target task.\n",
    "\n",
    "#### Reference: [How to finetune BERT for Text Classification ?](https://arxiv.org/pdf/1905.05583.pdf)\n",
    "\n",
    "> Note: This Kernel implements ITPT i.e. Within-Task Pretraining. First we will pretrain a RoBERTa model and then utilize the same for further finetuing tasks using different strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Reference: \n",
    "`Transformer Examples` - https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm_no_trainer.py\n",
    "\n",
    "> 90-95% of the code is from this great `run_mlm_no_trainer.py script` from `HuggingFace Examples Repository`. I have merely `changed few lines to adjust the code according to my task`. \n",
    "    \n",
    "    P.S. Make sure to understand everything instead of blindly copying the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 29 14:19:06 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   32C    P0    51W / 350W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:29.298883Z",
     "iopub.status.busy": "2021-05-21T19:09:29.298545Z",
     "iopub.status.idle": "2021-05-21T19:09:40.807034Z",
     "shell.execute_reply": "2021-05-21T19:09:40.806229Z",
     "shell.execute_reply.started": "2021-05-21T19:09:29.298854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.3.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from accelerate) (1.11.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:40.810486Z",
     "iopub.status.busy": "2021-05-21T19:09:40.810133Z",
     "iopub.status.idle": "2021-05-21T19:09:41.276891Z",
     "shell.execute_reply": "2021-05-21T19:09:41.276125Z",
     "shell.execute_reply.started": "2021-05-21T19:09:40.810452Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:41.279867Z",
     "iopub.status.busy": "2021-05-21T19:09:41.279483Z",
     "iopub.status.idle": "2021-05-21T19:09:48.552546Z",
     "shell.execute_reply": "2021-05-21T19:09:48.55181Z",
     "shell.execute_reply.started": "2021-05-21T19:09:41.279831Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING, \n",
    "    MODEL_MAPPING, \n",
    "    AutoConfig, \n",
    "    AutoModelForMaskedLM, \n",
    "    AutoTokenizer, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    get_scheduler, \n",
    "    set_seed\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:48.554259Z",
     "iopub.status.busy": "2021-05-21T19:09:48.553933Z",
     "iopub.status.idle": "2021-05-21T19:09:48.56673Z",
     "shell.execute_reply": "2021-05-21T19:09:48.565628Z",
     "shell.execute_reply.started": "2021-05-21T19:09:48.554228Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    train_file = \"mlm_data.csv\"\n",
    "    validation_file = \"mlm_data.csv\"\n",
    "    pad_to_max_length = True\n",
    "    fold = 0\n",
    "    model_name_or_path = \"SpanBERT/spanbert-large-cased\"\n",
    "    config_name = \"SpanBERT/spanbert-large-cased\"\n",
    "    tokenizer_name = \"SpanBERT/spanbert-large-cased\"\n",
    "    use_slow_tokenizer = False\n",
    "    per_device_train_batch_size = 8\n",
    "    per_device_eval_batch_size = 2\n",
    "    learning_rate = 5e-5\n",
    "    weight_decay = 0.0\n",
    "    num_train_epochs = 100  # change to 5\n",
    "    max_seq_length = 512\n",
    "    max_train_steps = None\n",
    "    gradient_accumulation_steps = 2\n",
    "    lr_scheduler_type = \"constant_with_warmup\"\n",
    "    num_warmup_steps = 0\n",
    "    output_dir = \"../output/spanbert\"\n",
    "    seed = 2021\n",
    "    model_type = \"SpanBERT/spanbert-large-cased\"\n",
    "    mlm_column = \"pn_history\"\n",
    "    line_by_line = False\n",
    "    path_original_dataset = \"../input/corpus.csv\"\n",
    "    preprocessing_num_workers = 4\n",
    "    overwrite_cache = True\n",
    "    mlm_probability = 0.15\n",
    "    additional_tokens = []\n",
    "\n",
    "\n",
    "config = TrainConfig()\n",
    "\n",
    "if config.train_file is not None:\n",
    "    extension = config.train_file.split(\".\")[-1]\n",
    "    assert extension in [\n",
    "        \"csv\",\n",
    "        \"json\",\n",
    "        \"txt\",\n",
    "    ], \"`train_file` should be a csv, json or txt file.\"\n",
    "if config.validation_file is not None:\n",
    "    extension = config.validation_file.split(\".\")[-1]\n",
    "    assert extension in [\n",
    "        \"csv\",\n",
    "        \"json\",\n",
    "        \"txt\",\n",
    "    ], \"`validation_file` should be a csv, json or txt file.\"\n",
    "if config.output_dir is not None:\n",
    "    os.makedirs(config.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:48.569506Z",
     "iopub.status.busy": "2021-05-21T19:09:48.569141Z",
     "iopub.status.idle": "2021-05-21T19:09:48.60221Z",
     "shell.execute_reply": "2021-05-21T19:09:48.601616Z",
     "shell.execute_reply.started": "2021-05-21T19:09:48.569469Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = TrainConfig()\n",
    "    accelerator = Accelerator()\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        filename=\"../output/transformer-ssl-spanbert-large.log\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(accelerator.state)\n",
    "    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "    \n",
    "    df = pd.read_csv(args.path_original_dataset)\n",
    "    \n",
    "    mlm_data = df.loc[df['fold']!=args.fold, [args.mlm_column]]\n",
    "    mlm_data = mlm_data.rename(columns={'excerpt':'text'})\n",
    "    mlm_data.to_csv('mlm_data.csv', index=False)\n",
    "\n",
    "    mlm_data_val = df.loc[df['fold']==args.fold, [args.mlm_column]]\n",
    "    mlm_data_val = mlm_data_val.rename(columns={'excerpt':'text'})\n",
    "    mlm_data_val.to_csv('mlm_data_val.csv', index=False)\n",
    "\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    extension = args.train_file.split(\".\")[-1]\n",
    "    if extension == \"txt\":\n",
    "        extension = \"text\"\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "    \n",
    "    if args.config_name:\n",
    "        config = AutoConfig.from_pretrained(args.config_name)\n",
    "    elif config.model_name_or_path:\n",
    "        config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[args.model_type]()\n",
    "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "    print(\"========= load tokenizer ============\")\n",
    "    if args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "    elif args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "        )\n",
    "    print(\"========= loaded tokenizer ============\")\n",
    "    if len(args.additional_tokens):\n",
    "        tokenizer.add_tokens(args.additional_tokens)\n",
    "    \n",
    "    if args.model_name_or_path:\n",
    "        model = AutoModelForMaskedLM.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "            config=config,\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelForMaskedLM.from_config(config)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "    \n",
    "    print(f\"tokenizer model_max_length: {tokenizer.model_max_length}\")\n",
    "    print(\"========= loaded tokenizer ============\")\n",
    "    if args.max_seq_length is None:\n",
    "        max_seq_length = tokenizer.model_max_length\n",
    "        if max_seq_length > 512:\n",
    "            logger.warning(\n",
    "                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n",
    "            )\n",
    "            max_seq_length = 512\n",
    "    else:\n",
    "        if args.max_seq_length > tokenizer.model_max_length:\n",
    "            logger.warning(\n",
    "                f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n",
    "                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "            )\n",
    "        max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "    )\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        total_length = (total_length // max_seq_length) * max_seq_length\n",
    "        result = {\n",
    "            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "    )\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    else:\n",
    "        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=args.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.num_warmup_steps,\n",
    "        num_training_steps=args.max_train_steps,\n",
    "    )\n",
    "\n",
    "    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    completed_steps = 0\n",
    "\n",
    "    best_perplexity = np.inf\n",
    "    n_update_best_perplexity = 0\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                completed_steps += 1\n",
    "\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        with torch.inference_mode():\n",
    "            for batch in eval_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**batch)\n",
    "\n",
    "                loss = outputs.loss\n",
    "                losses.append(accelerator.gather(loss.repeat(args.per_device_eval_batch_size)))\n",
    "\n",
    "        losses = torch.cat(losses)\n",
    "        losses = losses[: len(eval_dataset)]\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "\n",
    "        logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "\n",
    "        if perplexity < best_perplexity:\n",
    "            n_update_best_perplexity += 1\n",
    "            if n_update_best_perplexity % 5 == 0 or epoch==args.num_train_epochs-1:\n",
    "                best_perplexity = perplexity\n",
    "                accelerator.wait_for_everyone()\n",
    "                unwrapped_model = accelerator.unwrap_model(model)\n",
    "                unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:48.604627Z",
     "iopub.status.busy": "2021-05-21T19:09:48.604178Z",
     "iopub.status.idle": "2021-05-21T19:13:34.776786Z",
     "shell.execute_reply": "2021-05-21T19:13:34.77579Z",
     "shell.execute_reply.started": "2021-05-21T19:09:48.604571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-8d6c8657429f80d9/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fed631b95046caadf5191347587fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab0a17a50164d75a578962758872154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-8d6c8657429f80d9/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634ab9f618134fd9aead594aabbe9c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1a1dfe6956710e7344f6fc7595b16b878615c5f6f2b91e9699f6c8787af0d2fb.6b8ba0ec4a9062565fc1a89d50d0feecbd2295e94a5dd52cde59d9109618a95a\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= load tokenizer ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1a1dfe6956710e7344f6fc7595b16b878615c5f6f2b91e9699f6c8787af0d2fb.6b8ba0ec4a9062565fc1a89d50d0feecbd2295e94a5dd52cde59d9109618a95a\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/1863c6e5ed20880e869aedeb4d0866efd2bec702a76279326207bc20919a2718.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1a1dfe6956710e7344f6fc7595b16b878615c5f6f2b91e9699f6c8787af0d2fb.6b8ba0ec4a9062565fc1a89d50d0feecbd2295e94a5dd52cde59d9109618a95a\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1a1dfe6956710e7344f6fc7595b16b878615c5f6f2b91e9699f6c8787af0d2fb.6b8ba0ec4a9062565fc1a89d50d0feecbd2295e94a5dd52cde59d9109618a95a\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= loaded tokenizer ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/47c606d9dcccc6b316f18beea908fad9eccfec6213d9a8ea31e366fc4233934d.8466fabf7a827d20467e8c2781c1fff0ba40669185f5e0eb34035b8019a36d4f\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer model_max_length: 1000000000000000019884624838656\n",
      "========= loaded tokenizer ============\n",
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9406d5fd4ebc49df9bba4361619b516f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e379b3eb28e423180850bfd3011216e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b62eec270b24ca4b49a65f9cb478725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca8334abe72439abf79c1286fe4cbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632f18390ea94b56a02d24e1cf3bb5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b3bfb24d074ba7abbd3f799ad81914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7cb6f7d1c64863bbf6812d44c59f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d17a455a6a74e2eb1683279f17b5ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3427fae050e6473ea27d03864cc1e6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869afcdbf8844380ab23b460f71487c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea23b1dfaf404f33958c373a7fe52167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14480362ec5448759729ddd702d3a2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c6266591854cdd982b48d1a113b373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39aac776e7ec4aba8f82496f3db11d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8218f4d4b9a437eaae8c5e00bbd0fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2892e7bf5144cd59454a94d3c7bb8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n",
      "Configuration saved in ../output/spanbert/config.json\n",
      "Model weights saved in ../output/spanbert/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=true\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SpanBERT/spanbert-large-cased\", trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('./tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
