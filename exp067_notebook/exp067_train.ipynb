{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7426d7d",
   "metadata": {
    "id": "f7426d7d"
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "797404f7",
   "metadata": {
    "id": "797404f7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import pandas as pd\n",
    "import torch\n",
    "from logging import Logger, getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "import wandb\n",
    "from wandb.sdk.wandb_config import Config\n",
    "\n",
    "# Colaboratory環境ならTrue\n",
    "is_colab='google.colab' in sys.modules\n",
    "# Kaggle Notebook環境ならTrue\n",
    "is_kaggle='kaggle_web_client' in sys.modules\n",
    "\n",
    "def init_pandas() -> None:    \n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "def get_logger(filename:str) -> Logger:\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "def seed_everything(seed:int=42) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def init_wandb(wandb_key:str) -> Config:\n",
    "    #from kaggle_secrets import UserSecretsClient\n",
    "    #user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = wandb_key\n",
    "    wandb.login(key=secret_value_0)\n",
    "\n",
    "    my_ds_path = '.'\n",
    "    loader = yaml.SafeLoader\n",
    "    loader.add_implicit_resolver(\n",
    "        u'tag:yaml.org,2002:float',\n",
    "        re.compile(u'''^(?:\n",
    "         [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)?\n",
    "        |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)\n",
    "        |\\\\.[0-9_]+(?:[eE][-+][0-9]+)?\n",
    "        |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\\\.[0-9_]*\n",
    "        |[-+]?\\\\.(?:inf|Inf|INF)\n",
    "        |\\\\.(?:nan|NaN|NAN))$''', re.X),\n",
    "        list(u'-+0123456789.'))\n",
    "    with open(f'{my_ds_path}/config.yml') as f:\n",
    "        param = yaml.load(f, Loader=loader)\n",
    "    wandb.init(\n",
    "        project=param['project'],\n",
    "        config=param\n",
    "    )\n",
    "    wandb.config.update(param)\n",
    "    print(f'run name: {wandb.run.name}')    \n",
    "    return wandb.config\n",
    "\n",
    "def mk_output_dir(path:str) -> None:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73494049-1b8a-4400-a39d-18f47f6c69de",
   "metadata": {
    "id": "73494049-1b8a-4400-a39d-18f47f6c69de",
    "outputId": "4dbd2717-12b9-4b18-f61f-b4c7292e2d9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmpeg\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/workspace/exp067_notebook/wandb/run-20220423_100020-cf7vyjq2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mpeg/NBME-ScoreClinicalPatientNotes/runs/cf7vyjq2\" target=\"_blank\">celestial-leaf-22</a></strong> to <a href=\"https://wandb.ai/mpeg/NBME-ScoreClinicalPatientNotes\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run name: celestial-leaf-22\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# if is_kaggle:\n",
    "#     from kaggle_secrets import UserSecretsClient\n",
    "#     user_secrets = UserSecretsClient()\n",
    "#     secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "# elif is_colab:\n",
    "#     with open('/content/drive/MyDrive/dotfiles_for_colab/wandb_api.txt') as f:\n",
    "#         secret_value_0=f.readline()\n",
    "# else:\n",
    "#     raise ValueError()\n",
    "from getpass import getpass\n",
    "wandb_key = getpass()\n",
    "config = init_wandb(wandb_key=wandb_key)\n",
    "mk_output_dir(path=config.output_dir)\n",
    "LOGGER = get_logger(\n",
    "    filename=config.output_dir+'train'\n",
    ")\n",
    "seed_everything(seed=config.seed)\n",
    "init_pandas()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ae408",
   "metadata": {
    "id": "575ae408"
   },
   "source": [
    "# Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8771dcad",
   "metadata": {
    "id": "8771dcad"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_score(y_true:ndarray, y_pred:ndarray) -> float:\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def micro_f1(preds:list, truths:list) -> float:\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans:list, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20fdb1b",
   "metadata": {
    "id": "c20fdb1b"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import ast\n",
    "from pandas import DataFrame\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "def create_labels_for_scoring(df:DataFrame):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def get_result(df_oof:DataFrame, tokenizer:PreTrainedTokenizer, max_len:int) -> None:\n",
    "    labels = create_labels_for_scoring(df_oof)\n",
    "    predictions = df_oof[[i for i in range(max_len)]].values\n",
    "    char_probs = get_char_probs(df_oof['pn_history'].values, predictions, tokenizer)\n",
    "    \n",
    "    score=-100\n",
    "    for th in np.arange(0.3,0.7,0.005):\n",
    "        th = np.round(th,4)\n",
    "        results = get_results(char_probs, th=th)\n",
    "        preds = get_predictions(results)\n",
    "        tmp_score = get_score(labels, preds)\n",
    "        if tmp_score > score:\n",
    "            best_th=th\n",
    "            score=tmp_score\n",
    "    LOGGER.info(f'Score: {score:<.4f} Best threshold:: {best_th}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc99c98",
   "metadata": {
    "id": "7dc99c98"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f93fc583",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "f93fc583",
    "outputId": "9669fef5-fd31-4690-f0c1-7dff208e6b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape: (14300, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[chest pressure]</td>\n",
       "      <td>[203 217]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[intermittent episodes, episode]</td>\n",
       "      <td>[70 91, 176 183]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[felt as if he were going to pass out]</td>\n",
       "      <td>[222 258]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num                              annotation          location\n",
       "0  00016_000         0      16            0          [dad with recent heart attcak]         [696 724]\n",
       "1  00016_001         0      16            1             [mom with \"thyroid disease]         [668 693]\n",
       "2  00016_002         0      16            2                        [chest pressure]         [203 217]\n",
       "3  00016_003         0      16            3        [intermittent episodes, episode]  [70 91, 176 183]\n",
       "4  00016_004         0      16            4  [felt as if he were going to pass out]         [222 258]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.shape: (143, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Chest-pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Lightheaded</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_num  case_num                                       feature_text\n",
       "0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n",
       "1            1         0                 Family-history-of-thyroid-disorder\n",
       "2            2         0                                     Chest-pressure\n",
       "3            3         0                              Intermittent-symptoms\n",
       "4            4         0                                        Lightheaded"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_notes.shape: (42146, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pn_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17-year-old male, has come to the student heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17 yo male with recurrent palpitations for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Dillon Cleveland is a 17 y.o. male patient wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a 17 yo m c/o palpitation started 3 mos ago; \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17yo male with no pmh here for evaluation of p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pn_num  case_num                                         pn_history\n",
       "0       0         0  17-year-old male, has come to the student heal...\n",
       "1       1         0  17 yo male with recurrent palpitations for the...\n",
       "2       2         0  Dillon Cleveland is a 17 y.o. male patient wit...\n",
       "3       3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n",
       "4       4         0  17yo male with no pmh here for evaluation of p..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_train['annotation'] = df_train['annotation'].map(lambda x: ast.literal_eval(x))\n",
    "df_train['location'] = df_train['location'].map(lambda x: ast.literal_eval(x))\n",
    "\n",
    "features = pd.read_csv('../input/features.csv')\n",
    "features = preprocess_features(features)\n",
    "features_retranslated = (pd.read_pickle('./df_retranslated.pkl')\n",
    "                         .rename(columns={'ja': 'feature_text_ja',\n",
    "                                          'ko': 'feature_text_ko',\n",
    "                                          'ru': 'feature_text_ru',\n",
    "                                          'ca': 'feature_text_ca'})\n",
    "                         .apply(lambda x:x.str.replace(' ','-')))\n",
    "\n",
    "patient_notes = pd.read_csv('../input/patient_notes.csv')\n",
    "\n",
    "print(f\"df_train.shape: {df_train.shape}\")\n",
    "display(df_train.head())\n",
    "print(f\"features.shape: {features.shape}\")\n",
    "display(features.head())\n",
    "print(f\"patient_notes.shape: {patient_notes.shape}\")\n",
    "display(patient_notes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce0adc5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "ce0adc5c",
    "outputId": "87ae861e-5499-43b4-8922-939bf5917508"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>feature_text_ja</th>\n",
       "      <th>feature_text_ko</th>\n",
       "      <th>feature_text_ru</th>\n",
       "      <th>feature_text_ca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>History-of-families-of-MI-or-family-of-myocard...</td>\n",
       "      <td>MI-in-myocardial-infarction-MI-or-the-history-...</td>\n",
       "      <td>Family-History-of-MI-or-Family-History-Myocard...</td>\n",
       "      <td>Family-History-of-Mi-or-Family-History-of-Myoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>History-of-family-of-thyroid-disorder</td>\n",
       "      <td>Family-force-of-thyroid-disorder</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[chest pressure]</td>\n",
       "      <td>[203 217]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>Chest-pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[intermittent episodes, episode]</td>\n",
       "      <td>[70 91, 176 183]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[felt as if he were going to pass out]</td>\n",
       "      <td>[222 258]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>Light-head</td>\n",
       "      <td>Light</td>\n",
       "      <td>Lighthead.</td>\n",
       "      <td>Light-headed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num                              annotation          location                                       feature_text                                         pn_history                                    feature_text_ja                                    feature_text_ko                                    feature_text_ru                                    feature_text_ca\n",
       "0  00016_000         0      16            0          [dad with recent heart attcak]         [696 724]  Family-history-of-MI-OR-Family-history-of-myoc...  HPI: 17yo M presents with palpitations. Patien...  History-of-families-of-MI-or-family-of-myocard...  MI-in-myocardial-infarction-MI-or-the-history-...  Family-History-of-MI-or-Family-History-Myocard...  Family-History-of-Mi-or-Family-History-of-Myoc...\n",
       "1  00016_001         0      16            1             [mom with \"thyroid disease]         [668 693]                 Family-history-of-thyroid-disorder  HPI: 17yo M presents with palpitations. Patien...              History-of-family-of-thyroid-disorder                   Family-force-of-thyroid-disorder                 Family-history-of-thyroid-disorder                 Family-history-of-thyroid-disorder\n",
       "2  00016_002         0      16            2                        [chest pressure]         [203 217]                                     Chest-pressure  HPI: 17yo M presents with palpitations. Patien...                                     Chest-pressure                                     Chest-pressure                                     Chest-pressure                                     Chest-pressure\n",
       "3  00016_003         0      16            3        [intermittent episodes, episode]  [70 91, 176 183]                              Intermittent-symptoms  HPI: 17yo M presents with palpitations. Patien...                              Intermittent-symptoms                              Intermittent-symptoms                              Intermittent-symptoms                              Intermittent-symptoms\n",
       "4  00016_004         0      16            4  [felt as if he were going to pass out]         [222 258]                                        Lightheaded  HPI: 17yo M presents with palpitations. Patien...                                         Light-head                                              Light                                         Lighthead.                                       Light-headed"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = df_train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "df_train = df_train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "df_train = df_train.merge(features_retranslated, on=['feature_text'], how='left')\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66337109",
   "metadata": {
    "id": "66337109"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "def correct_annotation(df_train:DataFrame) -> None:\n",
    "    df_train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "    df_train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "    df_train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "    df_train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "    df_train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "    df_train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "    df_train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "    df_train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "    df_train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "    df_train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "    df_train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "    df_train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "    df_train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "    df_train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "    df_train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "    df_train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "    df_train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "    df_train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "    df_train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "    df_train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "    df_train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "    df_train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "    df_train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "    df_train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "    df_train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "    df_train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "    df_train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "    df_train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "    df_train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "    df_train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "    df_train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "    df_train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "    df_train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "    df_train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "    df_train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "    df_train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "    df_train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "    df_train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "    df_train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "    df_train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "    df_train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "    df_train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "    df_train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "    df_train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "    df_train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "    df_train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "    df_train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "    df_train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "    df_train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "    df_train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "    df_train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "    df_train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "    df_train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "    df_train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "    df_train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "    df_train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "    df_train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "    df_train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "    df_train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "    df_train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "    df_train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "    df_train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "    df_train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "    df_train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "    df_train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "    df_train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "    df_train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "    df_train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "    df_train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "    df_train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "    df_train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "    df_train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "    df_train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "    df_train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "    df_train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "    df_train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "    df_train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "    df_train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "    df_train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "    df_train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "    df_train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "    df_train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "    df_train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "    df_train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec3a7b5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "ec3a7b5f",
    "outputId": "aa8f87d6-117a-467e-ee00-3fb6090a37f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8181\n",
       "0    4399\n",
       "2    1296\n",
       "3     287\n",
       "4      99\n",
       "5      27\n",
       "6       9\n",
       "7       1\n",
       "8       1\n",
       "Name: annotation_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['annotation_length'] = df_train['annotation'].map(lambda x: len(x))\n",
    "display(df_train['annotation_length'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6437f",
   "metadata": {
    "id": "e6c6437f"
   },
   "source": [
    "# CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed511f87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "ed511f87",
    "outputId": "1d67adbd-5f7b-4036-e6d1-3e0957e5af1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    2860\n",
       "1    2860\n",
       "2    2860\n",
       "3    2860\n",
       "4    2860\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "kf = GroupKFold(n_splits=config.n_folds)\n",
    "groups = df_train['pn_num'].to_numpy()\n",
    "df_train.loc[:, 'fold'] = -1\n",
    "for n, (train_index, val_index) in enumerate(kf.split(df_train, df_train['location'], groups)):\n",
    "    df_train.loc[val_index, 'fold'] = n\n",
    "display(df_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "526e4ac9",
   "metadata": {
    "id": "526e4ac9"
   },
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    display(df_train.groupby('fold').size())\n",
    "    df_train = df_train.sample(n=500, random_state=0).reset_index(drop=True)\n",
    "    display(df_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2590f",
   "metadata": {
    "id": "11e2590f"
   },
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "BR6m_itarTmJ",
   "metadata": {
    "id": "BR6m_itarTmJ"
   },
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "\n",
    "transformers_path = Path(transformers.__file__[:-12])\n",
    "\n",
    "input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py', \"deberta__init__.py\"]:\n",
    "    if str(filename).startswith(\"deberta\"):\n",
    "        filepath = deberta_v2_path/str(filename).replace(\"deberta\", \"\")\n",
    "    else:\n",
    "        filepath = deberta_v2_path/filename\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "819ca57b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "819ca57b",
    "outputId": "367ad7d9-56c1-4479-aa01-9aa3cf22f8d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../output/exp067tokenizer/tokenizer_config.json',\n",
       " '../output/exp067tokenizer/special_tokens_map.json',\n",
       " '../output/exp067tokenizer/spm.model',\n",
       " '../output/exp067tokenizer/added_tokens.json',\n",
       " '../output/exp067tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.deberta_v2 import DebertaV2TokenizerFast\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained(config.model)\n",
    "tokenizer.save_pretrained(config.output_dir+'tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d057ad3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134,
     "referenced_widgets": [
      "ef054fe237094a8f89af36fe34f29546",
      "6c4b64ed971348e481146662547cd2f5",
      "423d87770df94d2f95386d76b39a12f1",
      "c4f6b02ff7c64618bec62ef579ebb0e2",
      "af5020d41f5043b59dbf26ea604860ab",
      "2675c5a9d05d4e528158c8a2ea0ac2f3",
      "1d0a80bd397e4343b229dfc0e8710fbd",
      "2489d9146ae14cdb992e20243dcefe41",
      "575391684d0446cab7ceb1232a451135",
      "e0b164ce58e742c0a3e68fc25f1cdbdb",
      "c96c1a3e94b143e69533878a57b3b4c1",
      "39e5db951bac4b928ad4780a50911407",
      "33dbeac5c3c3479cbf11bfb50d9dfb25",
      "c9b9d9599286411e947afb2025ced89b",
      "b86dad7854a34e03a0d73ef02b91bc34",
      "c14f830ddf7745d1bf121a0fe015931f",
      "a1ef94a77ac64563b17828197f72b3ce",
      "997c32f0f331437ebc1f9c348eae908b",
      "cdcba5824ebd49babda6a2046b3d04b0",
      "02ca5a1bd05f4677ab00d2317fab3e98",
      "ef59bad7b39f431ca0fd83f8f56b59de",
      "37c74228a2e64f629e6a37f183babcc4"
     ]
    },
    "id": "7d057ad3",
    "outputId": "38483c7e-20e6-4c99-cf89-5a9529718598"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pn_history max(lengths): 323\n",
      "feature_text max(lengths): 32\n",
      "max_len: 358\n"
     ]
    }
   ],
   "source": [
    "pn_history_lengths = []\n",
    "for text in patient_notes['pn_history'].fillna(\"\").to_list():\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    pn_history_lengths.append(length)\n",
    "pn_history_max_len = max(pn_history_lengths)\n",
    "LOGGER.info(f'pn_history max(lengths): {pn_history_max_len}')\n",
    "\n",
    "features_lengths = []\n",
    "for _, col in df_train.filter(like='feature_text').iteritems():\n",
    "    for text in col.fillna(\"\").to_list():\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "        features_lengths.append(length)\n",
    "feature_text_max_len = max(features_lengths)\n",
    "LOGGER.info(f'feature_text max(lengths): {feature_text_max_len}')\n",
    "\n",
    "config.max_len = pn_history_max_len+feature_text_max_len + 3\n",
    "LOGGER.info(f\"max_len: {config.max_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9f616",
   "metadata": {
    "id": "06f9f616"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1387f638",
   "metadata": {
    "id": "1387f638"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer:PreTrainedTokenizer, \n",
    "        max_len:int,\n",
    "        feature_text_max_len:int, \n",
    "        pn_history_max_len:int, \n",
    "        df:DataFrame,\n",
    "        feature_retranslate:bool=True\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.feature_text_max_len = feature_text_max_len\n",
    "        self.pn_history_max_len = pn_history_max_len\n",
    "        self.feature_texts = df.filter(like='feature_text').to_numpy()\n",
    "        self.pn_historys = df['pn_history'].to_numpy()\n",
    "        self.annotation_lengths = df['annotation_length'].to_numpy()\n",
    "        self.locations = df['location'].to_numpy()\n",
    "        self.feature_retranslate = feature_retranslate\n",
    "\n",
    "    def prepare_input_with_fixed_position(self, pn_history:str, feature_text:str) -> dict:\n",
    "\n",
    "        pn_history_token = self.tokenizer(\n",
    "            pn_history, \n",
    "            add_special_tokens=True,\n",
    "            max_length=self.pn_history_max_len+2, \n",
    "            padding='max_length',\n",
    "            return_offsets_mapping=False)\n",
    "        \n",
    "        feature_text_token = self.tokenizer(\n",
    "            feature_text, \n",
    "            add_special_tokens=True,\n",
    "            max_length=self.feature_text_max_len+2, \n",
    "            padding='max_length',\n",
    "            return_offsets_mapping=False)\n",
    "        for k,v in feature_text_token.items():\n",
    "            feature_text_token[k] = v[1:]\n",
    "\n",
    "        token = {\n",
    "            'input_ids': pn_history_token['input_ids']+feature_text_token['input_ids'],\n",
    "            'attention_mask': pn_history_token['attention_mask']+feature_text_token['attention_mask'],\n",
    "            'token_type_ids': pn_history_token['token_type_ids']+feature_text_token['token_type_ids']\n",
    "        }\n",
    "        for k, v in token.items():\n",
    "            token[k] = torch.tensor(v[:self.max_len], dtype=torch.long)\n",
    "        return token\n",
    "    \n",
    "    def prepare_input(self, text:str, feature_text:str) -> dict:\n",
    "        token = self.tokenizer(text, feature_text, \n",
    "                               add_special_tokens=True,\n",
    "                               max_length=self.max_len,\n",
    "                               padding=\"max_length\",\n",
    "                               return_offsets_mapping=False)\n",
    "        for k, v in token.items():\n",
    "            token[k] = torch.tensor(v[:self.max_len], dtype=torch.long)\n",
    "        return token\n",
    "    \n",
    "    def create_label(self, text:str, annotation_length:int, location_list:list) -> Tensor:\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True)\n",
    "        offset_mapping = encoded['offset_mapping']\n",
    "        ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "        label = np.zeros(len(offset_mapping))\n",
    "        label[ignore_idxes] = -1\n",
    "        if annotation_length != 0:\n",
    "            for location in location_list:\n",
    "                for loc in [s.split() for s in location.split(';')]:\n",
    "                    start_idx = -1\n",
    "                    end_idx = -1\n",
    "                    start, end = int(loc[0]), int(loc[1])\n",
    "                    for idx in range(len(offset_mapping)):\n",
    "                        if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                            start_idx = idx - 1\n",
    "                        if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                            end_idx = idx + 1\n",
    "                    if start_idx == -1:\n",
    "                        start_idx = end_idx\n",
    "                    if (start_idx != -1) & (end_idx != -1):\n",
    "                        label[start_idx:end_idx] = 1\n",
    "        return torch.tensor(label[:self.max_len], dtype=torch.float)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item:int) -> tuple:\n",
    "        if self.feature_retranslate:\n",
    "            feature_text=np.random.choice(self.feature_texts[item],p=config.choice_weight)\n",
    "        else:\n",
    "            feature_text=self.feature_texts[item][0]\n",
    "        inputs = self.prepare_input_with_fixed_position(\n",
    "            self.pn_historys[item],\n",
    "            feature_text)\n",
    "        label = self.create_label(\n",
    "            self.pn_historys[item], \n",
    "            self.annotation_lengths[item], \n",
    "            self.locations[item])\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd76e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TrainDataset(\n",
    "    tokenizer=tokenizer, \n",
    "    max_len=config.max_len,\n",
    "    feature_text_max_len=feature_text_max_len, \n",
    "    pn_history_max_len=pn_history_max_len, \n",
    "    df=df_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c4b6c",
   "metadata": {
    "id": "006c4b6c"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aab0ac97",
   "metadata": {
    "id": "aab0ac97"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "class CustomModel(Module):\n",
    "    def __init__(self, model_name:str, config_path:str=None, pretrained:bool=False) -> None:\n",
    "        super().__init__()\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(\n",
    "                model_name, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                config.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs:Tensor) -> Tensor:\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs:Tensor) -> Tensor:\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250ef911",
   "metadata": {
    "id": "250ef911"
   },
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "716ccfc0",
   "metadata": {
    "id": "716ccfc0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from math import floor\n",
    "from torch import inference_mode\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val:float, n=1) -> None:\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s) -> str:\n",
    "    m = floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent) -> str:\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3139cba-ebd0-4faf-bb86-de5f7f032293",
   "metadata": {
    "id": "a3139cba-ebd0-4faf-bb86-de5f7f032293"
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef0f8033",
   "metadata": {
    "id": "ef0f8033"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.optim import AdamW\n",
    "from torch import cuda\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "from wandb.sdk.wandb_config import Config\n",
    "\n",
    "def get_optimizer_params(model:Module, encoder_lr:float, decoder_lr:float, weight_decay:float=0.0) -> list:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "        'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "def get_scheduler(scheduler:str, optimizer, num_warmup_steps:int, num_train_steps:int, num_cycles:int):\n",
    "    if scheduler=='linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=num_warmup_steps, \n",
    "            num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif scheduler=='cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=num_warmup_steps, \n",
    "            num_training_steps=num_train_steps, \n",
    "            num_cycles=num_cycles\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('Invalid Scheduler Name.')\n",
    "    return scheduler\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, config:Config, tokenizer:PreTrainedTokenizer) -> None:\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.device = torch.device('cuda' if cuda.is_available() else 'cpu')\n",
    "\n",
    "    def train(self, model:Module, fold:int, tr_dl:DataLoader, optimizer, epoch:int, scheduler):\n",
    "        model.train()\n",
    "        scaler = cuda.amp.GradScaler(enabled=self.config.apex)\n",
    "        losses = AverageMeter()\n",
    "        start = end = time.time()\n",
    "        global_step = 0\n",
    "        for step, (inputs, labels) in enumerate(tr_dl):\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            batch_size = labels.size(0)\n",
    "            with cuda.amp.autocast(enabled=self.config.apex):\n",
    "                y_preds = model(inputs)\n",
    "            loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "            loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "            if self.config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / self.config.gradient_accumulation_steps\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            scaler.scale(loss).backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), self.config.max_grad_norm)\n",
    "            if (step + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                if self.config.batch_scheduler:\n",
    "                    scheduler.step()\n",
    "            end = time.time()\n",
    "            if step % self.config.print_freq == 0 or step == (len(tr_dl)-1):\n",
    "                print('Epoch: [{0}][{1}/{2}] '\n",
    "                    'Elapsed {remain:s} '\n",
    "                    'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                    'Grad: {grad_norm:.4f}  '\n",
    "                    'LR: {lr:.8f}  '\n",
    "                    .format(epoch+1, step, len(tr_dl), \n",
    "                            remain=timeSince(start, float(step+1)/len(tr_dl)),\n",
    "                            loss=losses,\n",
    "                            grad_norm=grad_norm,\n",
    "                            lr=scheduler.get_lr()[0]))\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                    f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "        return losses.avg\n",
    "\n",
    "    @inference_mode()\n",
    "    def validate(self, model:Module, vl_dl:DataLoader) -> tuple:\n",
    "        model.eval()\n",
    "        losses = AverageMeter()\n",
    "        preds = []\n",
    "        start = end = time.time()\n",
    "        for step, (inputs, labels) in enumerate(vl_dl):\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            batch_size = labels.size(0)\n",
    "            y_preds = model(inputs)\n",
    "            loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "            loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "            if self.config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / self.config.gradient_accumulation_steps\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "            end = time.time()\n",
    "            if step % self.config.print_freq == 0 or step == (len(vl_dl)-1):\n",
    "                print('EVAL: [{0}/{1}] '\n",
    "                    'Elapsed {remain:s} '\n",
    "                    'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                    .format(step, len(vl_dl),\n",
    "                            loss=losses,\n",
    "                            remain=timeSince(start, float(step+1)/len(vl_dl))))\n",
    "        return losses.avg, np.concatenate(preds)\n",
    "\n",
    "    def create_dl(self, df:DataFrame, feature_text_max_len:int, pn_history_max_len:int, is_train:bool) -> DataLoader:\n",
    "        ds = TrainDataset(\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_len=self.config.max_len,\n",
    "            feature_text_max_len=feature_text_max_len,\n",
    "            pn_history_max_len=pn_history_max_len,\n",
    "            df=df,\n",
    "            feature_retranslate=is_train)\n",
    "        return DataLoader(\n",
    "            ds,\n",
    "            batch_size=self.config.batch_size if is_train else self.config.batch_size * 2,\n",
    "            shuffle=is_train,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True, \n",
    "            drop_last=is_train)\n",
    "    \n",
    "    def log_epoch_result(self, f:int, ep:int, avg_tr_loss:float, avg_vl_loss:float, elapsed:float, score:float, best_th:float) -> None:\n",
    "        LOGGER.info(\n",
    "            f'Epoch {ep} - avg_train_loss: {avg_tr_loss:.4f}  avg_val_loss: {avg_vl_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(\n",
    "            f'Epoch {ep} - Score: {score:.4f} for th={best_th}')\n",
    "        wandb.log(\n",
    "            {\n",
    "                f\"[fold{f}] epoch\": ep, \n",
    "                f\"[fold{f}] avg_train_loss\": avg_tr_loss, \n",
    "                f\"[fold{f}] avg_val_loss\": avg_vl_loss,\n",
    "                f\"[fold{f}] score\": score,\n",
    "                f\"[fold{f}] best_th\": best_th\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        df:DataFrame,\n",
    "        feature_text_max_len:int, \n",
    "        pn_history_max_len:int) -> None:\n",
    "    \n",
    "        oof_df = pd.DataFrame()\n",
    "        for f in range(self.config.n_folds):\n",
    "            LOGGER.info(f\"========== fold: {f} training ==========\")\n",
    "            \n",
    "            model = CustomModel(\n",
    "                self.config.model, \n",
    "                config_path=None, \n",
    "                pretrained=True).to(self.device)\n",
    "\n",
    "            tr_df = df[df['fold'] != f].reset_index(drop=True)\n",
    "            tr_dl = self.create_dl(\n",
    "                df=tr_df, \n",
    "                feature_text_max_len=feature_text_max_len, \n",
    "                pn_history_max_len=pn_history_max_len, \n",
    "                is_train=True)\n",
    "            num_train_steps = int(len(tr_df) / self.config.batch_size * self.config.epochs)\n",
    "            \n",
    "            vl_df = df[df['fold'] == f].reset_index(drop=True)\n",
    "            vl_dl = self.create_dl(\n",
    "                df=vl_df, \n",
    "                feature_text_max_len=feature_text_max_len, \n",
    "                pn_history_max_len=pn_history_max_len, \n",
    "                is_train=False)\n",
    "            valid_texts = vl_df['pn_history'].to_numpy()\n",
    "            valid_labels = create_labels_for_scoring(vl_df)\n",
    "\n",
    "            optimizer_parameters = get_optimizer_params(\n",
    "                model,\n",
    "                encoder_lr=self.config.encoder_lr, \n",
    "                decoder_lr=self.config.decoder_lr,\n",
    "                weight_decay=self.config.weight_decay)\n",
    "            optimizer = AdamW(\n",
    "                optimizer_parameters, \n",
    "                lr=self.config.encoder_lr, \n",
    "                eps=self.config.eps, \n",
    "                betas=self.config.betas)\n",
    "            scheduler = get_scheduler(\n",
    "                scheduler=self.config.scheduler, \n",
    "                optimizer=optimizer, \n",
    "                num_warmup_steps=self.config.num_warmup_steps,\n",
    "                num_train_steps=num_train_steps,\n",
    "                num_cycles=self.config.num_cycles)\n",
    "            \n",
    "            best_score = -100.0\n",
    "\n",
    "            for epoch in range(self.config.epochs):\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                # train\n",
    "                avg_tr_loss = self.train(\n",
    "                    model,\n",
    "                    f, \n",
    "                    tr_dl, \n",
    "                    optimizer, \n",
    "                    epoch, \n",
    "                    scheduler)\n",
    "\n",
    "                # eval\n",
    "                avg_vl_loss, predictions = self.validate(\n",
    "                    model, \n",
    "                    vl_dl\n",
    "                )\n",
    "                predictions = predictions.reshape(\n",
    "                    (len(vl_df), \n",
    "                    self.config.max_len))\n",
    "                \n",
    "                # scoring\n",
    "                char_probs = get_char_probs(\n",
    "                    valid_texts, \n",
    "                    predictions, \n",
    "                    self.tokenizer)\n",
    "                # ここをしきい値で探索した最適な値にする\n",
    "                score=-100\n",
    "                for th in np.arange(0.3,0.7,0.005):\n",
    "                    th = np.round(th,4)\n",
    "                    results = get_results(char_probs, th=th)\n",
    "                    preds = get_predictions(results)\n",
    "                    tmp_score = get_score(valid_labels, preds)\n",
    "                    if tmp_score > score:\n",
    "                        best_th=th\n",
    "                        score=tmp_score\n",
    "                \n",
    "                self.log_epoch_result(\n",
    "                    f=f,\n",
    "                    ep=epoch+1, \n",
    "                    avg_tr_loss=avg_tr_loss, \n",
    "                    avg_vl_loss=avg_vl_loss, \n",
    "                    elapsed=time.time() - start_time, \n",
    "                    score=score, \n",
    "                    best_th=best_th)\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    LOGGER.info(f'Epoch {epoch+1} - Save Score: {best_score:.4f} Model')\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            'model': model.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                        f'{self.config.output_dir}{self.config.ckpt_name}_fold{f}_best.pth')\n",
    "\n",
    "            predictions = torch.load(\n",
    "                f'{self.config.output_dir}{self.config.ckpt_name}_fold{f}_best.pth', \n",
    "                map_location=torch.device('cpu'))['predictions']\n",
    "            vl_df[[i for i in range(self.config.max_len)]] = predictions\n",
    "            oof_df = pd.concat([oof_df, vl_df])\n",
    "            LOGGER.info(f\"========== fold: {f} result ==========\")\n",
    "            get_result(vl_df, self.tokenizer, self.config.max_len)\n",
    "            oof_df.to_pickle(f'{self.config.output_dir}oof_df_fold{f}.pkl')\n",
    "            wandb.alert(\n",
    "                title=f\"fold{f} Finished\", \n",
    "                text=f'{self.config.model} has finished its fold{f} running.'\n",
    "            )\n",
    "        \n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df, self.tokenizer, self.config.max_len)\n",
    "        oof_df.to_pickle(self.config.output_dir+'oof_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db38bc60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "db38bc60",
    "outputId": "52ff3339-036d-4fb9-b39c-950ae100d2f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/1906] Elapsed 0m 1s (remain 58m 40s) Loss: 0.6006(0.6006) Grad: inf  LR: 0.00001500  \n",
      "Epoch: [1][100/1906] Elapsed 0m 27s (remain 8m 14s) Loss: 0.0445(0.0880) Grad: 1529.0291  LR: 0.00001500  \n",
      "Epoch: [1][200/1906] Elapsed 0m 53s (remain 7m 30s) Loss: 0.0224(0.0613) Grad: 1517.4312  LR: 0.00001499  \n",
      "Epoch: [1][300/1906] Elapsed 1m 18s (remain 6m 55s) Loss: 0.0141(0.0496) Grad: 1541.2185  LR: 0.00001497  \n",
      "Epoch: [1][400/1906] Elapsed 1m 42s (remain 6m 26s) Loss: 0.0156(0.0425) Grad: 864.7016  LR: 0.00001495  \n",
      "Epoch: [1][500/1906] Elapsed 2m 8s (remain 5m 59s) Loss: 0.0051(0.0381) Grad: 758.2766  LR: 0.00001493  \n",
      "Epoch: [1][600/1906] Elapsed 2m 33s (remain 5m 32s) Loss: 0.0075(0.0350) Grad: 1255.6080  LR: 0.00001490  \n",
      "Epoch: [1][700/1906] Elapsed 2m 58s (remain 5m 6s) Loss: 0.0126(0.0328) Grad: 2487.5571  LR: 0.00001486  \n",
      "Epoch: [1][800/1906] Elapsed 3m 23s (remain 4m 41s) Loss: 0.0011(0.0309) Grad: 475.2433  LR: 0.00001482  \n",
      "Epoch: [1][900/1906] Elapsed 3m 49s (remain 4m 15s) Loss: 0.0061(0.0296) Grad: 375.1071  LR: 0.00001477  \n",
      "Epoch: [1][1000/1906] Elapsed 4m 13s (remain 3m 49s) Loss: 0.0171(0.0284) Grad: 2136.5767  LR: 0.00001472  \n",
      "Epoch: [1][1100/1906] Elapsed 4m 38s (remain 3m 23s) Loss: 0.0166(0.0275) Grad: 856.6058  LR: 0.00001466  \n",
      "Epoch: [1][1200/1906] Elapsed 5m 3s (remain 2m 58s) Loss: 0.0408(0.0266) Grad: 3632.4827  LR: 0.00001460  \n",
      "Epoch: [1][1300/1906] Elapsed 5m 28s (remain 2m 32s) Loss: 0.0600(0.0259) Grad: 4711.0317  LR: 0.00001453  \n",
      "Epoch: [1][1400/1906] Elapsed 5m 53s (remain 2m 7s) Loss: 0.0051(0.0251) Grad: 810.8511  LR: 0.00001445  \n",
      "Epoch: [1][1500/1906] Elapsed 6m 18s (remain 1m 42s) Loss: 0.0044(0.0244) Grad: 782.0533  LR: 0.00001437  \n",
      "Epoch: [1][1600/1906] Elapsed 6m 43s (remain 1m 16s) Loss: 0.0413(0.0238) Grad: 1958.9646  LR: 0.00001429  \n",
      "Epoch: [1][1700/1906] Elapsed 7m 8s (remain 0m 51s) Loss: 0.0390(0.0232) Grad: 5381.4468  LR: 0.00001420  \n",
      "Epoch: [1][1800/1906] Elapsed 7m 33s (remain 0m 26s) Loss: 0.0187(0.0229) Grad: 3479.5088  LR: 0.00001410  \n",
      "Epoch: [1][1900/1906] Elapsed 7m 57s (remain 0m 1s) Loss: 0.0101(0.0225) Grad: 1973.9128  LR: 0.00001400  \n",
      "Epoch: [1][1905/1906] Elapsed 7m 59s (remain 0m 0s) Loss: 0.0374(0.0225) Grad: 4425.0303  LR: 0.00001400  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 0s) Loss: 0.0131(0.0131) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0030(0.0141) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0042(0.0155) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0024(0.0143) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0225  avg_val_loss: 0.0143  time: 538s\n",
      "Epoch 1 - Score: 0.8615 for th=0.3\n",
      "Epoch 1 - Save Score: 0.8615 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/1906] Elapsed 0m 0s (remain 14m 6s) Loss: 0.0037(0.0037) Grad: 37574.8633  LR: 0.00001399  \n",
      "Epoch: [2][100/1906] Elapsed 0m 25s (remain 7m 40s) Loss: 0.0054(0.0139) Grad: 55776.1016  LR: 0.00001389  \n",
      "Epoch: [2][200/1906] Elapsed 0m 50s (remain 7m 10s) Loss: 0.0124(0.0131) Grad: 21388.6934  LR: 0.00001378  \n",
      "Epoch: [2][300/1906] Elapsed 1m 15s (remain 6m 43s) Loss: 0.0241(0.0130) Grad: 31872.4102  LR: 0.00001366  \n",
      "Epoch: [2][400/1906] Elapsed 1m 40s (remain 6m 17s) Loss: 0.0111(0.0124) Grad: 15971.2617  LR: 0.00001354  \n",
      "Epoch: [2][500/1906] Elapsed 2m 5s (remain 5m 51s) Loss: 0.0028(0.0120) Grad: 5655.6514  LR: 0.00001342  \n",
      "Epoch: [2][600/1906] Elapsed 2m 30s (remain 5m 26s) Loss: 0.0009(0.0122) Grad: 3356.9697  LR: 0.00001329  \n",
      "Epoch: [2][700/1906] Elapsed 2m 55s (remain 5m 2s) Loss: 0.0062(0.0121) Grad: 11371.4531  LR: 0.00001316  \n",
      "Epoch: [2][800/1906] Elapsed 3m 20s (remain 4m 36s) Loss: 0.0029(0.0118) Grad: 13725.5498  LR: 0.00001302  \n",
      "Epoch: [2][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0172(0.0116) Grad: 58102.6953  LR: 0.00001288  \n",
      "Epoch: [2][1000/1906] Elapsed 4m 10s (remain 3m 46s) Loss: 0.0247(0.0114) Grad: 11861.8584  LR: 0.00001273  \n",
      "Epoch: [2][1100/1906] Elapsed 4m 36s (remain 3m 21s) Loss: 0.0017(0.0112) Grad: 7137.7490  LR: 0.00001258  \n",
      "Epoch: [2][1200/1906] Elapsed 5m 1s (remain 2m 56s) Loss: 0.0073(0.0112) Grad: 9904.3643  LR: 0.00001243  \n",
      "Epoch: [2][1300/1906] Elapsed 5m 26s (remain 2m 31s) Loss: 0.0023(0.0112) Grad: 11541.3174  LR: 0.00001227  \n",
      "Epoch: [2][1400/1906] Elapsed 5m 51s (remain 2m 6s) Loss: 0.0024(0.0112) Grad: 6642.0732  LR: 0.00001211  \n",
      "Epoch: [2][1500/1906] Elapsed 6m 16s (remain 1m 41s) Loss: 0.0031(0.0109) Grad: 7767.3218  LR: 0.00001195  \n",
      "Epoch: [2][1600/1906] Elapsed 6m 41s (remain 1m 16s) Loss: 0.0010(0.0109) Grad: 4965.8252  LR: 0.00001178  \n",
      "Epoch: [2][1700/1906] Elapsed 7m 5s (remain 0m 51s) Loss: 0.0073(0.0108) Grad: 30751.7246  LR: 0.00001161  \n",
      "Epoch: [2][1800/1906] Elapsed 7m 30s (remain 0m 26s) Loss: 0.0017(0.0109) Grad: 24130.6660  LR: 0.00001144  \n",
      "Epoch: [2][1900/1906] Elapsed 7m 55s (remain 0m 1s) Loss: 0.0021(0.0107) Grad: 4719.7983  LR: 0.00001126  \n",
      "Epoch: [2][1905/1906] Elapsed 7m 56s (remain 0m 0s) Loss: 0.0015(0.0107) Grad: 5659.8228  LR: 0.00001125  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 0m 55s) Loss: 0.0105(0.0105) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0031(0.0124) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0122(0.0137) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0012(0.0126) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0107  avg_val_loss: 0.0126  time: 536s\n",
      "Epoch 2 - Score: 0.8780 for th=0.47\n",
      "Epoch 2 - Save Score: 0.8780 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/1906] Elapsed 0m 0s (remain 14m 7s) Loss: 0.0058(0.0058) Grad: 13797.7178  LR: 0.00001125  \n",
      "Epoch: [3][100/1906] Elapsed 0m 25s (remain 7m 38s) Loss: 0.0106(0.0078) Grad: 9635.3369  LR: 0.00001107  \n",
      "Epoch: [3][200/1906] Elapsed 0m 50s (remain 7m 10s) Loss: 0.0066(0.0075) Grad: 8183.1704  LR: 0.00001089  \n",
      "Epoch: [3][300/1906] Elapsed 1m 15s (remain 6m 43s) Loss: 0.0039(0.0080) Grad: 10828.4248  LR: 0.00001070  \n",
      "Epoch: [3][400/1906] Elapsed 1m 40s (remain 6m 18s) Loss: 0.0198(0.0083) Grad: 21310.9043  LR: 0.00001052  \n",
      "Epoch: [3][500/1906] Elapsed 2m 5s (remain 5m 53s) Loss: 0.0033(0.0087) Grad: 11003.6270  LR: 0.00001033  \n",
      "Epoch: [3][600/1906] Elapsed 2m 31s (remain 5m 28s) Loss: 0.0044(0.0089) Grad: 12139.2471  LR: 0.00001013  \n",
      "Epoch: [3][700/1906] Elapsed 2m 55s (remain 5m 2s) Loss: 0.0031(0.0090) Grad: 12724.6064  LR: 0.00000994  \n",
      "Epoch: [3][800/1906] Elapsed 3m 20s (remain 4m 37s) Loss: 0.0055(0.0089) Grad: 47760.0703  LR: 0.00000975  \n",
      "Epoch: [3][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0171(0.0091) Grad: 48872.4141  LR: 0.00000955  \n",
      "Epoch: [3][1000/1906] Elapsed 4m 10s (remain 3m 46s) Loss: 0.0195(0.0092) Grad: 43546.4180  LR: 0.00000935  \n",
      "Epoch: [3][1100/1906] Elapsed 4m 35s (remain 3m 21s) Loss: 0.0220(0.0095) Grad: 34531.6133  LR: 0.00000915  \n",
      "Epoch: [3][1200/1906] Elapsed 5m 0s (remain 2m 56s) Loss: 0.0040(0.0094) Grad: 10189.5459  LR: 0.00000895  \n",
      "Epoch: [3][1300/1906] Elapsed 5m 25s (remain 2m 31s) Loss: 0.0011(0.0092) Grad: 9429.2646  LR: 0.00000874  \n",
      "Epoch: [3][1400/1906] Elapsed 5m 50s (remain 2m 6s) Loss: 0.0010(0.0092) Grad: 5498.2227  LR: 0.00000854  \n",
      "Epoch: [3][1500/1906] Elapsed 6m 15s (remain 1m 41s) Loss: 0.0031(0.0093) Grad: 39890.9961  LR: 0.00000834  \n",
      "Epoch: [3][1600/1906] Elapsed 6m 40s (remain 1m 16s) Loss: 0.0545(0.0093) Grad: 113703.4375  LR: 0.00000813  \n",
      "Epoch: [3][1700/1906] Elapsed 7m 5s (remain 0m 51s) Loss: 0.0026(0.0093) Grad: 12255.7725  LR: 0.00000793  \n",
      "Epoch: [3][1800/1906] Elapsed 7m 30s (remain 0m 26s) Loss: 0.0027(0.0092) Grad: 6170.9214  LR: 0.00000772  \n",
      "Epoch: [3][1900/1906] Elapsed 7m 55s (remain 0m 1s) Loss: 0.0003(0.0093) Grad: 2424.0125  LR: 0.00000751  \n",
      "Epoch: [3][1905/1906] Elapsed 7m 56s (remain 0m 0s) Loss: 0.0084(0.0093) Grad: 10276.3477  LR: 0.00000750  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 0m 57s) Loss: 0.0134(0.0134) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0024(0.0123) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0107(0.0133) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0031(0.0122) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0093  avg_val_loss: 0.0122  time: 535s\n",
      "Epoch 3 - Score: 0.8813 for th=0.355\n",
      "Epoch 3 - Save Score: 0.8813 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/1906] Elapsed 0m 0s (remain 14m 23s) Loss: 0.0053(0.0053) Grad: 10264.8193  LR: 0.00000750  \n",
      "Epoch: [4][100/1906] Elapsed 0m 25s (remain 7m 38s) Loss: 0.0002(0.0078) Grad: 1251.3422  LR: 0.00000730  \n",
      "Epoch: [4][200/1906] Elapsed 0m 51s (remain 7m 13s) Loss: 0.0025(0.0074) Grad: 8598.3770  LR: 0.00000709  \n",
      "Epoch: [4][300/1906] Elapsed 1m 16s (remain 6m 45s) Loss: 0.0024(0.0078) Grad: 9560.1182  LR: 0.00000688  \n",
      "Epoch: [4][400/1906] Elapsed 1m 40s (remain 6m 18s) Loss: 0.0004(0.0074) Grad: 3444.8555  LR: 0.00000668  \n",
      "Epoch: [4][500/1906] Elapsed 2m 5s (remain 5m 52s) Loss: 0.0270(0.0071) Grad: 99898.1953  LR: 0.00000648  \n",
      "Epoch: [4][600/1906] Elapsed 2m 30s (remain 5m 27s) Loss: 0.0289(0.0075) Grad: 46527.1367  LR: 0.00000627  \n",
      "Epoch: [4][700/1906] Elapsed 2m 55s (remain 5m 2s) Loss: 0.0076(0.0076) Grad: 21915.9824  LR: 0.00000607  \n",
      "Epoch: [4][800/1906] Elapsed 3m 20s (remain 4m 37s) Loss: 0.0001(0.0076) Grad: 363.9773  LR: 0.00000587  \n",
      "Epoch: [4][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0016(0.0075) Grad: 6708.6738  LR: 0.00000567  \n",
      "Epoch: [4][1000/1906] Elapsed 4m 11s (remain 3m 47s) Loss: 0.0042(0.0076) Grad: 30369.0078  LR: 0.00000547  \n",
      "Epoch: [4][1100/1906] Elapsed 4m 36s (remain 3m 21s) Loss: 0.0037(0.0075) Grad: 26162.3379  LR: 0.00000527  \n",
      "Epoch: [4][1200/1906] Elapsed 5m 0s (remain 2m 56s) Loss: 0.0317(0.0076) Grad: 28474.1777  LR: 0.00000507  \n",
      "Epoch: [4][1300/1906] Elapsed 5m 25s (remain 2m 31s) Loss: 0.0265(0.0078) Grad: 49717.1172  LR: 0.00000488  \n",
      "Epoch: [4][1400/1906] Elapsed 5m 50s (remain 2m 6s) Loss: 0.0013(0.0079) Grad: 5662.1367  LR: 0.00000469  \n",
      "Epoch: [4][1500/1906] Elapsed 6m 15s (remain 1m 41s) Loss: 0.0085(0.0080) Grad: 18225.6387  LR: 0.00000450  \n",
      "Epoch: [4][1600/1906] Elapsed 6m 40s (remain 1m 16s) Loss: 0.0005(0.0080) Grad: 2523.3267  LR: 0.00000431  \n",
      "Epoch: [4][1700/1906] Elapsed 7m 5s (remain 0m 51s) Loss: 0.0003(0.0079) Grad: 6944.1558  LR: 0.00000413  \n",
      "Epoch: [4][1800/1906] Elapsed 7m 30s (remain 0m 26s) Loss: 0.0040(0.0079) Grad: 11920.4316  LR: 0.00000394  \n",
      "Epoch: [4][1900/1906] Elapsed 7m 55s (remain 0m 1s) Loss: 0.0003(0.0080) Grad: 1633.7212  LR: 0.00000376  \n",
      "Epoch: [4][1905/1906] Elapsed 7m 56s (remain 0m 0s) Loss: 0.0067(0.0080) Grad: 14886.5742  LR: 0.00000375  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 0m 58s) Loss: 0.0109(0.0109) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0028(0.0122) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0120(0.0139) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0047(0.0128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0080  avg_val_loss: 0.0128  time: 535s\n",
      "Epoch 4 - Score: 0.8844 for th=0.66\n",
      "Epoch 4 - Save Score: 0.8844 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/1906] Elapsed 0m 0s (remain 14m 27s) Loss: 0.0005(0.0005) Grad: 1707.2677  LR: 0.00000375  \n",
      "Epoch: [5][100/1906] Elapsed 0m 25s (remain 7m 36s) Loss: 0.0047(0.0055) Grad: 9204.2168  LR: 0.00000358  \n",
      "Epoch: [5][200/1906] Elapsed 0m 50s (remain 7m 8s) Loss: 0.0060(0.0069) Grad: 22812.1855  LR: 0.00000340  \n",
      "Epoch: [5][300/1906] Elapsed 1m 15s (remain 6m 42s) Loss: 0.0008(0.0076) Grad: 3371.0845  LR: 0.00000323  \n",
      "Epoch: [5][400/1906] Elapsed 1m 40s (remain 6m 17s) Loss: 0.0137(0.0075) Grad: 29222.9102  LR: 0.00000306  \n",
      "Epoch: [5][500/1906] Elapsed 2m 5s (remain 5m 51s) Loss: 0.0014(0.0073) Grad: 15594.7295  LR: 0.00000290  \n",
      "Epoch: [5][600/1906] Elapsed 2m 30s (remain 5m 25s) Loss: 0.0004(0.0071) Grad: 2866.6492  LR: 0.00000274  \n",
      "Epoch: [5][700/1906] Elapsed 2m 54s (remain 5m 0s) Loss: 0.0146(0.0070) Grad: 36361.0508  LR: 0.00000258  \n",
      "Epoch: [5][800/1906] Elapsed 3m 19s (remain 4m 35s) Loss: 0.0150(0.0068) Grad: 80437.5000  LR: 0.00000243  \n",
      "Epoch: [5][900/1906] Elapsed 3m 44s (remain 4m 10s) Loss: 0.0000(0.0068) Grad: 63.9679  LR: 0.00000228  \n",
      "Epoch: [5][1000/1906] Elapsed 4m 9s (remain 3m 45s) Loss: 0.0007(0.0069) Grad: 7410.0034  LR: 0.00000213  \n",
      "Epoch: [5][1100/1906] Elapsed 4m 35s (remain 3m 21s) Loss: 0.0127(0.0068) Grad: 32051.3789  LR: 0.00000199  \n",
      "Epoch: [5][1200/1906] Elapsed 4m 59s (remain 2m 56s) Loss: 0.0051(0.0068) Grad: 20365.4980  LR: 0.00000185  \n",
      "Epoch: [5][1300/1906] Elapsed 5m 24s (remain 2m 31s) Loss: 0.0001(0.0070) Grad: 269.5411  LR: 0.00000172  \n",
      "Epoch: [5][1400/1906] Elapsed 5m 49s (remain 2m 6s) Loss: 0.0037(0.0071) Grad: 19637.1992  LR: 0.00000159  \n",
      "Epoch: [5][1500/1906] Elapsed 6m 14s (remain 1m 41s) Loss: 0.0093(0.0070) Grad: 57166.5352  LR: 0.00000147  \n",
      "Epoch: [5][1600/1906] Elapsed 6m 39s (remain 1m 16s) Loss: 0.0107(0.0070) Grad: 23708.3672  LR: 0.00000135  \n",
      "Epoch: [5][1700/1906] Elapsed 7m 4s (remain 0m 51s) Loss: 0.0033(0.0070) Grad: 12275.4502  LR: 0.00000123  \n",
      "Epoch: [5][1800/1906] Elapsed 7m 29s (remain 0m 26s) Loss: 0.0040(0.0069) Grad: 73318.0234  LR: 0.00000112  \n",
      "Epoch: [5][1900/1906] Elapsed 7m 54s (remain 0m 1s) Loss: 0.0317(0.0070) Grad: 168943.0938  LR: 0.00000101  \n",
      "Epoch: [5][1905/1906] Elapsed 7m 56s (remain 0m 0s) Loss: 0.0041(0.0070) Grad: 18724.5684  LR: 0.00000101  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 0m 58s) Loss: 0.0125(0.0125) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0036(0.0145) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0156(0.0162) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0073(0.0149) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0070  avg_val_loss: 0.0149  time: 535s\n",
      "Epoch 5 - Score: 0.8836 for th=0.575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/1906] Elapsed 0m 0s (remain 14m 30s) Loss: 0.0000(0.0000) Grad: 52.7082  LR: 0.00000101  \n",
      "Epoch: [6][100/1906] Elapsed 0m 25s (remain 7m 37s) Loss: 0.0422(0.0076) Grad: 37491.3086  LR: 0.00000091  \n",
      "Epoch: [6][200/1906] Elapsed 0m 50s (remain 7m 9s) Loss: 0.0036(0.0072) Grad: 10368.0508  LR: 0.00000081  \n",
      "Epoch: [6][300/1906] Elapsed 1m 15s (remain 6m 45s) Loss: 0.0028(0.0074) Grad: 71705.1797  LR: 0.00000072  \n",
      "Epoch: [6][400/1906] Elapsed 1m 41s (remain 6m 19s) Loss: 0.0071(0.0070) Grad: 26276.8164  LR: 0.00000063  \n",
      "Epoch: [6][500/1906] Elapsed 2m 5s (remain 5m 53s) Loss: 0.0037(0.0069) Grad: 22582.3438  LR: 0.00000055  \n",
      "Epoch: [6][600/1906] Elapsed 2m 30s (remain 5m 27s) Loss: 0.0105(0.0067) Grad: 30523.7715  LR: 0.00000048  \n",
      "Epoch: [6][700/1906] Elapsed 2m 55s (remain 5m 2s) Loss: 0.0068(0.0067) Grad: 47482.7812  LR: 0.00000041  \n",
      "Epoch: [6][800/1906] Elapsed 3m 20s (remain 4m 36s) Loss: 0.0015(0.0067) Grad: 5983.4995  LR: 0.00000035  \n",
      "Epoch: [6][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0080(0.0067) Grad: 52683.3828  LR: 0.00000029  \n",
      "Epoch: [6][1000/1906] Elapsed 4m 11s (remain 3m 47s) Loss: 0.0635(0.0068) Grad: 83823.2344  LR: 0.00000023  \n",
      "Epoch: [6][1100/1906] Elapsed 4m 36s (remain 3m 21s) Loss: 0.0029(0.0067) Grad: 31651.5918  LR: 0.00000018  \n",
      "Epoch: [6][1200/1906] Elapsed 5m 1s (remain 2m 56s) Loss: 0.0054(0.0067) Grad: 25175.3320  LR: 0.00000014  \n",
      "Epoch: [6][1300/1906] Elapsed 5m 26s (remain 2m 31s) Loss: 0.0069(0.0066) Grad: 31754.6758  LR: 0.00000010  \n",
      "Epoch: [6][1400/1906] Elapsed 5m 51s (remain 2m 6s) Loss: 0.0002(0.0066) Grad: 2418.5098  LR: 0.00000007  \n",
      "Epoch: [6][1500/1906] Elapsed 6m 16s (remain 1m 41s) Loss: 0.0047(0.0065) Grad: 11843.3369  LR: 0.00000005  \n",
      "Epoch: [6][1600/1906] Elapsed 6m 41s (remain 1m 16s) Loss: 0.0007(0.0064) Grad: 3530.1985  LR: 0.00000003  \n",
      "Epoch: [6][1700/1906] Elapsed 7m 6s (remain 0m 51s) Loss: 0.0207(0.0063) Grad: 32123.9238  LR: 0.00000001  \n",
      "Epoch: [6][1800/1906] Elapsed 7m 30s (remain 0m 26s) Loss: 0.0001(0.0063) Grad: 210.0881  LR: 0.00000000  \n",
      "Epoch: [6][1900/1906] Elapsed 7m 55s (remain 0m 1s) Loss: 0.0171(0.0062) Grad: 56280.9531  LR: 0.00000000  \n",
      "Epoch: [6][1905/1906] Elapsed 7m 56s (remain 0m 0s) Loss: 0.0001(0.0062) Grad: 258.8398  LR: 0.00000000  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 0s) Loss: 0.0120(0.0120) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0034(0.0146) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0151(0.0163) \n",
      "EVAL: [238/239] Elapsed 0m 29s (remain 0m 0s) Loss: 0.0048(0.0150) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0062  avg_val_loss: 0.0150  time: 536s\n",
      "Epoch 6 - Score: 0.8836 for th=0.555\n",
      "========== fold: 0 result ==========\n",
      "Score: 0.8844 Best threshold:: 0.66\n",
      "========== fold: 1 training ==========\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/1906] Elapsed 0m 0s (remain 12m 39s) Loss: 1.0063(1.0063) Grad: inf  LR: 0.00001500  \n",
      "Epoch: [1][100/1906] Elapsed 0m 25s (remain 7m 34s) Loss: 0.0116(0.1205) Grad: 1643.7045  LR: 0.00001500  \n",
      "Epoch: [1][200/1906] Elapsed 0m 50s (remain 7m 7s) Loss: 0.0095(0.0748) Grad: 7135.2642  LR: 0.00001499  \n",
      "Epoch: [1][300/1906] Elapsed 1m 15s (remain 6m 43s) Loss: 0.0103(0.0573) Grad: 1382.4327  LR: 0.00001497  \n",
      "Epoch: [1][400/1906] Elapsed 1m 40s (remain 6m 17s) Loss: 0.0027(0.0486) Grad: 749.0032  LR: 0.00001495  \n",
      "Epoch: [1][500/1906] Elapsed 2m 5s (remain 5m 53s) Loss: 0.0045(0.0431) Grad: 1437.2698  LR: 0.00001493  \n",
      "Epoch: [1][600/1906] Elapsed 2m 31s (remain 5m 28s) Loss: 0.0481(0.0395) Grad: 3265.0762  LR: 0.00001490  \n",
      "Epoch: [1][700/1906] Elapsed 2m 56s (remain 5m 2s) Loss: 0.0297(0.0367) Grad: 1902.6162  LR: 0.00001486  \n",
      "Epoch: [1][800/1906] Elapsed 3m 21s (remain 4m 37s) Loss: 0.0088(0.0345) Grad: 1295.3027  LR: 0.00001482  \n",
      "Epoch: [1][900/1906] Elapsed 3m 46s (remain 4m 12s) Loss: 0.0143(0.0326) Grad: 7712.3887  LR: 0.00001477  \n",
      "Epoch: [1][1000/1906] Elapsed 4m 11s (remain 3m 47s) Loss: 0.0355(0.0315) Grad: 8694.8975  LR: 0.00001472  \n",
      "Epoch: [1][1100/1906] Elapsed 4m 36s (remain 3m 21s) Loss: 0.0213(0.0299) Grad: 1537.9070  LR: 0.00001466  \n",
      "Epoch: [1][1200/1906] Elapsed 5m 1s (remain 2m 56s) Loss: 0.0318(0.0288) Grad: 2290.3379  LR: 0.00001460  \n",
      "Epoch: [1][1300/1906] Elapsed 5m 26s (remain 2m 31s) Loss: 0.0209(0.0279) Grad: 3999.7439  LR: 0.00001453  \n",
      "Epoch: [1][1400/1906] Elapsed 5m 51s (remain 2m 6s) Loss: 0.0035(0.0269) Grad: 2033.1993  LR: 0.00001445  \n",
      "Epoch: [1][1500/1906] Elapsed 6m 16s (remain 1m 41s) Loss: 0.0222(0.0264) Grad: 1506.0372  LR: 0.00001437  \n",
      "Epoch: [1][1600/1906] Elapsed 6m 41s (remain 1m 16s) Loss: 0.0181(0.0255) Grad: 5607.1777  LR: 0.00001429  \n",
      "Epoch: [1][1700/1906] Elapsed 7m 6s (remain 0m 51s) Loss: 0.0421(0.0250) Grad: 2137.9270  LR: 0.00001420  \n",
      "Epoch: [1][1800/1906] Elapsed 7m 31s (remain 0m 26s) Loss: 0.0097(0.0244) Grad: 919.0313  LR: 0.00001410  \n",
      "Epoch: [1][1900/1906] Elapsed 7m 56s (remain 0m 1s) Loss: 0.0026(0.0240) Grad: 481.6689  LR: 0.00001400  \n",
      "Epoch: [1][1905/1906] Elapsed 7m 57s (remain 0m 0s) Loss: 0.0008(0.0239) Grad: 224.9129  LR: 0.00001400  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 1s) Loss: 0.0053(0.0053) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0047(0.0122) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0149(0.0141) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0007(0.0131) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0239  avg_val_loss: 0.0131  time: 535s\n",
      "Epoch 1 - Score: 0.8612 for th=0.465\n",
      "Epoch 1 - Save Score: 0.8612 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/1906] Elapsed 0m 0s (remain 14m 5s) Loss: 0.0034(0.0034) Grad: 4577.4028  LR: 0.00001399  \n",
      "Epoch: [2][100/1906] Elapsed 0m 25s (remain 7m 32s) Loss: 0.0095(0.0110) Grad: 20239.7715  LR: 0.00001389  \n",
      "Epoch: [2][200/1906] Elapsed 0m 50s (remain 7m 6s) Loss: 0.0126(0.0118) Grad: 52507.3672  LR: 0.00001378  \n",
      "Epoch: [2][300/1906] Elapsed 1m 15s (remain 6m 40s) Loss: 0.0028(0.0110) Grad: 7710.0879  LR: 0.00001366  \n",
      "Epoch: [2][400/1906] Elapsed 1m 40s (remain 6m 15s) Loss: 0.0107(0.0110) Grad: 35143.1484  LR: 0.00001354  \n",
      "Epoch: [2][500/1906] Elapsed 2m 5s (remain 5m 51s) Loss: 0.0010(0.0109) Grad: 3991.4290  LR: 0.00001342  \n",
      "Epoch: [2][600/1906] Elapsed 2m 30s (remain 5m 26s) Loss: 0.0012(0.0104) Grad: 4149.2979  LR: 0.00001329  \n",
      "Epoch: [2][700/1906] Elapsed 2m 55s (remain 5m 1s) Loss: 0.0108(0.0105) Grad: 16762.3340  LR: 0.00001316  \n",
      "Epoch: [2][800/1906] Elapsed 3m 20s (remain 4m 36s) Loss: 0.0005(0.0101) Grad: 1454.6656  LR: 0.00001302  \n",
      "Epoch: [2][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0055(0.0099) Grad: 35088.6094  LR: 0.00001288  \n",
      "Epoch: [2][1000/1906] Elapsed 4m 10s (remain 3m 46s) Loss: 0.0028(0.0099) Grad: 6810.2769  LR: 0.00001273  \n",
      "Epoch: [2][1100/1906] Elapsed 4m 35s (remain 3m 21s) Loss: 0.0093(0.0100) Grad: 27884.0391  LR: 0.00001258  \n",
      "Epoch: [2][1200/1906] Elapsed 5m 0s (remain 2m 56s) Loss: 0.0002(0.0098) Grad: 1605.9814  LR: 0.00001243  \n",
      "Epoch: [2][1300/1906] Elapsed 5m 25s (remain 2m 31s) Loss: 0.0063(0.0100) Grad: 21576.1094  LR: 0.00001227  \n",
      "Epoch: [2][1400/1906] Elapsed 5m 50s (remain 2m 6s) Loss: 0.0174(0.0100) Grad: 19649.2383  LR: 0.00001211  \n",
      "Epoch: [2][1500/1906] Elapsed 6m 15s (remain 1m 41s) Loss: 0.0051(0.0099) Grad: 14298.8447  LR: 0.00001195  \n",
      "Epoch: [2][1600/1906] Elapsed 6m 40s (remain 1m 16s) Loss: 0.0048(0.0100) Grad: 12997.5664  LR: 0.00001178  \n",
      "Epoch: [2][1700/1906] Elapsed 7m 5s (remain 0m 51s) Loss: 0.0017(0.0101) Grad: 4042.7668  LR: 0.00001161  \n",
      "Epoch: [2][1800/1906] Elapsed 7m 30s (remain 0m 26s) Loss: 0.0015(0.0100) Grad: 3545.7158  LR: 0.00001144  \n",
      "Epoch: [2][1900/1906] Elapsed 7m 55s (remain 0m 1s) Loss: 0.0005(0.0100) Grad: 2092.6177  LR: 0.00001126  \n",
      "Epoch: [2][1905/1906] Elapsed 7m 56s (remain 0m 0s) Loss: 0.0009(0.0100) Grad: 5090.7617  LR: 0.00001125  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 3s) Loss: 0.0049(0.0049) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0118(0.0117) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0167(0.0139) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0001(0.0129) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0100  avg_val_loss: 0.0129  time: 534s\n",
      "Epoch 2 - Score: 0.8824 for th=0.325\n",
      "Epoch 2 - Save Score: 0.8824 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/1906] Elapsed 0m 0s (remain 15m 12s) Loss: 0.0194(0.0194) Grad: 14930.6270  LR: 0.00001125  \n",
      "Epoch: [3][100/1906] Elapsed 0m 25s (remain 7m 39s) Loss: 0.0015(0.0089) Grad: 6639.5098  LR: 0.00001107  \n",
      "Epoch: [3][200/1906] Elapsed 0m 50s (remain 7m 11s) Loss: 0.0099(0.0093) Grad: 19350.7676  LR: 0.00001089  \n",
      "Epoch: [3][300/1906] Elapsed 1m 15s (remain 6m 44s) Loss: 0.0003(0.0087) Grad: 2777.9202  LR: 0.00001070  \n",
      "Epoch: [3][400/1906] Elapsed 1m 41s (remain 6m 19s) Loss: 0.0079(0.0085) Grad: 17446.9648  LR: 0.00001052  \n",
      "Epoch: [3][500/1906] Elapsed 2m 6s (remain 5m 53s) Loss: 0.0180(0.0086) Grad: 22617.0449  LR: 0.00001033  \n",
      "Epoch: [3][600/1906] Elapsed 2m 31s (remain 5m 27s) Loss: 0.0029(0.0086) Grad: 23861.1738  LR: 0.00001013  \n",
      "Epoch: [3][700/1906] Elapsed 2m 55s (remain 5m 2s) Loss: 0.0040(0.0085) Grad: 15521.0332  LR: 0.00000994  \n",
      "Epoch: [3][800/1906] Elapsed 3m 20s (remain 4m 37s) Loss: 0.0003(0.0084) Grad: 2214.5125  LR: 0.00000975  \n",
      "Epoch: [3][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0016(0.0083) Grad: 27141.9219  LR: 0.00000955  \n",
      "Epoch: [3][1000/1906] Elapsed 4m 10s (remain 3m 46s) Loss: 0.0006(0.0085) Grad: 2525.2659  LR: 0.00000935  \n",
      "Epoch: [3][1100/1906] Elapsed 4m 35s (remain 3m 21s) Loss: 0.0006(0.0085) Grad: 1541.8850  LR: 0.00000915  \n",
      "Epoch: [3][1200/1906] Elapsed 5m 1s (remain 2m 56s) Loss: 0.0004(0.0084) Grad: 1554.7013  LR: 0.00000895  \n",
      "Epoch: [3][1300/1906] Elapsed 5m 26s (remain 2m 31s) Loss: 0.0031(0.0084) Grad: 10022.0918  LR: 0.00000874  \n",
      "Epoch: [3][1400/1906] Elapsed 5m 51s (remain 2m 6s) Loss: 0.0148(0.0084) Grad: 30111.7617  LR: 0.00000854  \n",
      "Epoch: [3][1500/1906] Elapsed 6m 16s (remain 1m 41s) Loss: 0.0008(0.0084) Grad: 4996.1191  LR: 0.00000834  \n",
      "Epoch: [3][1600/1906] Elapsed 6m 41s (remain 1m 16s) Loss: 0.0117(0.0085) Grad: 71024.8750  LR: 0.00000813  \n",
      "Epoch: [3][1700/1906] Elapsed 7m 6s (remain 0m 51s) Loss: 0.0026(0.0085) Grad: 19779.7207  LR: 0.00000793  \n",
      "Epoch: [3][1800/1906] Elapsed 7m 32s (remain 0m 26s) Loss: 0.0054(0.0086) Grad: 43867.0938  LR: 0.00000772  \n",
      "Epoch: [3][1900/1906] Elapsed 7m 57s (remain 0m 1s) Loss: 0.0203(0.0085) Grad: 32199.6172  LR: 0.00000751  \n",
      "Epoch: [3][1905/1906] Elapsed 7m 58s (remain 0m 0s) Loss: 0.0114(0.0085) Grad: 18367.9355  LR: 0.00000750  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 0s) Loss: 0.0042(0.0042) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0160(0.0124) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0201(0.0146) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0001(0.0137) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0085  avg_val_loss: 0.0137  time: 536s\n",
      "Epoch 3 - Score: 0.8805 for th=0.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/1906] Elapsed 0m 0s (remain 14m 46s) Loss: 0.0091(0.0091) Grad: 19609.5977  LR: 0.00000750  \n",
      "Epoch: [4][100/1906] Elapsed 0m 25s (remain 7m 39s) Loss: 0.0003(0.0070) Grad: 1663.5278  LR: 0.00000730  \n",
      "Epoch: [4][200/1906] Elapsed 0m 50s (remain 7m 11s) Loss: 0.0077(0.0067) Grad: 27464.3047  LR: 0.00000709  \n",
      "Epoch: [4][300/1906] Elapsed 1m 15s (remain 6m 44s) Loss: 0.0016(0.0074) Grad: 10533.5781  LR: 0.00000688  \n",
      "Epoch: [4][400/1906] Elapsed 1m 40s (remain 6m 18s) Loss: 0.0001(0.0074) Grad: 496.6645  LR: 0.00000668  \n",
      "Epoch: [4][500/1906] Elapsed 2m 5s (remain 5m 52s) Loss: 0.0001(0.0074) Grad: 1041.9352  LR: 0.00000648  \n",
      "Epoch: [4][600/1906] Elapsed 2m 30s (remain 5m 27s) Loss: 0.0001(0.0077) Grad: 429.6658  LR: 0.00000627  \n",
      "Epoch: [4][700/1906] Elapsed 2m 55s (remain 5m 2s) Loss: 0.0340(0.0079) Grad: 45131.1523  LR: 0.00000607  \n",
      "Epoch: [4][800/1906] Elapsed 3m 20s (remain 4m 37s) Loss: 0.0022(0.0078) Grad: 11251.0293  LR: 0.00000587  \n",
      "Epoch: [4][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0003(0.0077) Grad: 1501.5573  LR: 0.00000567  \n",
      "Epoch: [4][1000/1906] Elapsed 4m 10s (remain 3m 46s) Loss: 0.0026(0.0078) Grad: 8249.7637  LR: 0.00000547  \n",
      "Epoch: [4][1100/1906] Elapsed 4m 35s (remain 3m 21s) Loss: 0.0023(0.0077) Grad: 10395.6465  LR: 0.00000527  \n",
      "Epoch: [4][1200/1906] Elapsed 5m 0s (remain 2m 56s) Loss: 0.0019(0.0076) Grad: 11725.3242  LR: 0.00000507  \n",
      "Epoch: [4][1300/1906] Elapsed 5m 25s (remain 2m 31s) Loss: 0.0048(0.0075) Grad: 41277.8359  LR: 0.00000488  \n",
      "Epoch: [4][1400/1906] Elapsed 5m 50s (remain 2m 6s) Loss: 0.0021(0.0074) Grad: 5894.9980  LR: 0.00000469  \n",
      "Epoch: [4][1500/1906] Elapsed 6m 15s (remain 1m 41s) Loss: 0.0003(0.0074) Grad: 2622.3179  LR: 0.00000450  \n",
      "Epoch: [4][1600/1906] Elapsed 6m 40s (remain 1m 16s) Loss: 0.0007(0.0074) Grad: 2705.7871  LR: 0.00000431  \n",
      "Epoch: [4][1700/1906] Elapsed 7m 5s (remain 0m 51s) Loss: 0.0166(0.0074) Grad: 30389.4141  LR: 0.00000413  \n",
      "Epoch: [4][1800/1906] Elapsed 7m 30s (remain 0m 26s) Loss: 0.0002(0.0074) Grad: 1598.5033  LR: 0.00000394  \n",
      "Epoch: [4][1900/1906] Elapsed 7m 55s (remain 0m 1s) Loss: 0.0001(0.0073) Grad: 306.6916  LR: 0.00000376  \n",
      "Epoch: [4][1905/1906] Elapsed 7m 57s (remain 0m 0s) Loss: 0.0107(0.0073) Grad: 19112.6777  LR: 0.00000375  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 1s) Loss: 0.0062(0.0062) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0223(0.0139) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0289(0.0159) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0001(0.0150) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0073  avg_val_loss: 0.0150  time: 535s\n",
      "Epoch 4 - Score: 0.8848 for th=0.35\n",
      "Epoch 4 - Save Score: 0.8848 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/1906] Elapsed 0m 0s (remain 14m 59s) Loss: 0.0051(0.0051) Grad: 26165.1230  LR: 0.00000375  \n",
      "Epoch: [5][100/1906] Elapsed 0m 25s (remain 7m 40s) Loss: 0.0014(0.0065) Grad: 7427.2344  LR: 0.00000358  \n",
      "Epoch: [5][200/1906] Elapsed 0m 50s (remain 7m 12s) Loss: 0.0177(0.0057) Grad: 32120.1953  LR: 0.00000340  \n",
      "Epoch: [5][300/1906] Elapsed 1m 16s (remain 6m 45s) Loss: 0.0013(0.0056) Grad: 81047.9453  LR: 0.00000323  \n",
      "Epoch: [5][400/1906] Elapsed 1m 41s (remain 6m 22s) Loss: 0.0011(0.0061) Grad: 39624.2461  LR: 0.00000306  \n",
      "Epoch: [5][500/1906] Elapsed 2m 6s (remain 5m 56s) Loss: 0.0009(0.0062) Grad: 5237.9912  LR: 0.00000290  \n",
      "Epoch: [5][600/1906] Elapsed 2m 31s (remain 5m 29s) Loss: 0.0064(0.0061) Grad: 15729.2109  LR: 0.00000274  \n",
      "Epoch: [5][700/1906] Elapsed 2m 56s (remain 5m 3s) Loss: 0.0012(0.0062) Grad: 10521.5469  LR: 0.00000258  \n",
      "Epoch: [5][800/1906] Elapsed 3m 21s (remain 4m 38s) Loss: 0.0282(0.0061) Grad: 153940.0312  LR: 0.00000243  \n",
      "Epoch: [5][900/1906] Elapsed 3m 46s (remain 4m 12s) Loss: 0.0005(0.0060) Grad: 3598.0815  LR: 0.00000228  \n",
      "Epoch: [5][1000/1906] Elapsed 4m 11s (remain 3m 47s) Loss: 0.0079(0.0061) Grad: 25064.0488  LR: 0.00000213  \n",
      "Epoch: [5][1100/1906] Elapsed 4m 36s (remain 3m 22s) Loss: 0.0004(0.0061) Grad: 2760.8831  LR: 0.00000199  \n",
      "Epoch: [5][1200/1906] Elapsed 5m 2s (remain 2m 57s) Loss: 0.0096(0.0062) Grad: 54223.8477  LR: 0.00000185  \n",
      "Epoch: [5][1300/1906] Elapsed 5m 27s (remain 2m 32s) Loss: 0.0177(0.0062) Grad: 26886.0117  LR: 0.00000172  \n",
      "Epoch: [5][1400/1906] Elapsed 5m 52s (remain 2m 7s) Loss: 0.0055(0.0061) Grad: 25571.6836  LR: 0.00000159  \n",
      "Epoch: [5][1500/1906] Elapsed 6m 17s (remain 1m 41s) Loss: 0.0149(0.0062) Grad: 36538.0781  LR: 0.00000147  \n",
      "Epoch: [5][1600/1906] Elapsed 6m 43s (remain 1m 16s) Loss: 0.0342(0.0063) Grad: 51239.3789  LR: 0.00000135  \n",
      "Epoch: [5][1700/1906] Elapsed 7m 8s (remain 0m 51s) Loss: 0.0003(0.0063) Grad: 2339.6904  LR: 0.00000123  \n",
      "Epoch: [5][1800/1906] Elapsed 7m 33s (remain 0m 26s) Loss: 0.0049(0.0062) Grad: 5223.4785  LR: 0.00000112  \n",
      "Epoch: [5][1900/1906] Elapsed 7m 58s (remain 0m 1s) Loss: 0.0000(0.0062) Grad: 220.4246  LR: 0.00000101  \n",
      "Epoch: [5][1905/1906] Elapsed 7m 59s (remain 0m 0s) Loss: 0.0103(0.0062) Grad: 44323.3320  LR: 0.00000101  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 6s) Loss: 0.0078(0.0078) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0220(0.0147) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0293(0.0169) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0000(0.0161) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0062  avg_val_loss: 0.0161  time: 536s\n",
      "Epoch 5 - Score: 0.8812 for th=0.345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/1906] Elapsed 0m 0s (remain 14m 38s) Loss: 0.0041(0.0041) Grad: 15353.1055  LR: 0.00000101  \n",
      "Epoch: [6][100/1906] Elapsed 0m 25s (remain 7m 31s) Loss: 0.0001(0.0051) Grad: 436.2000  LR: 0.00000091  \n",
      "Epoch: [6][200/1906] Elapsed 0m 50s (remain 7m 6s) Loss: 0.0083(0.0051) Grad: 13693.9434  LR: 0.00000081  \n",
      "Epoch: [6][300/1906] Elapsed 1m 15s (remain 6m 43s) Loss: 0.0142(0.0049) Grad: 71121.6094  LR: 0.00000072  \n",
      "Epoch: [6][400/1906] Elapsed 1m 41s (remain 6m 19s) Loss: 0.0021(0.0048) Grad: 9484.0889  LR: 0.00000063  \n",
      "Epoch: [6][500/1906] Elapsed 2m 6s (remain 5m 53s) Loss: 0.0007(0.0049) Grad: 5058.2051  LR: 0.00000055  \n",
      "Epoch: [6][600/1906] Elapsed 2m 31s (remain 5m 28s) Loss: 0.0006(0.0052) Grad: 7032.0693  LR: 0.00000048  \n",
      "Epoch: [6][700/1906] Elapsed 2m 56s (remain 5m 2s) Loss: 0.0000(0.0051) Grad: 130.0101  LR: 0.00000041  \n",
      "Epoch: [6][800/1906] Elapsed 3m 21s (remain 4m 37s) Loss: 0.0006(0.0052) Grad: 4389.5215  LR: 0.00000035  \n",
      "Epoch: [6][900/1906] Elapsed 3m 45s (remain 4m 12s) Loss: 0.0124(0.0054) Grad: 38388.4492  LR: 0.00000029  \n",
      "Epoch: [6][1000/1906] Elapsed 4m 10s (remain 3m 46s) Loss: 0.0018(0.0055) Grad: 12314.0391  LR: 0.00000023  \n",
      "Epoch: [6][1100/1906] Elapsed 4m 36s (remain 3m 21s) Loss: 0.0018(0.0055) Grad: 23758.5488  LR: 0.00000018  \n",
      "Epoch: [6][1200/1906] Elapsed 5m 1s (remain 2m 56s) Loss: 0.0131(0.0056) Grad: 113254.3984  LR: 0.00000014  \n",
      "Epoch: [6][1300/1906] Elapsed 5m 25s (remain 2m 31s) Loss: 0.0000(0.0057) Grad: 254.8900  LR: 0.00000010  \n",
      "Epoch: [6][1400/1906] Elapsed 5m 51s (remain 2m 6s) Loss: 0.0119(0.0056) Grad: 32418.0879  LR: 0.00000007  \n",
      "Epoch: [6][1500/1906] Elapsed 6m 16s (remain 1m 41s) Loss: 0.0081(0.0056) Grad: 13671.9404  LR: 0.00000005  \n",
      "Epoch: [6][1600/1906] Elapsed 6m 41s (remain 1m 16s) Loss: 0.0017(0.0055) Grad: 12224.5371  LR: 0.00000003  \n",
      "Epoch: [6][1700/1906] Elapsed 7m 6s (remain 0m 51s) Loss: 0.0107(0.0055) Grad: 26721.2559  LR: 0.00000001  \n",
      "Epoch: [6][1800/1906] Elapsed 7m 31s (remain 0m 26s) Loss: 0.0115(0.0055) Grad: 151977.4062  LR: 0.00000000  \n",
      "Epoch: [6][1900/1906] Elapsed 7m 56s (remain 0m 1s) Loss: 0.0322(0.0056) Grad: 139878.7656  LR: 0.00000000  \n",
      "Epoch: [6][1905/1906] Elapsed 7m 57s (remain 0m 0s) Loss: 0.0039(0.0056) Grad: 27259.4004  LR: 0.00000000  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 1s) Loss: 0.0085(0.0085) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0227(0.0153) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0301(0.0176) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0000(0.0167) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0056  avg_val_loss: 0.0167  time: 535s\n",
      "Epoch 6 - Score: 0.8808 for th=0.3\n",
      "========== fold: 1 result ==========\n",
      "Score: 0.8848 Best threshold:: 0.35\n",
      "========== fold: 2 training ==========\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/1906] Elapsed 0m 0s (remain 13m 3s) Loss: 0.9086(0.9086) Grad: inf  LR: 0.00001500  \n",
      "Epoch: [1][100/1906] Elapsed 0m 25s (remain 7m 36s) Loss: 0.0112(0.1079) Grad: 2182.5891  LR: 0.00001500  \n",
      "Epoch: [1][200/1906] Elapsed 0m 50s (remain 7m 12s) Loss: 0.0366(0.0702) Grad: 6752.6948  LR: 0.00001499  \n",
      "Epoch: [1][300/1906] Elapsed 1m 15s (remain 6m 43s) Loss: 0.0163(0.0543) Grad: 5261.9121  LR: 0.00001497  \n",
      "Epoch: [1][400/1906] Elapsed 1m 40s (remain 6m 18s) Loss: 0.0132(0.0473) Grad: 3405.0090  LR: 0.00001495  \n",
      "Epoch: [1][500/1906] Elapsed 2m 5s (remain 5m 53s) Loss: 0.0082(0.0423) Grad: 2977.6626  LR: 0.00001493  \n",
      "Epoch: [1][600/1906] Elapsed 2m 30s (remain 5m 27s) Loss: 0.0191(0.0386) Grad: 1954.3705  LR: 0.00001490  \n",
      "Epoch: [1][700/1906] Elapsed 2m 56s (remain 5m 2s) Loss: 0.0520(0.0358) Grad: 5556.7935  LR: 0.00001486  \n",
      "Epoch: [1][800/1906] Elapsed 3m 21s (remain 4m 37s) Loss: 0.0032(0.0338) Grad: 323.7139  LR: 0.00001482  \n",
      "Epoch: [1][900/1906] Elapsed 3m 46s (remain 4m 12s) Loss: 0.0183(0.0323) Grad: 1693.7875  LR: 0.00001477  \n",
      "Epoch: [1][1000/1906] Elapsed 4m 11s (remain 3m 47s) Loss: 0.0009(0.0309) Grad: 527.5370  LR: 0.00001472  \n",
      "Epoch: [1][1100/1906] Elapsed 4m 36s (remain 3m 22s) Loss: 0.0207(0.0298) Grad: 6924.5781  LR: 0.00001466  \n",
      "Epoch: [1][1200/1906] Elapsed 5m 2s (remain 2m 57s) Loss: 0.0063(0.0285) Grad: 1960.4971  LR: 0.00001460  \n",
      "Epoch: [1][1300/1906] Elapsed 5m 27s (remain 2m 32s) Loss: 0.0136(0.0276) Grad: 1304.6328  LR: 0.00001453  \n",
      "Epoch: [1][1400/1906] Elapsed 5m 51s (remain 2m 6s) Loss: 0.0048(0.0267) Grad: 584.0219  LR: 0.00001445  \n",
      "Epoch: [1][1500/1906] Elapsed 6m 16s (remain 1m 41s) Loss: 0.0280(0.0260) Grad: 1710.8510  LR: 0.00001437  \n",
      "Epoch: [1][1600/1906] Elapsed 6m 41s (remain 1m 16s) Loss: 0.0407(0.0254) Grad: 5172.1978  LR: 0.00001429  \n",
      "Epoch: [1][1700/1906] Elapsed 7m 6s (remain 0m 51s) Loss: 0.0043(0.0248) Grad: 545.4727  LR: 0.00001420  \n",
      "Epoch: [1][1800/1906] Elapsed 7m 31s (remain 0m 26s) Loss: 0.0373(0.0241) Grad: 6771.3638  LR: 0.00001410  \n",
      "Epoch: [1][1900/1906] Elapsed 7m 56s (remain 0m 1s) Loss: 0.0088(0.0236) Grad: 1167.8531  LR: 0.00001400  \n",
      "Epoch: [1][1905/1906] Elapsed 7m 57s (remain 0m 0s) Loss: 0.0169(0.0236) Grad: 1856.4183  LR: 0.00001400  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 2s) Loss: 0.0063(0.0063) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0044(0.0126) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0087(0.0131) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0012(0.0122) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0236  avg_val_loss: 0.0122  time: 537s\n",
      "Epoch 1 - Score: 0.8635 for th=0.585\n",
      "Epoch 1 - Save Score: 0.8635 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/1906] Elapsed 0m 0s (remain 15m 31s) Loss: 0.0074(0.0074) Grad: 21075.2891  LR: 0.00001399  \n",
      "Epoch: [2][100/1906] Elapsed 0m 25s (remain 7m 33s) Loss: 0.0025(0.0116) Grad: 8725.1270  LR: 0.00001389  \n",
      "Epoch: [2][200/1906] Elapsed 0m 50s (remain 7m 6s) Loss: 0.0054(0.0103) Grad: 10504.4072  LR: 0.00001378  \n",
      "Epoch: [2][300/1906] Elapsed 1m 15s (remain 6m 41s) Loss: 0.0082(0.0104) Grad: 77086.1406  LR: 0.00001366  \n",
      "Epoch: [2][400/1906] Elapsed 1m 40s (remain 6m 15s) Loss: 0.0068(0.0110) Grad: 10695.7246  LR: 0.00001354  \n",
      "Epoch: [2][500/1906] Elapsed 2m 4s (remain 5m 50s) Loss: 0.0074(0.0108) Grad: 28287.5586  LR: 0.00001342  \n",
      "Epoch: [2][600/1906] Elapsed 2m 29s (remain 5m 25s) Loss: 0.0125(0.0107) Grad: 32904.2852  LR: 0.00001329  \n",
      "Epoch: [2][700/1906] Elapsed 2m 55s (remain 5m 1s) Loss: 0.0128(0.0107) Grad: 22923.4531  LR: 0.00001316  \n",
      "Epoch: [2][800/1906] Elapsed 3m 20s (remain 4m 36s) Loss: 0.0040(0.0106) Grad: 32252.1250  LR: 0.00001302  \n",
      "Epoch: [2][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0098(0.0106) Grad: 13766.3838  LR: 0.00001288  \n",
      "Epoch: [2][1000/1906] Elapsed 4m 10s (remain 3m 46s) Loss: 0.0043(0.0106) Grad: 11592.6777  LR: 0.00001273  \n",
      "Epoch: [2][1100/1906] Elapsed 4m 35s (remain 3m 21s) Loss: 0.0020(0.0105) Grad: 6332.6714  LR: 0.00001258  \n",
      "Epoch: [2][1200/1906] Elapsed 4m 59s (remain 2m 56s) Loss: 0.0026(0.0104) Grad: 10081.4287  LR: 0.00001243  \n",
      "Epoch: [2][1300/1906] Elapsed 5m 24s (remain 2m 30s) Loss: 0.0005(0.0102) Grad: 2021.3949  LR: 0.00001227  \n",
      "Epoch: [2][1400/1906] Elapsed 5m 49s (remain 2m 6s) Loss: 0.0010(0.0102) Grad: 3477.7515  LR: 0.00001211  \n",
      "Epoch: [2][1500/1906] Elapsed 6m 14s (remain 1m 41s) Loss: 0.0002(0.0102) Grad: 1085.5388  LR: 0.00001195  \n",
      "Epoch: [2][1600/1906] Elapsed 6m 39s (remain 1m 16s) Loss: 0.0043(0.0102) Grad: 8497.4482  LR: 0.00001178  \n",
      "Epoch: [2][1700/1906] Elapsed 7m 4s (remain 0m 51s) Loss: 0.0280(0.0101) Grad: 31281.2246  LR: 0.00001161  \n",
      "Epoch: [2][1800/1906] Elapsed 7m 29s (remain 0m 26s) Loss: 0.0018(0.0101) Grad: 11165.5273  LR: 0.00001144  \n",
      "Epoch: [2][1900/1906] Elapsed 7m 54s (remain 0m 1s) Loss: 0.0008(0.0101) Grad: 11598.3936  LR: 0.00001126  \n",
      "Epoch: [2][1905/1906] Elapsed 7m 55s (remain 0m 0s) Loss: 0.0023(0.0101) Grad: 7167.2656  LR: 0.00001125  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 3s) Loss: 0.0076(0.0076) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 17s) Loss: 0.0051(0.0145) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0066(0.0144) \n",
      "EVAL: [238/239] Elapsed 0m 29s (remain 0m 0s) Loss: 0.0001(0.0132) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0101  avg_val_loss: 0.0132  time: 534s\n",
      "Epoch 2 - Score: 0.8772 for th=0.515\n",
      "Epoch 2 - Save Score: 0.8772 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/1906] Elapsed 0m 0s (remain 15m 21s) Loss: 0.0048(0.0048) Grad: 15509.4648  LR: 0.00001125  \n",
      "Epoch: [3][100/1906] Elapsed 0m 25s (remain 7m 37s) Loss: 0.0039(0.0084) Grad: 20311.5918  LR: 0.00001107  \n",
      "Epoch: [3][200/1906] Elapsed 0m 50s (remain 7m 7s) Loss: 0.0170(0.0082) Grad: 18751.7246  LR: 0.00001089  \n",
      "Epoch: [3][300/1906] Elapsed 1m 15s (remain 6m 41s) Loss: 0.0742(0.0086) Grad: 79153.2500  LR: 0.00001070  \n",
      "Epoch: [3][400/1906] Elapsed 1m 40s (remain 6m 15s) Loss: 0.0064(0.0088) Grad: 23775.1094  LR: 0.00001052  \n",
      "Epoch: [3][500/1906] Elapsed 2m 4s (remain 5m 50s) Loss: 0.0002(0.0087) Grad: 724.7931  LR: 0.00001033  \n",
      "Epoch: [3][600/1906] Elapsed 2m 30s (remain 5m 26s) Loss: 0.0002(0.0088) Grad: 1205.2024  LR: 0.00001013  \n",
      "Epoch: [3][700/1906] Elapsed 2m 55s (remain 5m 1s) Loss: 0.0001(0.0088) Grad: 193.7339  LR: 0.00000994  \n",
      "Epoch: [3][800/1906] Elapsed 3m 19s (remain 4m 35s) Loss: 0.0094(0.0088) Grad: 8384.6885  LR: 0.00000975  \n",
      "Epoch: [3][900/1906] Elapsed 3m 44s (remain 4m 10s) Loss: 0.0077(0.0087) Grad: 29756.9199  LR: 0.00000955  \n",
      "Epoch: [3][1000/1906] Elapsed 4m 9s (remain 3m 45s) Loss: 0.0015(0.0088) Grad: 11389.2080  LR: 0.00000935  \n",
      "Epoch: [3][1100/1906] Elapsed 4m 34s (remain 3m 20s) Loss: 0.0054(0.0088) Grad: 18981.6543  LR: 0.00000915  \n",
      "Epoch: [3][1200/1906] Elapsed 4m 59s (remain 2m 55s) Loss: 0.0160(0.0087) Grad: 19079.8809  LR: 0.00000895  \n",
      "Epoch: [3][1300/1906] Elapsed 5m 23s (remain 2m 30s) Loss: 0.0108(0.0086) Grad: 11252.1807  LR: 0.00000874  \n",
      "Epoch: [3][1400/1906] Elapsed 5m 48s (remain 2m 5s) Loss: 0.0018(0.0086) Grad: 6729.2271  LR: 0.00000854  \n",
      "Epoch: [3][1500/1906] Elapsed 6m 13s (remain 1m 40s) Loss: 0.0018(0.0087) Grad: 14655.8828  LR: 0.00000834  \n",
      "Epoch: [3][1600/1906] Elapsed 6m 38s (remain 1m 15s) Loss: 0.0080(0.0087) Grad: 22505.7188  LR: 0.00000813  \n",
      "Epoch: [3][1700/1906] Elapsed 7m 3s (remain 0m 51s) Loss: 0.0064(0.0088) Grad: 18474.8613  LR: 0.00000793  \n",
      "Epoch: [3][1800/1906] Elapsed 7m 28s (remain 0m 26s) Loss: 0.0186(0.0088) Grad: 52172.5781  LR: 0.00000772  \n",
      "Epoch: [3][1900/1906] Elapsed 7m 52s (remain 0m 1s) Loss: 0.0031(0.0087) Grad: 9205.9707  LR: 0.00000751  \n",
      "Epoch: [3][1905/1906] Elapsed 7m 54s (remain 0m 0s) Loss: 0.0035(0.0087) Grad: 9397.9873  LR: 0.00000750  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 3s) Loss: 0.0104(0.0104) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0047(0.0154) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0079(0.0151) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0001(0.0138) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0087  avg_val_loss: 0.0138  time: 532s\n",
      "Epoch 3 - Score: 0.8811 for th=0.515\n",
      "Epoch 3 - Save Score: 0.8811 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/1906] Elapsed 0m 0s (remain 14m 55s) Loss: 0.0017(0.0017) Grad: 2764.8450  LR: 0.00000750  \n",
      "Epoch: [4][100/1906] Elapsed 0m 25s (remain 7m 39s) Loss: 0.0181(0.0078) Grad: 32354.4766  LR: 0.00000730  \n",
      "Epoch: [4][200/1906] Elapsed 0m 50s (remain 7m 10s) Loss: 0.0289(0.0076) Grad: 55762.8320  LR: 0.00000709  \n",
      "Epoch: [4][300/1906] Elapsed 1m 15s (remain 6m 42s) Loss: 0.0082(0.0084) Grad: 11249.9131  LR: 0.00000688  \n",
      "Epoch: [4][400/1906] Elapsed 1m 40s (remain 6m 16s) Loss: 0.0005(0.0084) Grad: 2473.7346  LR: 0.00000668  \n",
      "Epoch: [4][500/1906] Elapsed 2m 5s (remain 5m 51s) Loss: 0.0064(0.0080) Grad: 26522.1855  LR: 0.00000648  \n",
      "Epoch: [4][600/1906] Elapsed 2m 30s (remain 5m 26s) Loss: 0.0064(0.0080) Grad: 23239.7031  LR: 0.00000627  \n",
      "Epoch: [4][700/1906] Elapsed 2m 55s (remain 5m 1s) Loss: 0.0055(0.0082) Grad: 12067.0195  LR: 0.00000607  \n",
      "Epoch: [4][800/1906] Elapsed 3m 20s (remain 4m 36s) Loss: 0.0024(0.0079) Grad: 10282.7354  LR: 0.00000587  \n",
      "Epoch: [4][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0000(0.0079) Grad: 195.4820  LR: 0.00000567  \n",
      "Epoch: [4][1000/1906] Elapsed 4m 10s (remain 3m 46s) Loss: 0.0136(0.0079) Grad: 68528.2500  LR: 0.00000547  \n",
      "Epoch: [4][1100/1906] Elapsed 4m 35s (remain 3m 21s) Loss: 0.0005(0.0077) Grad: 4270.3740  LR: 0.00000527  \n",
      "Epoch: [4][1200/1906] Elapsed 5m 0s (remain 2m 56s) Loss: 0.0281(0.0077) Grad: 76131.1875  LR: 0.00000507  \n",
      "Epoch: [4][1300/1906] Elapsed 5m 25s (remain 2m 31s) Loss: 0.0061(0.0077) Grad: 33798.8984  LR: 0.00000488  \n",
      "Epoch: [4][1400/1906] Elapsed 5m 50s (remain 2m 6s) Loss: 0.0000(0.0076) Grad: 113.6663  LR: 0.00000469  \n",
      "Epoch: [4][1500/1906] Elapsed 6m 15s (remain 1m 41s) Loss: 0.0001(0.0077) Grad: 990.0108  LR: 0.00000450  \n",
      "Epoch: [4][1600/1906] Elapsed 6m 40s (remain 1m 16s) Loss: 0.0295(0.0077) Grad: 49246.7656  LR: 0.00000431  \n",
      "Epoch: [4][1700/1906] Elapsed 7m 5s (remain 0m 51s) Loss: 0.0001(0.0076) Grad: 1167.2023  LR: 0.00000413  \n",
      "Epoch: [4][1800/1906] Elapsed 7m 30s (remain 0m 26s) Loss: 0.0058(0.0076) Grad: 23784.7344  LR: 0.00000394  \n",
      "Epoch: [4][1900/1906] Elapsed 7m 55s (remain 0m 1s) Loss: 0.0073(0.0077) Grad: 36849.9453  LR: 0.00000376  \n",
      "Epoch: [4][1905/1906] Elapsed 7m 57s (remain 0m 0s) Loss: 0.0064(0.0077) Grad: 7611.5122  LR: 0.00000375  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 5s) Loss: 0.0085(0.0085) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0067(0.0159) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0058(0.0158) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0001(0.0145) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0077  avg_val_loss: 0.0145  time: 535s\n",
      "Epoch 4 - Score: 0.8794 for th=0.575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/1906] Elapsed 0m 0s (remain 14m 48s) Loss: 0.0028(0.0028) Grad: 9631.1094  LR: 0.00000375  \n",
      "Epoch: [5][100/1906] Elapsed 0m 25s (remain 7m 32s) Loss: 0.0012(0.0053) Grad: 3206.0491  LR: 0.00000358  \n",
      "Epoch: [5][200/1906] Elapsed 0m 50s (remain 7m 7s) Loss: 0.0062(0.0058) Grad: 20451.3145  LR: 0.00000340  \n",
      "Epoch: [5][300/1906] Elapsed 1m 15s (remain 6m 40s) Loss: 0.0001(0.0060) Grad: 140.0103  LR: 0.00000323  \n",
      "Epoch: [5][400/1906] Elapsed 1m 39s (remain 6m 15s) Loss: 0.0105(0.0067) Grad: 23160.0410  LR: 0.00000306  \n",
      "Epoch: [5][500/1906] Elapsed 2m 5s (remain 5m 50s) Loss: 0.0004(0.0065) Grad: 2003.4817  LR: 0.00000290  \n",
      "Epoch: [5][600/1906] Elapsed 2m 29s (remain 5m 25s) Loss: 0.0012(0.0064) Grad: 3200.5037  LR: 0.00000274  \n",
      "Epoch: [5][700/1906] Elapsed 2m 55s (remain 5m 0s) Loss: 0.0036(0.0066) Grad: 11178.0752  LR: 0.00000258  \n",
      "Epoch: [5][800/1906] Elapsed 3m 20s (remain 4m 35s) Loss: 0.0034(0.0065) Grad: 4886.8359  LR: 0.00000243  \n",
      "Epoch: [5][900/1906] Elapsed 3m 44s (remain 4m 10s) Loss: 0.0053(0.0064) Grad: 18523.6270  LR: 0.00000228  \n",
      "Epoch: [5][1000/1906] Elapsed 4m 9s (remain 3m 45s) Loss: 0.0005(0.0065) Grad: 1063.3479  LR: 0.00000213  \n",
      "Epoch: [5][1100/1906] Elapsed 4m 34s (remain 3m 20s) Loss: 0.0034(0.0064) Grad: 14259.5938  LR: 0.00000199  \n",
      "Epoch: [5][1200/1906] Elapsed 4m 59s (remain 2m 56s) Loss: 0.0123(0.0063) Grad: 23050.2461  LR: 0.00000185  \n",
      "Epoch: [5][1300/1906] Elapsed 5m 24s (remain 2m 31s) Loss: 0.0015(0.0063) Grad: 3366.1946  LR: 0.00000172  \n",
      "Epoch: [5][1400/1906] Elapsed 5m 49s (remain 2m 6s) Loss: 0.0001(0.0064) Grad: 110.7776  LR: 0.00000159  \n",
      "Epoch: [5][1500/1906] Elapsed 6m 14s (remain 1m 41s) Loss: 0.0003(0.0064) Grad: 1643.8521  LR: 0.00000147  \n",
      "Epoch: [5][1600/1906] Elapsed 6m 39s (remain 1m 16s) Loss: 0.0018(0.0064) Grad: 5074.2915  LR: 0.00000135  \n",
      "Epoch: [5][1700/1906] Elapsed 7m 4s (remain 0m 51s) Loss: 0.0200(0.0064) Grad: 10855.5967  LR: 0.00000123  \n",
      "Epoch: [5][1800/1906] Elapsed 7m 28s (remain 0m 26s) Loss: 0.0002(0.0064) Grad: 1279.9010  LR: 0.00000112  \n",
      "Epoch: [5][1900/1906] Elapsed 7m 53s (remain 0m 1s) Loss: 0.0294(0.0064) Grad: 33128.2383  LR: 0.00000101  \n",
      "Epoch: [5][1905/1906] Elapsed 7m 54s (remain 0m 0s) Loss: 0.0076(0.0064) Grad: 9989.3203  LR: 0.00000101  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 4s) Loss: 0.0085(0.0085) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0074(0.0167) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0041(0.0165) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0001(0.0150) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0064  avg_val_loss: 0.0150  time: 533s\n",
      "Epoch 5 - Score: 0.8829 for th=0.45\n",
      "Epoch 5 - Save Score: 0.8829 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/1906] Elapsed 0m 0s (remain 15m 6s) Loss: 0.0004(0.0004) Grad: 2786.8425  LR: 0.00000101  \n",
      "Epoch: [6][100/1906] Elapsed 0m 25s (remain 7m 32s) Loss: 0.0053(0.0044) Grad: 22625.3672  LR: 0.00000091  \n",
      "Epoch: [6][200/1906] Elapsed 0m 50s (remain 7m 9s) Loss: 0.0024(0.0055) Grad: 5754.3857  LR: 0.00000081  \n",
      "Epoch: [6][300/1906] Elapsed 1m 15s (remain 6m 41s) Loss: 0.0030(0.0061) Grad: 20505.8340  LR: 0.00000072  \n",
      "Epoch: [6][400/1906] Elapsed 1m 40s (remain 6m 16s) Loss: 0.0404(0.0061) Grad: 219187.7656  LR: 0.00000063  \n",
      "Epoch: [6][500/1906] Elapsed 2m 4s (remain 5m 50s) Loss: 0.0021(0.0059) Grad: 19726.6152  LR: 0.00000055  \n",
      "Epoch: [6][600/1906] Elapsed 2m 29s (remain 5m 24s) Loss: 0.0002(0.0057) Grad: 1905.2695  LR: 0.00000048  \n",
      "Epoch: [6][700/1906] Elapsed 2m 54s (remain 4m 59s) Loss: 0.0035(0.0057) Grad: 21252.2852  LR: 0.00000041  \n",
      "Epoch: [6][800/1906] Elapsed 3m 19s (remain 4m 34s) Loss: 0.0001(0.0056) Grad: 314.1557  LR: 0.00000035  \n",
      "Epoch: [6][900/1906] Elapsed 3m 44s (remain 4m 10s) Loss: 0.0001(0.0056) Grad: 268.8907  LR: 0.00000029  \n",
      "Epoch: [6][1000/1906] Elapsed 4m 9s (remain 3m 45s) Loss: 0.0003(0.0056) Grad: 2052.7830  LR: 0.00000023  \n",
      "Epoch: [6][1100/1906] Elapsed 4m 34s (remain 3m 20s) Loss: 0.0257(0.0055) Grad: 144559.8438  LR: 0.00000018  \n",
      "Epoch: [6][1200/1906] Elapsed 4m 59s (remain 2m 55s) Loss: 0.0006(0.0056) Grad: 2044.8790  LR: 0.00000014  \n",
      "Epoch: [6][1300/1906] Elapsed 5m 24s (remain 2m 30s) Loss: 0.0110(0.0056) Grad: 23853.9395  LR: 0.00000010  \n",
      "Epoch: [6][1400/1906] Elapsed 5m 49s (remain 2m 5s) Loss: 0.0010(0.0056) Grad: 3676.8794  LR: 0.00000007  \n",
      "Epoch: [6][1500/1906] Elapsed 6m 14s (remain 1m 40s) Loss: 0.0004(0.0055) Grad: 4682.5254  LR: 0.00000005  \n",
      "Epoch: [6][1600/1906] Elapsed 6m 38s (remain 1m 16s) Loss: 0.0113(0.0056) Grad: 50838.5430  LR: 0.00000003  \n",
      "Epoch: [6][1700/1906] Elapsed 7m 3s (remain 0m 51s) Loss: 0.0042(0.0056) Grad: 54426.5312  LR: 0.00000001  \n",
      "Epoch: [6][1800/1906] Elapsed 7m 28s (remain 0m 26s) Loss: 0.0001(0.0056) Grad: 602.5148  LR: 0.00000000  \n",
      "Epoch: [6][1900/1906] Elapsed 7m 53s (remain 0m 1s) Loss: 0.0185(0.0056) Grad: 20242.0645  LR: 0.00000000  \n",
      "Epoch: [6][1905/1906] Elapsed 7m 54s (remain 0m 0s) Loss: 0.0184(0.0056) Grad: 47766.5938  LR: 0.00000000  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 1s) Loss: 0.0089(0.0089) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0086(0.0185) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0044(0.0184) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0000(0.0168) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0056  avg_val_loss: 0.0168  time: 533s\n",
      "Epoch 6 - Score: 0.8822 for th=0.515\n",
      "========== fold: 2 result ==========\n",
      "Score: 0.8829 Best threshold:: 0.45\n",
      "========== fold: 3 training ==========\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/1906] Elapsed 0m 0s (remain 14m 36s) Loss: 0.6470(0.6470) Grad: inf  LR: 0.00001500  \n",
      "Epoch: [1][100/1906] Elapsed 0m 25s (remain 7m 36s) Loss: 0.0345(0.0865) Grad: 2999.7876  LR: 0.00001500  \n",
      "Epoch: [1][200/1906] Elapsed 0m 50s (remain 7m 10s) Loss: 0.0105(0.0566) Grad: 1454.8508  LR: 0.00001499  \n",
      "Epoch: [1][300/1906] Elapsed 1m 15s (remain 6m 43s) Loss: 0.0129(0.0471) Grad: 1188.7983  LR: 0.00001497  \n",
      "Epoch: [1][400/1906] Elapsed 1m 40s (remain 6m 17s) Loss: 0.0071(0.0409) Grad: 1196.0090  LR: 0.00001495  \n",
      "Epoch: [1][500/1906] Elapsed 2m 5s (remain 5m 51s) Loss: 0.0117(0.0370) Grad: 1414.2686  LR: 0.00001493  \n",
      "Epoch: [1][600/1906] Elapsed 2m 30s (remain 5m 25s) Loss: 0.0411(0.0340) Grad: 4515.7695  LR: 0.00001490  \n",
      "Epoch: [1][700/1906] Elapsed 2m 54s (remain 5m 0s) Loss: 0.0256(0.0318) Grad: 2132.2915  LR: 0.00001486  \n",
      "Epoch: [1][800/1906] Elapsed 3m 19s (remain 4m 35s) Loss: 0.0007(0.0299) Grad: 242.1227  LR: 0.00001482  \n",
      "Epoch: [1][900/1906] Elapsed 3m 44s (remain 4m 10s) Loss: 0.1318(0.0286) Grad: 10760.5889  LR: 0.00001477  \n",
      "Epoch: [1][1000/1906] Elapsed 4m 9s (remain 3m 45s) Loss: 0.0072(0.0275) Grad: 840.6426  LR: 0.00001472  \n",
      "Epoch: [1][1100/1906] Elapsed 4m 34s (remain 3m 20s) Loss: 0.0186(0.0262) Grad: 4088.5759  LR: 0.00001466  \n",
      "Epoch: [1][1200/1906] Elapsed 4m 59s (remain 2m 55s) Loss: 0.0308(0.0252) Grad: 4640.9067  LR: 0.00001460  \n",
      "Epoch: [1][1300/1906] Elapsed 5m 24s (remain 2m 30s) Loss: 0.0115(0.0246) Grad: 840.6278  LR: 0.00001453  \n",
      "Epoch: [1][1400/1906] Elapsed 5m 48s (remain 2m 5s) Loss: 0.0437(0.0240) Grad: 4222.3755  LR: 0.00001445  \n",
      "Epoch: [1][1500/1906] Elapsed 6m 13s (remain 1m 40s) Loss: 0.0301(0.0235) Grad: 7463.0229  LR: 0.00001437  \n",
      "Epoch: [1][1600/1906] Elapsed 6m 39s (remain 1m 16s) Loss: 0.0037(0.0229) Grad: 231.8024  LR: 0.00001429  \n",
      "Epoch: [1][1700/1906] Elapsed 7m 4s (remain 0m 51s) Loss: 0.0077(0.0225) Grad: 472.7743  LR: 0.00001420  \n",
      "Epoch: [1][1800/1906] Elapsed 7m 29s (remain 0m 26s) Loss: 0.0514(0.0222) Grad: 1789.1908  LR: 0.00001410  \n",
      "Epoch: [1][1900/1906] Elapsed 7m 54s (remain 0m 1s) Loss: 0.0178(0.0218) Grad: 7562.0425  LR: 0.00001400  \n",
      "Epoch: [1][1905/1906] Elapsed 7m 55s (remain 0m 0s) Loss: 0.0373(0.0218) Grad: 7829.0146  LR: 0.00001400  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 6s) Loss: 0.0097(0.0097) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0226(0.0135) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0146(0.0145) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0010(0.0132) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0218  avg_val_loss: 0.0132  time: 533s\n",
      "Epoch 1 - Score: 0.8618 for th=0.45\n",
      "Epoch 1 - Save Score: 0.8618 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/1906] Elapsed 0m 0s (remain 15m 40s) Loss: 0.0184(0.0184) Grad: 237774.0000  LR: 0.00001399  \n",
      "Epoch: [2][100/1906] Elapsed 0m 25s (remain 7m 42s) Loss: 0.0029(0.0110) Grad: 5480.0410  LR: 0.00001389  \n",
      "Epoch: [2][200/1906] Elapsed 0m 50s (remain 7m 11s) Loss: 0.0065(0.0111) Grad: 14557.2207  LR: 0.00001378  \n",
      "Epoch: [2][300/1906] Elapsed 1m 15s (remain 6m 44s) Loss: 0.0003(0.0115) Grad: 776.2796  LR: 0.00001366  \n",
      "Epoch: [2][400/1906] Elapsed 1m 40s (remain 6m 17s) Loss: 0.0152(0.0115) Grad: 19904.1094  LR: 0.00001354  \n",
      "Epoch: [2][500/1906] Elapsed 2m 5s (remain 5m 52s) Loss: 0.0072(0.0111) Grad: 11900.3164  LR: 0.00001342  \n",
      "Epoch: [2][600/1906] Elapsed 2m 30s (remain 5m 26s) Loss: 0.0036(0.0108) Grad: 16726.2070  LR: 0.00001329  \n",
      "Epoch: [2][700/1906] Elapsed 2m 55s (remain 5m 1s) Loss: 0.0085(0.0107) Grad: 12649.2676  LR: 0.00001316  \n",
      "Epoch: [2][800/1906] Elapsed 3m 20s (remain 4m 36s) Loss: 0.0014(0.0108) Grad: 8082.4058  LR: 0.00001302  \n",
      "Epoch: [2][900/1906] Elapsed 3m 44s (remain 4m 10s) Loss: 0.0046(0.0107) Grad: 7826.2676  LR: 0.00001288  \n",
      "Epoch: [2][1000/1906] Elapsed 4m 9s (remain 3m 45s) Loss: 0.0025(0.0107) Grad: 10282.8984  LR: 0.00001273  \n",
      "Epoch: [2][1100/1906] Elapsed 4m 34s (remain 3m 20s) Loss: 0.0103(0.0105) Grad: 17826.4746  LR: 0.00001258  \n",
      "Epoch: [2][1200/1906] Elapsed 4m 59s (remain 2m 56s) Loss: 0.0105(0.0107) Grad: 19345.9414  LR: 0.00001243  \n",
      "Epoch: [2][1300/1906] Elapsed 5m 24s (remain 2m 31s) Loss: 0.0032(0.0105) Grad: 27308.8730  LR: 0.00001227  \n",
      "Epoch: [2][1400/1906] Elapsed 5m 49s (remain 2m 6s) Loss: 0.0061(0.0105) Grad: 21859.2148  LR: 0.00001211  \n",
      "Epoch: [2][1500/1906] Elapsed 6m 14s (remain 1m 41s) Loss: 0.0144(0.0105) Grad: 20276.1973  LR: 0.00001195  \n",
      "Epoch: [2][1600/1906] Elapsed 6m 39s (remain 1m 16s) Loss: 0.0042(0.0104) Grad: 24670.8398  LR: 0.00001178  \n",
      "Epoch: [2][1700/1906] Elapsed 7m 4s (remain 0m 51s) Loss: 0.0072(0.0104) Grad: 52793.9375  LR: 0.00001161  \n",
      "Epoch: [2][1800/1906] Elapsed 7m 29s (remain 0m 26s) Loss: 0.0085(0.0103) Grad: 21754.8867  LR: 0.00001144  \n",
      "Epoch: [2][1900/1906] Elapsed 7m 54s (remain 0m 1s) Loss: 0.0098(0.0102) Grad: 23451.2266  LR: 0.00001126  \n",
      "Epoch: [2][1905/1906] Elapsed 7m 55s (remain 0m 0s) Loss: 0.0087(0.0102) Grad: 37716.4023  LR: 0.00001125  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 4s) Loss: 0.0102(0.0102) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0239(0.0123) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0104(0.0149) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0002(0.0134) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0102  avg_val_loss: 0.0134  time: 533s\n",
      "Epoch 2 - Score: 0.8749 for th=0.575\n",
      "Epoch 2 - Save Score: 0.8749 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/1906] Elapsed 0m 0s (remain 15m 36s) Loss: 0.0031(0.0031) Grad: 10984.2012  LR: 0.00001125  \n",
      "Epoch: [3][100/1906] Elapsed 0m 25s (remain 7m 34s) Loss: 0.0010(0.0078) Grad: 10035.7568  LR: 0.00001107  \n",
      "Epoch: [3][200/1906] Elapsed 0m 50s (remain 7m 11s) Loss: 0.0033(0.0070) Grad: 10098.4521  LR: 0.00001089  \n",
      "Epoch: [3][300/1906] Elapsed 1m 15s (remain 6m 44s) Loss: 0.0012(0.0071) Grad: 6495.3931  LR: 0.00001070  \n",
      "Epoch: [3][400/1906] Elapsed 1m 40s (remain 6m 17s) Loss: 0.0020(0.0079) Grad: 24313.0312  LR: 0.00001052  \n",
      "Epoch: [3][500/1906] Elapsed 2m 5s (remain 5m 52s) Loss: 0.0001(0.0081) Grad: 490.7624  LR: 0.00001033  \n",
      "Epoch: [3][600/1906] Elapsed 2m 30s (remain 5m 27s) Loss: 0.0007(0.0081) Grad: 3481.6147  LR: 0.00001013  \n",
      "Epoch: [3][700/1906] Elapsed 2m 55s (remain 5m 2s) Loss: 0.0005(0.0082) Grad: 3729.0339  LR: 0.00000994  \n",
      "Epoch: [3][800/1906] Elapsed 3m 20s (remain 4m 37s) Loss: 0.0001(0.0084) Grad: 417.8151  LR: 0.00000975  \n",
      "Epoch: [3][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0014(0.0083) Grad: 4896.8608  LR: 0.00000955  \n",
      "Epoch: [3][1000/1906] Elapsed 4m 10s (remain 3m 46s) Loss: 0.0365(0.0083) Grad: 116583.3359  LR: 0.00000935  \n",
      "Epoch: [3][1100/1906] Elapsed 4m 35s (remain 3m 21s) Loss: 0.0074(0.0084) Grad: 13357.2285  LR: 0.00000915  \n",
      "Epoch: [3][1200/1906] Elapsed 5m 0s (remain 2m 56s) Loss: 0.0003(0.0086) Grad: 5289.8560  LR: 0.00000895  \n",
      "Epoch: [3][1300/1906] Elapsed 5m 25s (remain 2m 31s) Loss: 0.0033(0.0086) Grad: 9384.7422  LR: 0.00000874  \n",
      "Epoch: [3][1400/1906] Elapsed 5m 50s (remain 2m 6s) Loss: 0.0283(0.0088) Grad: 37077.1367  LR: 0.00000854  \n",
      "Epoch: [3][1500/1906] Elapsed 6m 14s (remain 1m 41s) Loss: 0.0032(0.0087) Grad: 11912.8584  LR: 0.00000834  \n",
      "Epoch: [3][1600/1906] Elapsed 6m 39s (remain 1m 16s) Loss: 0.0060(0.0087) Grad: 21155.0898  LR: 0.00000813  \n",
      "Epoch: [3][1700/1906] Elapsed 7m 4s (remain 0m 51s) Loss: 0.0006(0.0088) Grad: 2756.3018  LR: 0.00000793  \n",
      "Epoch: [3][1800/1906] Elapsed 7m 29s (remain 0m 26s) Loss: 0.0009(0.0088) Grad: 4298.5806  LR: 0.00000772  \n",
      "Epoch: [3][1900/1906] Elapsed 7m 54s (remain 0m 1s) Loss: 0.0014(0.0089) Grad: 45781.4219  LR: 0.00000751  \n",
      "Epoch: [3][1905/1906] Elapsed 7m 55s (remain 0m 0s) Loss: 0.0237(0.0089) Grad: 121854.4609  LR: 0.00000750  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 4s) Loss: 0.0111(0.0111) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 17s) Loss: 0.0222(0.0132) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0113(0.0159) \n",
      "EVAL: [238/239] Elapsed 0m 29s (remain 0m 0s) Loss: 0.0001(0.0143) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0089  avg_val_loss: 0.0143  time: 534s\n",
      "Epoch 3 - Score: 0.8771 for th=0.38\n",
      "Epoch 3 - Save Score: 0.8771 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/1906] Elapsed 0m 0s (remain 15m 41s) Loss: 0.0191(0.0191) Grad: 23869.9395  LR: 0.00000750  \n",
      "Epoch: [4][100/1906] Elapsed 0m 25s (remain 7m 38s) Loss: 0.0035(0.0085) Grad: 8921.7686  LR: 0.00000730  \n",
      "Epoch: [4][200/1906] Elapsed 0m 50s (remain 7m 8s) Loss: 0.0112(0.0086) Grad: 32252.3359  LR: 0.00000709  \n",
      "Epoch: [4][300/1906] Elapsed 1m 15s (remain 6m 41s) Loss: 0.0014(0.0080) Grad: 10974.6611  LR: 0.00000688  \n",
      "Epoch: [4][400/1906] Elapsed 1m 40s (remain 6m 15s) Loss: 0.0007(0.0074) Grad: 12279.8096  LR: 0.00000668  \n",
      "Epoch: [4][500/1906] Elapsed 2m 4s (remain 5m 50s) Loss: 0.0063(0.0074) Grad: 15526.6035  LR: 0.00000648  \n",
      "Epoch: [4][600/1906] Elapsed 2m 29s (remain 5m 25s) Loss: 0.0017(0.0074) Grad: 65611.6406  LR: 0.00000627  \n",
      "Epoch: [4][700/1906] Elapsed 2m 54s (remain 5m 0s) Loss: 0.0008(0.0075) Grad: 4200.7769  LR: 0.00000607  \n",
      "Epoch: [4][800/1906] Elapsed 3m 19s (remain 4m 35s) Loss: 0.0055(0.0075) Grad: 25301.4688  LR: 0.00000587  \n",
      "Epoch: [4][900/1906] Elapsed 3m 44s (remain 4m 10s) Loss: 0.0208(0.0075) Grad: 19243.6309  LR: 0.00000567  \n",
      "Epoch: [4][1000/1906] Elapsed 4m 9s (remain 3m 45s) Loss: 0.0010(0.0076) Grad: 5748.6519  LR: 0.00000547  \n",
      "Epoch: [4][1100/1906] Elapsed 4m 33s (remain 3m 20s) Loss: 0.0148(0.0076) Grad: 18316.2324  LR: 0.00000527  \n",
      "Epoch: [4][1200/1906] Elapsed 4m 59s (remain 2m 55s) Loss: 0.0091(0.0078) Grad: 18796.3066  LR: 0.00000507  \n",
      "Epoch: [4][1300/1906] Elapsed 5m 23s (remain 2m 30s) Loss: 0.0000(0.0078) Grad: 138.4787  LR: 0.00000488  \n",
      "Epoch: [4][1400/1906] Elapsed 5m 48s (remain 2m 5s) Loss: 0.0090(0.0079) Grad: 18669.6172  LR: 0.00000469  \n",
      "Epoch: [4][1500/1906] Elapsed 6m 13s (remain 1m 40s) Loss: 0.0074(0.0078) Grad: 48915.4805  LR: 0.00000450  \n",
      "Epoch: [4][1600/1906] Elapsed 6m 38s (remain 1m 15s) Loss: 0.0045(0.0077) Grad: 22975.4805  LR: 0.00000431  \n",
      "Epoch: [4][1700/1906] Elapsed 7m 3s (remain 0m 51s) Loss: 0.0078(0.0076) Grad: 54572.6328  LR: 0.00000413  \n",
      "Epoch: [4][1800/1906] Elapsed 7m 28s (remain 0m 26s) Loss: 0.0009(0.0075) Grad: 4681.6704  LR: 0.00000394  \n",
      "Epoch: [4][1900/1906] Elapsed 7m 53s (remain 0m 1s) Loss: 0.0000(0.0078) Grad: 134.9633  LR: 0.00000376  \n",
      "Epoch: [4][1905/1906] Elapsed 7m 54s (remain 0m 0s) Loss: 0.0352(0.0078) Grad: 36467.0977  LR: 0.00000375  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 5s) Loss: 0.0141(0.0141) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0268(0.0144) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0126(0.0176) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0000(0.0157) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0078  avg_val_loss: 0.0157  time: 532s\n",
      "Epoch 4 - Score: 0.8785 for th=0.545\n",
      "Epoch 4 - Save Score: 0.8785 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/1906] Elapsed 0m 0s (remain 15m 28s) Loss: 0.0182(0.0182) Grad: 48758.8477  LR: 0.00000375  \n",
      "Epoch: [5][100/1906] Elapsed 0m 25s (remain 7m 37s) Loss: 0.0002(0.0068) Grad: 2365.0435  LR: 0.00000358  \n",
      "Epoch: [5][200/1906] Elapsed 0m 51s (remain 7m 13s) Loss: 0.0021(0.0073) Grad: 53219.0469  LR: 0.00000340  \n",
      "Epoch: [5][300/1906] Elapsed 1m 16s (remain 6m 47s) Loss: 0.0001(0.0067) Grad: 984.3024  LR: 0.00000323  \n",
      "Epoch: [5][400/1906] Elapsed 1m 41s (remain 6m 19s) Loss: 0.0076(0.0069) Grad: 19090.9004  LR: 0.00000306  \n",
      "Epoch: [5][500/1906] Elapsed 2m 6s (remain 5m 54s) Loss: 0.0058(0.0068) Grad: 12801.9014  LR: 0.00000290  \n",
      "Epoch: [5][600/1906] Elapsed 2m 31s (remain 5m 28s) Loss: 0.0021(0.0071) Grad: 24225.2793  LR: 0.00000274  \n",
      "Epoch: [5][700/1906] Elapsed 2m 56s (remain 5m 3s) Loss: 0.0048(0.0070) Grad: 38018.3672  LR: 0.00000258  \n",
      "Epoch: [5][800/1906] Elapsed 3m 21s (remain 4m 38s) Loss: 0.0066(0.0070) Grad: 10757.9902  LR: 0.00000243  \n",
      "Epoch: [5][900/1906] Elapsed 3m 46s (remain 4m 12s) Loss: 0.0161(0.0070) Grad: 62580.4102  LR: 0.00000228  \n",
      "Epoch: [5][1000/1906] Elapsed 4m 11s (remain 3m 47s) Loss: 0.0001(0.0069) Grad: 201.7148  LR: 0.00000213  \n",
      "Epoch: [5][1100/1906] Elapsed 4m 36s (remain 3m 22s) Loss: 0.0001(0.0069) Grad: 642.3620  LR: 0.00000199  \n",
      "Epoch: [5][1200/1906] Elapsed 5m 1s (remain 2m 56s) Loss: 0.0005(0.0067) Grad: 2502.8384  LR: 0.00000185  \n",
      "Epoch: [5][1300/1906] Elapsed 5m 25s (remain 2m 31s) Loss: 0.0042(0.0067) Grad: 16236.0908  LR: 0.00000172  \n",
      "Epoch: [5][1400/1906] Elapsed 5m 50s (remain 2m 6s) Loss: 0.0013(0.0067) Grad: 10113.1348  LR: 0.00000159  \n",
      "Epoch: [5][1500/1906] Elapsed 6m 15s (remain 1m 41s) Loss: 0.0012(0.0067) Grad: 6241.7451  LR: 0.00000147  \n",
      "Epoch: [5][1600/1906] Elapsed 6m 40s (remain 1m 16s) Loss: 0.0033(0.0067) Grad: 9735.2061  LR: 0.00000135  \n",
      "Epoch: [5][1700/1906] Elapsed 7m 5s (remain 0m 51s) Loss: 0.0453(0.0067) Grad: 79916.7578  LR: 0.00000123  \n",
      "Epoch: [5][1800/1906] Elapsed 7m 30s (remain 0m 26s) Loss: 0.0000(0.0066) Grad: 122.3205  LR: 0.00000112  \n",
      "Epoch: [5][1900/1906] Elapsed 7m 55s (remain 0m 1s) Loss: 0.0027(0.0066) Grad: 20048.4258  LR: 0.00000101  \n",
      "Epoch: [5][1905/1906] Elapsed 7m 56s (remain 0m 0s) Loss: 0.0161(0.0066) Grad: 33589.4805  LR: 0.00000101  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 3s) Loss: 0.0153(0.0153) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0360(0.0160) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0134(0.0192) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0000(0.0172) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0066  avg_val_loss: 0.0172  time: 534s\n",
      "Epoch 5 - Score: 0.8812 for th=0.64\n",
      "Epoch 5 - Save Score: 0.8812 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/1906] Elapsed 0m 0s (remain 15m 25s) Loss: 0.0025(0.0025) Grad: 7586.3970  LR: 0.00000101  \n",
      "Epoch: [6][100/1906] Elapsed 0m 25s (remain 7m 34s) Loss: 0.0195(0.0040) Grad: 20433.4473  LR: 0.00000091  \n",
      "Epoch: [6][200/1906] Elapsed 0m 50s (remain 7m 5s) Loss: 0.0003(0.0057) Grad: 3011.3818  LR: 0.00000081  \n",
      "Epoch: [6][300/1906] Elapsed 1m 15s (remain 6m 41s) Loss: 0.0000(0.0056) Grad: 65.9716  LR: 0.00000072  \n",
      "Epoch: [6][400/1906] Elapsed 1m 40s (remain 6m 15s) Loss: 0.0001(0.0054) Grad: 293.2040  LR: 0.00000063  \n",
      "Epoch: [6][500/1906] Elapsed 2m 4s (remain 5m 49s) Loss: 0.0021(0.0058) Grad: 9889.3809  LR: 0.00000055  \n",
      "Epoch: [6][600/1906] Elapsed 2m 29s (remain 5m 25s) Loss: 0.0027(0.0056) Grad: 56802.1406  LR: 0.00000048  \n",
      "Epoch: [6][700/1906] Elapsed 2m 54s (remain 5m 0s) Loss: 0.0056(0.0058) Grad: 40789.7852  LR: 0.00000041  \n",
      "Epoch: [6][800/1906] Elapsed 3m 19s (remain 4m 35s) Loss: 0.0000(0.0057) Grad: 117.5270  LR: 0.00000035  \n",
      "Epoch: [6][900/1906] Elapsed 3m 44s (remain 4m 10s) Loss: 0.0006(0.0058) Grad: 8614.6826  LR: 0.00000029  \n",
      "Epoch: [6][1000/1906] Elapsed 4m 9s (remain 3m 45s) Loss: 0.0120(0.0059) Grad: 34156.3516  LR: 0.00000023  \n",
      "Epoch: [6][1100/1906] Elapsed 4m 33s (remain 3m 20s) Loss: 0.0055(0.0059) Grad: 28058.0254  LR: 0.00000018  \n",
      "Epoch: [6][1200/1906] Elapsed 4m 58s (remain 2m 55s) Loss: 0.0091(0.0059) Grad: 30975.8047  LR: 0.00000014  \n",
      "Epoch: [6][1300/1906] Elapsed 5m 23s (remain 2m 30s) Loss: 0.0027(0.0058) Grad: 13592.4600  LR: 0.00000010  \n",
      "Epoch: [6][1400/1906] Elapsed 5m 48s (remain 2m 5s) Loss: 0.0150(0.0058) Grad: 50322.6797  LR: 0.00000007  \n",
      "Epoch: [6][1500/1906] Elapsed 6m 12s (remain 1m 40s) Loss: 0.0001(0.0059) Grad: 1187.0500  LR: 0.00000005  \n",
      "Epoch: [6][1600/1906] Elapsed 6m 37s (remain 1m 15s) Loss: 0.0032(0.0059) Grad: 20844.5684  LR: 0.00000003  \n",
      "Epoch: [6][1700/1906] Elapsed 7m 2s (remain 0m 50s) Loss: 0.0097(0.0059) Grad: 100114.1562  LR: 0.00000001  \n",
      "Epoch: [6][1800/1906] Elapsed 7m 27s (remain 0m 26s) Loss: 0.0012(0.0059) Grad: 9889.6992  LR: 0.00000000  \n",
      "Epoch: [6][1900/1906] Elapsed 7m 52s (remain 0m 1s) Loss: 0.0001(0.0060) Grad: 446.5917  LR: 0.00000000  \n",
      "Epoch: [6][1905/1906] Elapsed 7m 53s (remain 0m 0s) Loss: 0.0096(0.0060) Grad: 56791.9492  LR: 0.00000000  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 2s) Loss: 0.0155(0.0155) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0375(0.0165) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0136(0.0197) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0000(0.0176) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0060  avg_val_loss: 0.0176  time: 531s\n",
      "Epoch 6 - Score: 0.8797 for th=0.695\n",
      "========== fold: 3 result ==========\n",
      "Score: 0.8812 Best threshold:: 0.64\n",
      "========== fold: 4 training ==========\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/1906] Elapsed 0m 0s (remain 13m 23s) Loss: 0.8638(0.8638) Grad: inf  LR: 0.00001500  \n",
      "Epoch: [1][100/1906] Elapsed 0m 25s (remain 7m 29s) Loss: 0.0597(0.1136) Grad: 3800.8384  LR: 0.00001500  \n",
      "Epoch: [1][200/1906] Elapsed 0m 49s (remain 7m 3s) Loss: 0.0150(0.0727) Grad: 1526.3492  LR: 0.00001499  \n",
      "Epoch: [1][300/1906] Elapsed 1m 14s (remain 6m 38s) Loss: 0.0066(0.0574) Grad: 857.4569  LR: 0.00001497  \n",
      "Epoch: [1][400/1906] Elapsed 1m 39s (remain 6m 13s) Loss: 0.0301(0.0488) Grad: 3079.7739  LR: 0.00001495  \n",
      "Epoch: [1][500/1906] Elapsed 2m 4s (remain 5m 49s) Loss: 0.0188(0.0431) Grad: 1997.1191  LR: 0.00001493  \n",
      "Epoch: [1][600/1906] Elapsed 2m 29s (remain 5m 24s) Loss: 0.0178(0.0396) Grad: 1036.5685  LR: 0.00001490  \n",
      "Epoch: [1][700/1906] Elapsed 2m 54s (remain 4m 59s) Loss: 0.0147(0.0365) Grad: 948.0223  LR: 0.00001486  \n",
      "Epoch: [1][800/1906] Elapsed 3m 19s (remain 4m 34s) Loss: 0.0049(0.0345) Grad: 1107.9357  LR: 0.00001482  \n",
      "Epoch: [1][900/1906] Elapsed 3m 44s (remain 4m 9s) Loss: 0.0054(0.0325) Grad: 1288.9294  LR: 0.00001477  \n",
      "Epoch: [1][1000/1906] Elapsed 4m 8s (remain 3m 45s) Loss: 0.0096(0.0309) Grad: 1189.9141  LR: 0.00001472  \n",
      "Epoch: [1][1100/1906] Elapsed 4m 33s (remain 3m 20s) Loss: 0.0173(0.0296) Grad: 4166.1035  LR: 0.00001466  \n",
      "Epoch: [1][1200/1906] Elapsed 4m 58s (remain 2m 55s) Loss: 0.0060(0.0285) Grad: 987.6180  LR: 0.00001460  \n",
      "Epoch: [1][1300/1906] Elapsed 5m 23s (remain 2m 30s) Loss: 0.0004(0.0274) Grad: 113.9121  LR: 0.00001453  \n",
      "Epoch: [1][1400/1906] Elapsed 5m 48s (remain 2m 5s) Loss: 0.0038(0.0266) Grad: 369.2095  LR: 0.00001445  \n",
      "Epoch: [1][1500/1906] Elapsed 6m 13s (remain 1m 40s) Loss: 0.0083(0.0259) Grad: 1917.6660  LR: 0.00001437  \n",
      "Epoch: [1][1600/1906] Elapsed 6m 38s (remain 1m 15s) Loss: 0.0036(0.0252) Grad: 601.1807  LR: 0.00001429  \n",
      "Epoch: [1][1700/1906] Elapsed 7m 3s (remain 0m 50s) Loss: 0.0236(0.0248) Grad: 1253.0483  LR: 0.00001420  \n",
      "Epoch: [1][1800/1906] Elapsed 7m 28s (remain 0m 26s) Loss: 0.0189(0.0243) Grad: 1106.2957  LR: 0.00001410  \n",
      "Epoch: [1][1900/1906] Elapsed 7m 52s (remain 0m 1s) Loss: 0.0222(0.0236) Grad: 625.9100  LR: 0.00001400  \n",
      "Epoch: [1][1905/1906] Elapsed 7m 54s (remain 0m 0s) Loss: 0.0028(0.0236) Grad: 361.1306  LR: 0.00001400  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 5s) Loss: 0.0208(0.0208) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0055(0.0123) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0019(0.0140) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0071(0.0129) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0236  avg_val_loss: 0.0129  time: 532s\n",
      "Epoch 1 - Score: 0.8626 for th=0.345\n",
      "Epoch 1 - Save Score: 0.8626 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/1906] Elapsed 0m 0s (remain 16m 5s) Loss: 0.0009(0.0009) Grad: 2358.7688  LR: 0.00001399  \n",
      "Epoch: [2][100/1906] Elapsed 0m 26s (remain 7m 45s) Loss: 0.0138(0.0123) Grad: 7631.7856  LR: 0.00001389  \n",
      "Epoch: [2][200/1906] Elapsed 0m 51s (remain 7m 13s) Loss: 0.0030(0.0120) Grad: 17083.3398  LR: 0.00001378  \n",
      "Epoch: [2][300/1906] Elapsed 1m 16s (remain 6m 46s) Loss: 0.0143(0.0115) Grad: 20091.6367  LR: 0.00001366  \n",
      "Epoch: [2][400/1906] Elapsed 1m 41s (remain 6m 19s) Loss: 0.0025(0.0114) Grad: 5341.1694  LR: 0.00001354  \n",
      "Epoch: [2][500/1906] Elapsed 2m 6s (remain 5m 53s) Loss: 0.0090(0.0108) Grad: 15104.7803  LR: 0.00001342  \n",
      "Epoch: [2][600/1906] Elapsed 2m 31s (remain 5m 27s) Loss: 0.0043(0.0107) Grad: 10666.6943  LR: 0.00001329  \n",
      "Epoch: [2][700/1906] Elapsed 2m 56s (remain 5m 2s) Loss: 0.0201(0.0106) Grad: 53249.2734  LR: 0.00001316  \n",
      "Epoch: [2][800/1906] Elapsed 3m 20s (remain 4m 37s) Loss: 0.0034(0.0106) Grad: 14959.7617  LR: 0.00001302  \n",
      "Epoch: [2][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0020(0.0106) Grad: 8098.3945  LR: 0.00001288  \n",
      "Epoch: [2][1000/1906] Elapsed 4m 10s (remain 3m 46s) Loss: 0.0006(0.0106) Grad: 2502.2705  LR: 0.00001273  \n",
      "Epoch: [2][1100/1906] Elapsed 4m 35s (remain 3m 21s) Loss: 0.0019(0.0106) Grad: 5037.3203  LR: 0.00001258  \n",
      "Epoch: [2][1200/1906] Elapsed 5m 0s (remain 2m 56s) Loss: 0.0113(0.0105) Grad: 27341.9746  LR: 0.00001243  \n",
      "Epoch: [2][1300/1906] Elapsed 5m 24s (remain 2m 31s) Loss: 0.0001(0.0104) Grad: 366.0100  LR: 0.00001227  \n",
      "Epoch: [2][1400/1906] Elapsed 5m 49s (remain 2m 6s) Loss: 0.0300(0.0104) Grad: 41492.1055  LR: 0.00001211  \n",
      "Epoch: [2][1500/1906] Elapsed 6m 14s (remain 1m 41s) Loss: 0.0148(0.0103) Grad: 16099.9004  LR: 0.00001195  \n",
      "Epoch: [2][1600/1906] Elapsed 6m 39s (remain 1m 16s) Loss: 0.0006(0.0102) Grad: 4324.0459  LR: 0.00001178  \n",
      "Epoch: [2][1700/1906] Elapsed 7m 4s (remain 0m 51s) Loss: 0.0140(0.0102) Grad: 14577.7295  LR: 0.00001161  \n",
      "Epoch: [2][1800/1906] Elapsed 7m 28s (remain 0m 26s) Loss: 0.0069(0.0101) Grad: 11843.0254  LR: 0.00001144  \n",
      "Epoch: [2][1900/1906] Elapsed 7m 53s (remain 0m 1s) Loss: 0.0010(0.0100) Grad: 9204.3877  LR: 0.00001126  \n",
      "Epoch: [2][1905/1906] Elapsed 7m 54s (remain 0m 0s) Loss: 0.0239(0.0100) Grad: 21533.8320  LR: 0.00001125  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 5s) Loss: 0.0191(0.0191) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 16s) Loss: 0.0142(0.0129) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0009(0.0142) \n",
      "EVAL: [238/239] Elapsed 0m 28s (remain 0m 0s) Loss: 0.0112(0.0131) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0100  avg_val_loss: 0.0131  time: 533s\n",
      "Epoch 2 - Score: 0.8762 for th=0.44\n",
      "Epoch 2 - Save Score: 0.8762 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/1906] Elapsed 0m 0s (remain 15m 3s) Loss: 0.0051(0.0051) Grad: 11060.0039  LR: 0.00001125  \n",
      "Epoch: [3][100/1906] Elapsed 0m 25s (remain 7m 42s) Loss: 0.0186(0.0075) Grad: 33847.4688  LR: 0.00001107  \n",
      "Epoch: [3][200/1906] Elapsed 0m 50s (remain 7m 12s) Loss: 0.0035(0.0076) Grad: 5707.6216  LR: 0.00001089  \n",
      "Epoch: [3][300/1906] Elapsed 1m 15s (remain 6m 43s) Loss: 0.0164(0.0075) Grad: 41192.2812  LR: 0.00001070  \n",
      "Epoch: [3][400/1906] Elapsed 1m 40s (remain 6m 17s) Loss: 0.0002(0.0079) Grad: 1594.2341  LR: 0.00001052  \n",
      "Epoch: [3][500/1906] Elapsed 2m 5s (remain 5m 51s) Loss: 0.0111(0.0080) Grad: 20182.3359  LR: 0.00001033  \n",
      "Epoch: [3][600/1906] Elapsed 2m 30s (remain 5m 25s) Loss: 0.0002(0.0082) Grad: 624.0928  LR: 0.00001013  \n",
      "Epoch: [3][700/1906] Elapsed 2m 54s (remain 5m 0s) Loss: 0.0033(0.0083) Grad: 25394.6562  LR: 0.00000994  \n",
      "Epoch: [3][800/1906] Elapsed 3m 20s (remain 4m 35s) Loss: 0.0007(0.0084) Grad: 6474.2778  LR: 0.00000975  \n",
      "Epoch: [3][900/1906] Elapsed 3m 45s (remain 4m 11s) Loss: 0.0145(0.0086) Grad: 20105.3164  LR: 0.00000955  \n",
      "Epoch: [3][1000/1906] Elapsed 4m 10s (remain 3m 46s) Loss: 0.0038(0.0087) Grad: 32311.5332  LR: 0.00000935  \n",
      "Epoch: [3][1100/1906] Elapsed 4m 35s (remain 3m 21s) Loss: 0.0020(0.0088) Grad: 11236.5400  LR: 0.00000915  \n",
      "Epoch: [3][1200/1906] Elapsed 5m 0s (remain 2m 56s) Loss: 0.0042(0.0089) Grad: 24525.3184  LR: 0.00000895  \n",
      "Epoch: [3][1300/1906] Elapsed 5m 27s (remain 2m 32s) Loss: 0.0079(0.0086) Grad: 23140.8789  LR: 0.00000874  \n",
      "Epoch: [3][1400/1906] Elapsed 5m 54s (remain 2m 7s) Loss: 0.0175(0.0087) Grad: 39861.1641  LR: 0.00000854  \n",
      "Epoch: [3][1500/1906] Elapsed 6m 21s (remain 1m 42s) Loss: 0.0017(0.0087) Grad: 5372.5747  LR: 0.00000834  \n",
      "Epoch: [3][1600/1906] Elapsed 6m 48s (remain 1m 17s) Loss: 0.0003(0.0088) Grad: 2628.6763  LR: 0.00000813  \n",
      "Epoch: [3][1700/1906] Elapsed 7m 15s (remain 0m 52s) Loss: 0.0016(0.0088) Grad: 8269.0576  LR: 0.00000793  \n",
      "Epoch: [3][1800/1906] Elapsed 7m 42s (remain 0m 26s) Loss: 0.0001(0.0088) Grad: 915.3019  LR: 0.00000772  \n",
      "Epoch: [3][1900/1906] Elapsed 8m 9s (remain 0m 1s) Loss: 0.0009(0.0088) Grad: 4023.0728  LR: 0.00000751  \n",
      "Epoch: [3][1905/1906] Elapsed 8m 10s (remain 0m 0s) Loss: 0.0013(0.0088) Grad: 8000.4893  LR: 0.00000750  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0074(0.0074) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 17s) Loss: 0.0066(0.0132) \n",
      "EVAL: [200/239] Elapsed 0m 25s (remain 0m 4s) Loss: 0.0004(0.0149) \n",
      "EVAL: [238/239] Elapsed 0m 29s (remain 0m 0s) Loss: 0.0108(0.0138) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0088  avg_val_loss: 0.0138  time: 551s\n",
      "Epoch 3 - Score: 0.8757 for th=0.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/1906] Elapsed 0m 0s (remain 17m 25s) Loss: 0.0006(0.0006) Grad: 2530.5293  LR: 0.00000750  \n",
      "Epoch: [4][100/1906] Elapsed 0m 27s (remain 8m 19s) Loss: 0.0031(0.0071) Grad: 12343.9131  LR: 0.00000730  \n",
      "Epoch: [4][200/1906] Elapsed 0m 55s (remain 7m 46s) Loss: 0.0035(0.0077) Grad: 15362.0596  LR: 0.00000709  \n",
      "Epoch: [4][300/1906] Elapsed 1m 22s (remain 7m 17s) Loss: 0.0004(0.0074) Grad: 1447.2618  LR: 0.00000688  \n",
      "Epoch: [4][400/1906] Elapsed 1m 49s (remain 6m 50s) Loss: 0.0008(0.0078) Grad: 14182.1934  LR: 0.00000668  \n",
      "Epoch: [4][500/1906] Elapsed 2m 16s (remain 6m 23s) Loss: 0.0177(0.0078) Grad: 48160.5742  LR: 0.00000648  \n",
      "Epoch: [4][600/1906] Elapsed 2m 43s (remain 5m 55s) Loss: 0.0017(0.0076) Grad: 12716.2900  LR: 0.00000627  \n",
      "Epoch: [4][700/1906] Elapsed 3m 10s (remain 5m 28s) Loss: 0.0012(0.0078) Grad: 6550.3320  LR: 0.00000607  \n",
      "Epoch: [4][800/1906] Elapsed 3m 38s (remain 5m 0s) Loss: 0.0026(0.0076) Grad: 18074.2031  LR: 0.00000587  \n",
      "Epoch: [4][900/1906] Elapsed 4m 5s (remain 4m 33s) Loss: 0.0007(0.0078) Grad: 18093.8223  LR: 0.00000567  \n",
      "Epoch: [4][1000/1906] Elapsed 4m 32s (remain 4m 6s) Loss: 0.0034(0.0077) Grad: 15425.7734  LR: 0.00000547  \n",
      "Epoch: [4][1100/1906] Elapsed 4m 59s (remain 3m 38s) Loss: 0.0046(0.0077) Grad: 5477.2710  LR: 0.00000527  \n",
      "Epoch: [4][1200/1906] Elapsed 5m 26s (remain 3m 11s) Loss: 0.0070(0.0077) Grad: 16043.4365  LR: 0.00000507  \n",
      "Epoch: [4][1300/1906] Elapsed 5m 53s (remain 2m 44s) Loss: 0.0106(0.0077) Grad: 32062.6113  LR: 0.00000488  \n",
      "Epoch: [4][1400/1906] Elapsed 6m 20s (remain 2m 17s) Loss: 0.0007(0.0077) Grad: 1766.6462  LR: 0.00000469  \n",
      "Epoch: [4][1500/1906] Elapsed 6m 48s (remain 1m 50s) Loss: 0.0000(0.0076) Grad: 117.0306  LR: 0.00000450  \n",
      "Epoch: [4][1600/1906] Elapsed 7m 15s (remain 1m 22s) Loss: 0.0145(0.0076) Grad: 15147.7363  LR: 0.00000431  \n",
      "Epoch: [4][1700/1906] Elapsed 7m 42s (remain 0m 55s) Loss: 0.0194(0.0076) Grad: 66111.7578  LR: 0.00000413  \n",
      "Epoch: [4][1800/1906] Elapsed 8m 9s (remain 0m 28s) Loss: 0.0008(0.0077) Grad: 2242.2815  LR: 0.00000394  \n",
      "Epoch: [4][1900/1906] Elapsed 8m 36s (remain 0m 1s) Loss: 0.0169(0.0077) Grad: 26484.9707  LR: 0.00000376  \n",
      "Epoch: [4][1905/1906] Elapsed 8m 38s (remain 0m 0s) Loss: 0.0005(0.0077) Grad: 1644.7094  LR: 0.00000375  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0074(0.0074) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 17s) Loss: 0.0058(0.0130) \n",
      "EVAL: [200/239] Elapsed 0m 25s (remain 0m 4s) Loss: 0.0005(0.0146) \n",
      "EVAL: [238/239] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0114(0.0135) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0077  avg_val_loss: 0.0135  time: 580s\n",
      "Epoch 4 - Score: 0.8758 for th=0.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/1906] Elapsed 0m 0s (remain 17m 19s) Loss: 0.0080(0.0080) Grad: 9182.7021  LR: 0.00000375  \n",
      "Epoch: [5][100/1906] Elapsed 0m 27s (remain 8m 7s) Loss: 0.0001(0.0053) Grad: 591.7947  LR: 0.00000358  \n",
      "Epoch: [5][200/1906] Elapsed 0m 54s (remain 7m 38s) Loss: 0.0116(0.0056) Grad: 15810.3994  LR: 0.00000340  \n",
      "Epoch: [5][300/1906] Elapsed 1m 20s (remain 7m 10s) Loss: 0.0001(0.0056) Grad: 155.7479  LR: 0.00000323  \n",
      "Epoch: [5][400/1906] Elapsed 1m 47s (remain 6m 42s) Loss: 0.0717(0.0059) Grad: 126774.9375  LR: 0.00000306  \n",
      "Epoch: [5][500/1906] Elapsed 2m 13s (remain 6m 15s) Loss: 0.0010(0.0061) Grad: 4982.3071  LR: 0.00000290  \n",
      "Epoch: [5][600/1906] Elapsed 2m 40s (remain 5m 48s) Loss: 0.0005(0.0058) Grad: 1688.1675  LR: 0.00000274  \n",
      "Epoch: [5][700/1906] Elapsed 3m 7s (remain 5m 21s) Loss: 0.0018(0.0064) Grad: 2974.3447  LR: 0.00000258  \n",
      "Epoch: [5][800/1906] Elapsed 3m 34s (remain 4m 55s) Loss: 0.0074(0.0064) Grad: 24904.9102  LR: 0.00000243  \n",
      "Epoch: [5][900/1906] Elapsed 4m 1s (remain 4m 29s) Loss: 0.0127(0.0064) Grad: 12196.2754  LR: 0.00000228  \n",
      "Epoch: [5][1000/1906] Elapsed 4m 28s (remain 4m 2s) Loss: 0.0139(0.0064) Grad: 15162.6914  LR: 0.00000213  \n",
      "Epoch: [5][1100/1906] Elapsed 4m 55s (remain 3m 36s) Loss: 0.0041(0.0064) Grad: 7552.0835  LR: 0.00000199  \n",
      "Epoch: [5][1200/1906] Elapsed 5m 23s (remain 3m 9s) Loss: 0.0777(0.0065) Grad: 63962.9102  LR: 0.00000185  \n",
      "Epoch: [5][1300/1906] Elapsed 5m 50s (remain 2m 42s) Loss: 0.0004(0.0065) Grad: 1435.6426  LR: 0.00000172  \n",
      "Epoch: [5][1400/1906] Elapsed 6m 17s (remain 2m 16s) Loss: 0.0058(0.0064) Grad: 13613.2939  LR: 0.00000159  \n",
      "Epoch: [5][1500/1906] Elapsed 6m 45s (remain 1m 49s) Loss: 0.0000(0.0065) Grad: 157.0411  LR: 0.00000147  \n",
      "Epoch: [5][1600/1906] Elapsed 7m 12s (remain 1m 22s) Loss: 0.0194(0.0066) Grad: 9299.0137  LR: 0.00000135  \n",
      "Epoch: [5][1700/1906] Elapsed 7m 39s (remain 0m 55s) Loss: 0.0008(0.0067) Grad: 1644.6801  LR: 0.00000123  \n",
      "Epoch: [5][1800/1906] Elapsed 8m 6s (remain 0m 28s) Loss: 0.0003(0.0066) Grad: 626.7543  LR: 0.00000112  \n",
      "Epoch: [5][1900/1906] Elapsed 8m 33s (remain 0m 1s) Loss: 0.0199(0.0065) Grad: 6332.6177  LR: 0.00000101  \n",
      "Epoch: [5][1905/1906] Elapsed 8m 34s (remain 0m 0s) Loss: 0.0112(0.0065) Grad: 12765.2676  LR: 0.00000101  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0027(0.0027) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 17s) Loss: 0.0062(0.0146) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0002(0.0167) \n",
      "EVAL: [238/239] Elapsed 0m 29s (remain 0m 0s) Loss: 0.0117(0.0154) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0065  avg_val_loss: 0.0154  time: 575s\n",
      "Epoch 5 - Score: 0.8787 for th=0.375\n",
      "Epoch 5 - Save Score: 0.8787 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/1906] Elapsed 0m 0s (remain 17m 2s) Loss: 0.0010(0.0010) Grad: 5342.1636  LR: 0.00000101  \n",
      "Epoch: [6][100/1906] Elapsed 0m 27s (remain 8m 9s) Loss: 0.0007(0.0040) Grad: 9262.1924  LR: 0.00000091  \n",
      "Epoch: [6][200/1906] Elapsed 0m 54s (remain 7m 41s) Loss: 0.0082(0.0042) Grad: 11424.8857  LR: 0.00000081  \n",
      "Epoch: [6][300/1906] Elapsed 1m 21s (remain 7m 15s) Loss: 0.0098(0.0044) Grad: 13509.7051  LR: 0.00000072  \n",
      "Epoch: [6][400/1906] Elapsed 1m 48s (remain 6m 47s) Loss: 0.0014(0.0047) Grad: 20124.8828  LR: 0.00000063  \n",
      "Epoch: [6][500/1906] Elapsed 2m 15s (remain 6m 20s) Loss: 0.0001(0.0050) Grad: 1228.5563  LR: 0.00000055  \n",
      "Epoch: [6][600/1906] Elapsed 2m 42s (remain 5m 53s) Loss: 0.0264(0.0050) Grad: 34054.3867  LR: 0.00000048  \n",
      "Epoch: [6][700/1906] Elapsed 3m 9s (remain 5m 26s) Loss: 0.0008(0.0051) Grad: 7187.6431  LR: 0.00000041  \n",
      "Epoch: [6][800/1906] Elapsed 3m 36s (remain 4m 59s) Loss: 0.0002(0.0051) Grad: 2505.5427  LR: 0.00000035  \n",
      "Epoch: [6][900/1906] Elapsed 4m 4s (remain 4m 32s) Loss: 0.0029(0.0051) Grad: 16368.8584  LR: 0.00000029  \n",
      "Epoch: [6][1000/1906] Elapsed 4m 31s (remain 4m 5s) Loss: 0.0040(0.0053) Grad: 34036.5586  LR: 0.00000023  \n",
      "Epoch: [6][1100/1906] Elapsed 4m 58s (remain 3m 37s) Loss: 0.0127(0.0052) Grad: 34193.2031  LR: 0.00000018  \n",
      "Epoch: [6][1200/1906] Elapsed 5m 25s (remain 3m 10s) Loss: 0.0087(0.0053) Grad: 9495.2676  LR: 0.00000014  \n",
      "Epoch: [6][1300/1906] Elapsed 5m 52s (remain 2m 43s) Loss: 0.0197(0.0053) Grad: 18040.4980  LR: 0.00000010  \n",
      "Epoch: [6][1400/1906] Elapsed 6m 20s (remain 2m 17s) Loss: 0.0006(0.0053) Grad: 3828.4026  LR: 0.00000007  \n",
      "Epoch: [6][1500/1906] Elapsed 6m 48s (remain 1m 50s) Loss: 0.0046(0.0055) Grad: 21077.3262  LR: 0.00000005  \n",
      "Epoch: [6][1600/1906] Elapsed 7m 14s (remain 1m 22s) Loss: 0.0002(0.0054) Grad: 1312.2286  LR: 0.00000003  \n",
      "Epoch: [6][1700/1906] Elapsed 7m 41s (remain 0m 55s) Loss: 0.0013(0.0055) Grad: 24797.7461  LR: 0.00000001  \n",
      "Epoch: [6][1800/1906] Elapsed 8m 7s (remain 0m 28s) Loss: 0.0056(0.0056) Grad: 16199.1484  LR: 0.00000000  \n",
      "Epoch: [6][1900/1906] Elapsed 8m 34s (remain 0m 1s) Loss: 0.0000(0.0056) Grad: 64.0238  LR: 0.00000000  \n",
      "Epoch: [6][1905/1906] Elapsed 8m 35s (remain 0m 0s) Loss: 0.0218(0.0057) Grad: 14392.5557  LR: 0.00000000  \n",
      "EVAL: [0/239] Elapsed 0m 0s (remain 1m 8s) Loss: 0.0028(0.0028) \n",
      "EVAL: [100/239] Elapsed 0m 12s (remain 0m 17s) Loss: 0.0069(0.0156) \n",
      "EVAL: [200/239] Elapsed 0m 24s (remain 0m 4s) Loss: 0.0001(0.0179) \n",
      "EVAL: [238/239] Elapsed 0m 29s (remain 0m 0s) Loss: 0.0122(0.0165) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0057  avg_val_loss: 0.0165  time: 575s\n",
      "Epoch 6 - Score: 0.8781 for th=0.51\n",
      "========== fold: 4 result ==========\n",
      "Score: 0.8787 Best threshold:: 0.375\n",
      "========== CV ==========\n",
      "Score: 0.8809 Best threshold:: 0.36\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "trainer = Trainer(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.run(\n",
    "    df=df_train,\n",
    "    feature_text_max_len=feature_text_max_len, \n",
    "    pn_history_max_len=pn_history_max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a96081-3aa7-4948-be95-67e31d3bd7e6",
   "metadata": {
    "id": "f5a96081-3aa7-4948-be95-67e31d3bd7e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536e1b8954204f7e8a670e7b7e510252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>▁</td></tr><tr><td>[fold0] avg_val_loss</td><td>▁</td></tr><tr><td>[fold0] best_th</td><td>▁</td></tr><tr><td>[fold0] epoch</td><td>▁</td></tr><tr><td>[fold0] loss</td><td>████▆▄▂▁▂▁▂▁▂▁▁▂▁▁▃▂▁▂▂▂▂▁▁▂▂▂▂▂▁▁▂▁▂▁▂▁</td></tr><tr><td>[fold0] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold0] score</td><td>▁</td></tr><tr><td>[fold1] avg_train_loss</td><td>▁</td></tr><tr><td>[fold1] avg_val_loss</td><td>▁</td></tr><tr><td>[fold1] best_th</td><td>▁</td></tr><tr><td>[fold1] epoch</td><td>▁</td></tr><tr><td>[fold1] loss</td><td>▇██▆▅▃▂▂▂▃▃▂▁▂▃▃▃▃▂▃▃▅▂▃▂▃▂▂▃▂▃▂▁▂▂▃▂▂▁▂</td></tr><tr><td>[fold1] lr</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold1] score</td><td>▁</td></tr><tr><td>[fold2] avg_train_loss</td><td>▁</td></tr><tr><td>[fold2] avg_val_loss</td><td>▁</td></tr><tr><td>[fold2] best_th</td><td>▁</td></tr><tr><td>[fold2] epoch</td><td>▁</td></tr><tr><td>[fold2] loss</td><td>████▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>[fold2] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold2] score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>0.13472</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.06083</td></tr><tr><td>[fold0] best_th</td><td>0.3</td></tr><tr><td>[fold0] epoch</td><td>1</td></tr><tr><td>[fold0] loss</td><td>0.02692</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] score</td><td>0.0</td></tr><tr><td>[fold1] avg_train_loss</td><td>0.09764</td></tr><tr><td>[fold1] avg_val_loss</td><td>0.06451</td></tr><tr><td>[fold1] best_th</td><td>0.3</td></tr><tr><td>[fold1] epoch</td><td>1</td></tr><tr><td>[fold1] loss</td><td>0.04477</td></tr><tr><td>[fold1] lr</td><td>0.0</td></tr><tr><td>[fold1] score</td><td>0.0</td></tr><tr><td>[fold2] avg_train_loss</td><td>0.18683</td></tr><tr><td>[fold2] avg_val_loss</td><td>0.07739</td></tr><tr><td>[fold2] best_th</td><td>0.3</td></tr><tr><td>[fold2] epoch</td><td>1</td></tr><tr><td>[fold2] loss</td><td>0.11282</td></tr><tr><td>[fold2] lr</td><td>0.0</td></tr><tr><td>[fold2] score</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">woven-resonance-40</strong>: <a href=\"https://wandb.ai/masakiaota/NBME-ScoreClinicalPatientNotes/runs/2szvhbs2\" target=\"_blank\">https://wandb.ai/masakiaota/NBME-ScoreClinicalPatientNotes/runs/2szvhbs2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220422_232601-2szvhbs2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "commit_msg = '\"run_name: ' + wandb.run.name[:wandb.run.name.rfind('-')] + '\"'\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../output/* ../drive/MyDrive/exp067_output/\n",
    "!cp exp067_train.ipynb ../drive/MyDrive/exp067_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9e729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "exp058_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-02T05:40:40.926468",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02ca5a1bd05f4677ab00d2317fab3e98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1d0a80bd397e4343b229dfc0e8710fbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2489d9146ae14cdb992e20243dcefe41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2675c5a9d05d4e528158c8a2ea0ac2f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33dbeac5c3c3479cbf11bfb50d9dfb25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1ef94a77ac64563b17828197f72b3ce",
      "placeholder": "​",
      "style": "IPY_MODEL_997c32f0f331437ebc1f9c348eae908b",
      "value": "100%"
     }
    },
    "37c74228a2e64f629e6a37f183babcc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39e5db951bac4b928ad4780a50911407": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_33dbeac5c3c3479cbf11bfb50d9dfb25",
       "IPY_MODEL_c9b9d9599286411e947afb2025ced89b",
       "IPY_MODEL_b86dad7854a34e03a0d73ef02b91bc34"
      ],
      "layout": "IPY_MODEL_c14f830ddf7745d1bf121a0fe015931f"
     }
    },
    "423d87770df94d2f95386d76b39a12f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2489d9146ae14cdb992e20243dcefe41",
      "max": 42146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_575391684d0446cab7ceb1232a451135",
      "value": 42146
     }
    },
    "575391684d0446cab7ceb1232a451135": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c4b64ed971348e481146662547cd2f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2675c5a9d05d4e528158c8a2ea0ac2f3",
      "placeholder": "​",
      "style": "IPY_MODEL_1d0a80bd397e4343b229dfc0e8710fbd",
      "value": "100%"
     }
    },
    "997c32f0f331437ebc1f9c348eae908b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1ef94a77ac64563b17828197f72b3ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af5020d41f5043b59dbf26ea604860ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b86dad7854a34e03a0d73ef02b91bc34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef59bad7b39f431ca0fd83f8f56b59de",
      "placeholder": "​",
      "style": "IPY_MODEL_37c74228a2e64f629e6a37f183babcc4",
      "value": " 143/143 [00:00&lt;00:00, 2821.84it/s]"
     }
    },
    "c14f830ddf7745d1bf121a0fe015931f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4f6b02ff7c64618bec62ef579ebb0e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0b164ce58e742c0a3e68fc25f1cdbdb",
      "placeholder": "​",
      "style": "IPY_MODEL_c96c1a3e94b143e69533878a57b3b4c1",
      "value": " 42146/42146 [00:23&lt;00:00, 1910.44it/s]"
     }
    },
    "c96c1a3e94b143e69533878a57b3b4c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9b9d9599286411e947afb2025ced89b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdcba5824ebd49babda6a2046b3d04b0",
      "max": 143,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_02ca5a1bd05f4677ab00d2317fab3e98",
      "value": 143
     }
    },
    "cdcba5824ebd49babda6a2046b3d04b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0b164ce58e742c0a3e68fc25f1cdbdb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef054fe237094a8f89af36fe34f29546": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6c4b64ed971348e481146662547cd2f5",
       "IPY_MODEL_423d87770df94d2f95386d76b39a12f1",
       "IPY_MODEL_c4f6b02ff7c64618bec62ef579ebb0e2"
      ],
      "layout": "IPY_MODEL_af5020d41f5043b59dbf26ea604860ab"
     }
    },
    "ef59bad7b39f431ca0fd83f8f56b59de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
